<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Briefing - January 31, 2026</title>
    <style>
        :root {
            --bg: #0d1117;
            --bg-secondary: #161b22;
            --bg-tertiary: #21262d;
            --text: #e6edf3;
            --text-secondary: #8b949e;
            --text-tertiary: #6e7681;
            --accent: #58a6ff;
            --accent-subtle: #388bfd26;
            --border: #30363d;
            --green: #3fb950;
            --green-subtle: rgba(63, 185, 80, 0.15);
            --yellow: #d29922;
            --yellow-subtle: rgba(210, 153, 34, 0.15);
            --red: #f85149;
        }

        @media (prefers-color-scheme: light) {
            :root {
                --bg: #ffffff;
                --bg-secondary: #f6f8fa;
                --bg-tertiary: #eaeef2;
                --text: #1f2328;
                --text-secondary: #656d76;
                --text-tertiary: #8b949e;
                --accent: #0969da;
                --accent-subtle: #0969da1a;
                --border: #d0d7de;
                --green: #1a7f37;
                --green-subtle: rgba(26, 127, 55, 0.12);
                --yellow: #9a6700;
                --yellow-subtle: rgba(154, 103, 0, 0.12);
                --red: #cf222e;
            }
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans', Helvetica, Arial, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 0;
            min-height: 100vh;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        header {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border);
        }

        h1 {
            font-size: 1.75rem;
            font-weight: 600;
            margin-bottom: 0.25rem;
            letter-spacing: -0.02em;
        }

        .subtitle {
            color: var(--text-secondary);
            font-size: 0.95rem;
        }

        .stats {
            display: flex;
            gap: 1.5rem;
            margin-top: 0.75rem;
            font-size: 0.85rem;
            color: var(--text-secondary);
        }

        .stat-value {
            color: var(--text);
            font-weight: 600;
        }

        .nav-links {
            display: flex;
            gap: 1rem;
            margin-top: 1rem;
        }

        .nav-links a {
            color: var(--accent);
            text-decoration: none;
            font-size: 0.9rem;
        }

        .nav-links a:hover {
            text-decoration: underline;
        }

        /* Video Cards */
        .video-card {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 10px;
            margin-bottom: 1.25rem;
            overflow: hidden;
        }

        .video-header {
            padding: 1.25rem 1.5rem 1rem;
        }

        .video-title {
            font-size: 1.05rem;
            font-weight: 600;
            margin-bottom: 0.6rem;
            line-height: 1.4;
        }

        .video-title a {
            color: var(--text);
            text-decoration: none;
            transition: color 0.15s ease;
        }

        .video-title a:hover {
            color: var(--accent);
        }

        .video-meta {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 0.6rem;
            font-size: 0.8rem;
            color: var(--text-tertiary);
        }

        .channel-info {
            display: inline-flex;
            align-items: center;
            gap: 0.45rem;
        }

        .channel-icon {
            width: 22px;
            height: 22px;
            border-radius: 50%;
            object-fit: cover;
            flex-shrink: 0;
            background: var(--bg-tertiary);
        }

        .channel-name {
            font-weight: 500;
            color: var(--text);
        }

        .channel-subs {
            color: var(--text-tertiary);
            font-size: 0.75rem;
        }

        /* Channel Pill */
        .channel-pill {
            display: inline-flex;
            align-items: center;
            gap: 0.4rem;
            background: var(--bg-tertiary);
            padding: 0.3rem 0.7rem 0.3rem 0.4rem;
            border-radius: 20px;
            font-size: 0.8rem;
        }

        .channel-pill .channel-icon {
            width: 20px;
            height: 20px;
        }

        .channel-pill .channel-name {
            font-weight: 500;
            color: var(--text);
        }

        .channel-pill .channel-subs {
            color: var(--text-tertiary);
            font-size: 0.7rem;
            margin-left: 0.15rem;
        }

        /* Tags */
        .video-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.4rem;
            margin-top: 0.6rem;
        }

        .tag {
            display: inline-block;
            background: var(--accent-subtle);
            color: var(--accent);
            padding: 0.2rem 0.6rem;
            border-radius: 12px;
            font-size: 0.72rem;
            font-weight: 500;
        }

        .tag-person {
            background: rgba(136, 87, 255, 0.15);
            color: #a371f7;
        }

        .tag-company {
            background: var(--green-subtle);
            color: var(--green);
        }

        .tag-event {
            background: var(--yellow-subtle);
            color: var(--yellow);
        }

        @media (prefers-color-scheme: light) {
            .tag-person {
                background: rgba(130, 80, 223, 0.12);
                color: #6639ba;
            }
        }

        .channel-badge {
            background: var(--accent-subtle);
            color: var(--accent);
            padding: 0.2rem 0.55rem;
            border-radius: 4px;
            font-size: 0.75rem;
            font-weight: 500;
        }

        .high-trust-badge {
            background: var(--green-subtle);
            color: var(--green);
        }

        .meta-sep {
            color: var(--border);
        }

        /* TL;DR Section */
        .tldr {
            padding: 0.9rem 1.5rem;
            background: var(--bg-tertiary);
            border-top: 1px solid var(--border);
            font-size: 0.9rem;
            color: var(--text-secondary);
            line-height: 1.55;
        }

        /* Summary Section */
        .summary-section {
            padding: 1rem 1.5rem 1.25rem;
            border-top: 1px solid var(--border);
        }

        .summary-preview {
            font-size: 0.92rem;
            line-height: 1.7;
            color: var(--text);
        }

        .summary-full {
            display: none;
            font-size: 0.92rem;
            line-height: 1.7;
        }

        .summary-full.expanded {
            display: block;
        }

        .summary-full h3 {
            font-size: 0.95rem;
            font-weight: 600;
            color: var(--text);
            margin: 1.25rem 0 0.6rem;
        }

        .summary-full h3:first-child {
            margin-top: 0;
        }

        .summary-full h4 {
            font-size: 0.9rem;
            font-weight: 600;
            color: var(--text);
            margin: 1rem 0 0.4rem;
        }

        .summary-full ul {
            margin: 0.4rem 0;
            padding-left: 1.3rem;
        }

        .summary-full li {
            margin: 0.35rem 0;
        }

        /* Nested lists - indentation only, no color/size change */
        .summary-full ul ul {
            margin: 0.2rem 0;
        }

        .summary-full ul ul li {
            margin: 0.25rem 0;
        }

        .summary-full strong {
            color: var(--text);
            font-weight: 600;
        }

        .summary-full p {
            margin: 0.6rem 0;
            line-height: 1.65;
        }

        .summary-full blockquote {
            margin: 1rem 0;
            padding: 0.75rem 1rem;
            background: var(--bg-tertiary);
            border-left: 3px solid var(--accent);
            border-radius: 0 6px 6px 0;
            font-style: italic;
            color: var(--text-secondary);
        }

        .summary-full em {
            font-style: italic;
        }

        /* Toggle Buttons */
        .toggle-btn {
            background: none;
            border: none;
            color: var(--accent);
            padding: 0;
            font-size: 0.85rem;
            font-weight: 500;
            cursor: pointer;
            margin-top: 0.6rem;
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
            transition: opacity 0.15s ease;
        }

        .toggle-btn:hover {
            opacity: 0.8;
        }

        .toggle-btn svg {
            width: 16px;
            height: 16px;
            transition: transform 0.2s ease;
        }

        .toggle-btn.expanded svg {
            transform: rotate(180deg);
        }

        /* Transcript Section */
        .transcript-section {
            padding: 0 1.5rem 1.25rem;
        }

        .transcript-toggle {
            background: var(--bg-tertiary);
            border: 1px solid var(--border);
            color: var(--text-secondary);
            padding: 0.5rem 0.9rem;
            border-radius: 6px;
            font-size: 0.8rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.15s ease;
        }

        .transcript-toggle:hover {
            background: var(--border);
            color: var(--text);
        }

        .transcript-content {
            display: none;
            margin-top: 0.75rem;
            padding: 1rem;
            background: var(--bg);
            border-radius: 6px;
            font-size: 0.85rem;
            line-height: 1.75;
            white-space: pre-wrap;
            max-height: 400px;
            overflow-y: auto;
            border: 1px solid var(--border);
            color: var(--text-secondary);
        }

        .transcript-content.expanded {
            display: block;
            animation: fadeIn 0.2s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        /* Empty State */
        .empty-state {
            text-align: center;
            padding: 4rem 2rem;
            color: var(--text-secondary);
        }

        .empty-state h2 {
            font-size: 1.2rem;
            margin-bottom: 0.4rem;
            color: var(--text);
        }

        /* Index Page Styles */
        .day-list {
            list-style: none;
        }

        .day-card {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 10px;
            margin-bottom: 1rem;
            overflow: hidden;
            transition: border-color 0.15s ease;
        }

        .day-card:hover {
            border-color: var(--accent);
        }

        .day-card a {
            display: block;
            padding: 1.25rem 1.5rem;
            text-decoration: none;
            color: inherit;
        }

        .day-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }

        .day-date {
            font-weight: 600;
            font-size: 1rem;
            color: var(--text);
        }

        .day-count {
            color: var(--text-tertiary);
            font-size: 0.85rem;
        }

        .day-previews {
            display: flex;
            flex-direction: column;
            gap: 0.6rem;
        }

        .day-preview-item {
            display: flex;
            flex-direction: column;
            gap: 0.15rem;
            padding: 0.5rem 0;
            border-bottom: 1px solid var(--border);
        }

        .day-preview-item:last-child {
            border-bottom: none;
            padding-bottom: 0;
        }

        .preview-meta {
            display: flex;
            align-items: center;
            gap: 0.4rem;
            font-size: 0.75rem;
            color: var(--text-tertiary);
        }

        .preview-channel-icon {
            width: 16px;
            height: 16px;
            border-radius: 50%;
            object-fit: cover;
            flex-shrink: 0;
        }

        .preview-title {
            font-size: 0.9rem;
            font-weight: 500;
            color: var(--text);
            line-height: 1.3;
        }

        .preview-tldr {
            font-size: 0.8rem;
            color: var(--text-tertiary);
            line-height: 1.4;
        }

        footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border);
            text-align: center;
            color: var(--text-tertiary);
            font-size: 0.8rem;
        }

        /* Mobile Adjustments */
        @media (max-width: 600px) {
            .container {
                padding: 1.25rem 1rem;
            }

            .video-header, .tldr, .summary-section, .transcript-section {
                padding-left: 1rem;
                padding-right: 1rem;
            }

            .video-title {
                font-size: 1rem;
            }

            .transcript-content {
                max-height: 300px;
            }
        }
        </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Daily Briefing</h1>
            <p class="subtitle">January 31, 2026</p>
            <div class="stats">
                <span><span class="stat-value">2</span> videos</span>
            </div>
            <nav class="nav-links">
                <a href="index.html">&larr; All Briefings</a>
            </nav>
        </header>

        <main>
            
            <article class="video-card">
                <div class="video-header">
                    <h2 class="video-title">
                        <a href="https://www.youtube.com/watch?v=gXY1kx7zlkk" target="_blank" rel="noopener">ICE Chaos in Minneapolis Clawdbot Takeover Why the Dollar is Dropping</a>
                    </h2>
                    <div class="video-meta">
                        <span class="channel-pill"><img class="channel-icon" src="https://yt3.ggpht.com/ytc/AIdro_muNFL-sKuOPm72UvG-ofixqx70KVyRS4365-fTtxH_cg=s240-c-k-c0x00ffffff-no-rj" alt="" loading="lazy"><span class="channel-name">All-In Podcast</span><span class="channel-subs">(1.0M)</span></span>
                        <span class="meta-sep">·</span><span>90:02</span>
                        <span class="meta-sep">·</span><span>420.9K views</span>
                        <span class="meta-sep">·</span>
                        <span>2026-01-31</span>
                    </div>
                    <div class="video-tags"><span class="tag tag-person">Chamath Palihapitiya</span> <span class="tag tag-person">David Sacks</span> <span class="tag tag-person">Larry Fink</span> <span class="tag tag-company">Anthropic</span> <span class="tag tag-company">Meta</span></div>
                </div>
                <div class="tldr">Davos saw a shift towards business and Trump's influence, while domestic debates on immigration intensified following tragic incidents in Minneapolis.</div>
                <div class="summary-section">
                    <div class="summary-preview" id="preview-0">Davos saw a shift towards business and Trump's influence, while domestic debates on immigration intensified following tragic incidents in Minneapolis. Concurrently, new AI agents like Maltbot are transforming personal productivity and challenging proprietary models, and...</div>
                    <div class="summary-full" id="full-0">
                        <h3>TL;DR</h3>
<p>Davos saw a shift towards business and Trump's influence, while domestic debates on immigration intensified following tragic incidents in Minneapolis. Concurrently, new AI agents like Maltbot are transforming personal productivity and challenging proprietary models, and macroeconomic factors like dollar devaluation are exacerbating wealth inequality and fueling populism.</p>
<h3>Davos Reflections &amp; Shifting Global Dynamics</h3>
<ul>
<li><strong>Davos underwent a significant transformation, becoming more business-centric and dominated by the US and Donald Trump.</strong></li>
<ul>
<ul>
<li>The event, previously mocked, was largely preoccupied with anticipation and reaction to Trump's speech.</li>
<li>Larry Fink, chairman of the event, reportedly aimed to orchestrate Trump's presence for a more business-focused agenda.</li>
<li>A notable increase in American attendance compared to previous years was observed.</li>
</ul>
</ul>
<li><strong>European policies, particularly on net-zero and open borders, faced sharp public criticism from US officials.</strong></li>
<ul>
<ul>
<li>Howard Lutnick, US Secretary of Commerce, publicly stated that Europeans had "completely failed" and "wrecked their economies" with net-zero and open border policies.</li>
<li>Al Gore reportedly booed Lutnick's remarks, indicating strong ideological divides on these issues.</li>
<li>Despite economic consequences, many European countries remain committed to the net-zero agenda.</li>
</ul>
</ul>
<li><strong>Europe is perceived as seeking greater autonomy but remains heavily reliant on the US for stability.</strong></li>
<ul>
<ul>
<li>A sentiment that Europeans cannot fully trust the US as a reliable partner has led to efforts to form independent economic and trading blocks.</li>
<li>However, it's argued that mid-tier powers cannot redefine the international system, and Europe's history underscores the critical role of US presence in maintaining post-1945 peace.</li>
<li>Donald Trump's push for NATO allies to increase defense spending (aiming for 5% from 3%) was highlighted as a successful objective.</li>
<li>Trump's negotiation tactic of suggesting the acquisition of Greenland (or Iceland) was eventually diffused but yielded concessions.</li>
</ul>
</ul>
</ul>
<h3>Controversial Immigration Enforcement &amp; Political Drivers</h3>
<ul>
<li><strong>Federal immigration operations in Minneapolis led to two tragic deaths amidst intense local resistance.</strong></li>
<ul>
<ul>
<li>Operation Metro Surge deployed 3,000 federal agents to Minnesota.</li>
<li>Renee Good, 37, was shot by an ICE agent after accelerating her car (January 7th).</li>
<li>Alex Prey, 37, an ICU nurse, was shot and killed by Border Patrol agents after seeking confrontation and allegedly kicking an ICE car while armed (January 24th).</li>
<li>Both individuals were described as "foot soldiers" in "Antifa-style operations" that physically interfered with law enforcement.</li>
</ul>
</ul>
<li><strong>Minneapolis local authorities are accused of "massive resistance" to federal immigration laws.</strong></li>
<ul>
<ul>
<li>Governor Tim Waltz and Mayor Jacob Frey allegedly instructed local police not to cooperate with ICE, effectively encouraging a vacuum that agitators exploited.</li>
<li>This non-cooperation reportedly forces federal agents to conduct more dangerous street arrests; for example, an illegal alien charged with vehicular homicide was released by local authorities instead of being turned over to ICE.</li>
</ul>
</ul>
<li><strong>Political motivations behind immigration policies are intensely debated, linking to electoral power and public safety.</strong></li>
<ul>
<ul>
<li>Polling indicates a significant majority of Americans (55-64%) favor deporting all illegal immigrants.</li>
<li>Proponents argue strict enforcement reduces crime, citing a 21% drop in murders last year.</li>
<li>A core theory suggests Democrats resist mass deportations because illegal immigrants contribute to census counts, influencing congressional seats and electoral votes (e.g., claiming Trump would have gained 9 electoral votes in the last election if numbers were adjusted).</li>
<li>Counterarguments question the electoral logic, noting the long path to citizenship for voting and shifts in populist support.</li>
</ul>
</ul>
<li><strong>Tactics of federal agents and their leadership face criticism, with calls for greater accountability.</strong></li>
<ul>
<ul>
<li>Concerns were raised about federal agents wearing masks, not identifying themselves, and lacking body cameras.</li>
<li>Arguments for agents to always have warrants or probable cause, rather than "randomly asking for papers."</li>
<li>Steven Miller and other Trump appointees are blamed for promoting chaotic, provocative tactics that have led to plummeting approval ratings for Trump on immigration.</li>
<li>An alternative policy proposed is to target businesses that hire illegal aliens with fines and jail time, thereby removing the core incentive for illegal immigration.</li>
</ul>
</ul>
<li><strong>A path to citizenship for long-term, law-abiding contributors is suggested to prevent further conflict.</strong></li>
<ul>
<ul>
<li>A compromise could involve offering permanent residency or citizenship to individuals who have resided in the US for a significant period, paid taxes, and adhered to other laws, as a means to avert potential "civil war."</li>
</ul>
</ul>
</ul>
<h3>The Transformative Impact of AI Agents &amp; Open Source Models</h3>
<ul>
<li><strong>AI personal assistants, or "agents," are emerging as a new, dominant form factor, evolving beyond simple chatbots.</strong></li>
<ul>
<ul>
<li>Claudebot (now Maltbot) is an open-source personal assistant that can integrate with user services like Gmail, Notion, Slack, and WhatsApp.</li>
<li>Demonstrated capabilities include conducting guest research, managing CRMs, drafting and sending emails, updating calendars, and even self-building SAS tools.</li>
<li>"Replicants" (virtual employees) can perform significant portions of tasks (e.g., 40 out of 50 producer hours per week, 95% of SDR tasks) and continuously learn.</li>
</ul>
</ul>
<li><strong>Open-source AI models, exemplified by Kimmy K2.5, are rapidly gaining ground against proprietary "blackbox" AI.</strong></li>
<ul>
<ul>
<li>Kimmy K2.5 is highlighted as a powerful trillion-parameter mixture of expert model capable of solving complex, multi-step problems in parallel.</li>
<li><strong>Key advantages of open source:</strong> enhanced transparency, sovereignty for entities, greater execution control, auditable model weights, local hosting options, and superior data privacy.</li>
<li>The ability to run powerful AI models on commodity hardware (e.g., stacked Mac Studios) is predicted to reduce API costs by 90% or more, allowing for infinite, private usage.</li>
</ul>
</ul>
<li><strong>Current AI policy debates are struggling to keep pace with these rapid technological advancements.</strong></li>
<ul>
<ul>
<li>Policy discussions often frame AI through the lens of social media or niche chatbot applications, failing to grasp the broader utility and potential of AI agents.</li>
<li>The shift to local, open-source models fundamentally challenges assumptions underpinning current legislative approaches, which primarily focus on centrally hosted, closed models.</li>
<li>There is strong advocacy for a federal AI preemption law to prevent fragmented, innovation-stifling state-level regulations.</li>
</ul>
</ul>
<li><strong>Challenges and opportunities arise with the proliferation of open-source models.</strong></li>
<ul>
<ul>
<li><strong>Security concerns</strong> include potential for "secret prompts" or "code injections" in open-source models, especially those originating from foreign countries (e.g., China).</li>
<li>There's a critical need for robust "red-teaming" and validation mechanisms to ensure open-source models are "bulletproof" and reliable, presenting a significant business opportunity.</li>
<li>Despite initial investment, maintaining independent "forks" of open-source models can be challenging due to rapid updates from core developers.</li>
<li>It's noted that American companies, such as Meta, are also actively developing open-source AI models.</li>
</ul>
</ul>
</ul>
<h3>Macroeconomic Headwinds: Dollar Devaluation and Fiscal Strain</h3>
<ul>
<li><strong>The US dollar is experiencing significant devaluation amid aggressive money printing and a global shift away from US Treasuries.</strong></li>
<ul>
<ul>
<li>The dollar index has fallen 10% in the last year, reaching a four-year low.</li>
<li>M2 money supply growth has accelerated to $2.5 trillion per year, with proposals for an additional $500 billion (e.g., for military spending), pushing annual increases close to $3 trillion.</li>
<li>Central banks globally are increasingly favoring gold over US Treasuries in their reserves, with gold now forming a larger share of holdings.</li>
<li>When valued in ounces of gold, the US stock market is actually down since the pre-COVID era, despite nominal dollar gains.</li>
</ul>
</ul>
<li><strong>Escalating debt servicing costs pose a substantial and spiraling fiscal challenge for the US.</strong></li>
<ul>
<ul>
<li>The 30-year US Treasury yield is at 4.9%, significantly higher than the average government borrowing cost of 3.3%.</li>
<li>Refinancing the current $39 trillion national debt at prevailing rates would add an estimated $700 billion annually in interest costs, roughly 70% of the current defense budget.</li>
<li>This situation is attributed to a long-term trend of increasing government spending in democracies without adequate constitutional constraints.</li>
</ul>
</ul>
<li><strong>Dollar devaluation is directly linked to exacerbating wealth inequality and fueling populism.</strong></li>
<ul>
<ul>
<li>Asset owners (e.g., stocks, real estate) benefit from inflation and devaluation as their asset values increase in dollar terms, leading to growing wealth.</li>
<li>The majority of Americans, who are net asset negative and rely on income, do not benefit from this "ddollarization" and increasingly feel "oppressed" and "left behind."</li>
<li>This widening wealth gap is identified as a fundamental driver of populism, demands for socialism, and contributes to civil unrest.</li>
<li>A "nonviolent path" to wealth redistribution, such as a wealth tax, is discussed as a potential (though problematic) measure to avert "civil war," but concern is raised about potential for "extraordinary theft" by special interests if not properly managed.</li>
</ul>
</ul>
</ul>
<h3>Notable Quotes</h3>
<blockquote>
"I've been coming here for 30 years and you guys have completely failed. You've wrecked your economies with all this net zero stuff and climate change and energy. and he just started blasting on that and then all the open border stuff." - Howard Lutnick, US Secretary of Commerce, on European economic policies at Davos.
"What Claudebot demonstrated this week is you're one terms of service update away from everything breaking, right? Because at one point, Anthropic didn't like what was going on and they said, 'No, this is not allowed.'" - Chamath Palihapitiya, on the fragility of closed-source AI.
"The reason why they wear masks is because they're getting doxed by these groups who are organizing with each other on signal and they're being followed around. They're being stalked. They're being chased to their hotels and these guys follow them around. They block their cars by um interposing vehicles. They're interfering in in official operations." - David Sacks, on why federal agents use masks during operations in Minneapolis.
"As the dollar devalues and everything inflates in value, our asset prices go up and we get wealthier and wealthier and wealthier. The majority of Americans do not own assets. They are net asset negative. As a result, they live off of income and they do not benefit from the ddollarization like asset holders do. And this is what is ultimately fueling populism in the United States." - David Friedberg, on the economic drivers of populism.
</blockquote>
                    </div>
                    <button class="toggle-btn" id="sum-btn-0" onclick="toggleSummary(0)">
                        Read more <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M6 9l6 6 6-6"/></svg>
                    </button>
                </div>
                
                <div class="transcript-section">
                    <button class="transcript-toggle" id="trans-btn-0" onclick="toggleTranscript(0)">
                        View transcript
                    </button>
                    <div class="transcript-content" id="transcript-0">
                        All right, everybody. Welcome back to the Allin podcast. Your favorite podcast, podcaster's favorite podcast. With me again, the original Quartet is here. Schmoth Poly Hotia in just an absolutely fabulous winter sweater. January looking great. Look at the size of those buttons. &gt;&gt; Huge buttons. &gt;&gt; How many rhinos died to provide those buttons? &gt;&gt; Zero rhinos. Zero rhinos. &gt;&gt; I'm a simple man that lives by simple means. Okay. Beautiful. Beautiful. And your sultan of science, David Freriededberg. What's the background here? Is that in Is that a melancholy and infinite sadness background? I'm trying to figure it out. &gt;&gt; J, we don't talk about my backgrounds. Thank you. &gt;&gt; It looks like melancholy and infinite sadness by the double album by the amazing band um Smashing Pumpkins. Am I close or is it the original artwork of that? &gt;&gt; Don't talk about my backgrounds. &gt;&gt; You don't talk about your backgrounds. Talk about your background. much giving me so much to work with here. Luckily, I have my straight man, my brother in arms, my Davos party crashing partner, David Saxs. &gt;&gt; I got you your first invite to something elite and exclusive. &gt;&gt; I mean, I got invited to go 25 years ago. They just wanted 50 dimes, but we had a fun time. &gt;&gt; Yeah, we had a good time. &gt;&gt; We had a good time. Uh any uh post Davos WF impressions? We had a lot of interesting meetings. So most of which I don't think we can talk about on air, but um yeah, it was it was an interesting uh interesting event. &gt;&gt; We were staying in a log cabin that was like 300 years old. The ceilings were like six feet high and the door frames were like 5t high. So bumped our heads a couple of times. &gt;&gt; Yeah, &gt;&gt; it was pretty crazy. &gt;&gt; It was brutal. I mean, &gt;&gt; it looked good on the web. The photos of the place looked amazing. &gt;&gt; The Airbnb photos look great. &gt;&gt; Yeah. But, uh, I think you need to be inside the circle. You need to be inside the thick of it. Not driving in every day, but it was a distinctly different Davos. Uh, we've mocked Davos here for many years, but this one was a business takeover and a Trump takeover. &gt;&gt; Correct. &gt;&gt; Chimath, you would have loved it. It was 1.5 days of everybody hand ringing of what Donald Trump would say when daddy got there. Then for the 75 minutes daddy gave his talk, the entire city shut down. Everybody ran to a television set. Then for the next 1.5 days, everybody talked about what Trump said. &gt;&gt; Basically, I think there are two really big differences with the Davos based on what I heard because this is the first time I've attended. one is it was much more business- ccentric and then second there were a lot more Americans there a much bigger American presence and I think that that owes to the fact that President Trump gave a major speech there and I think it was Larry Fank who's chairman of the whole thing now who sort of orchestrated that &gt;&gt; and he he wanted to get President Trump there and I think he's pushed &gt;&gt; for them to be a little bit more business centric and it feels like they're catering a little bit less to kind of boutique European political issues, although that's clearly a big part of it. I would say in terms of memorable moments, a big one was I'm only going to talk about this because I think it was publicly reported otherwise I wouldn't bring it up, but there was this opening night dinner and so Larry Fink calls on like five people to give little speeches and the last one he called on was Howard Lutnik, our Secretary of Commerce. And Howard just goes up there and he just starts dropping truth bombs on them and and uh and again he said all this on the record in his remarks at Davos I think either that day or the next day. So again I'm not talking out of school. It was all stuff that he said but basically he said to like all the Europeans he said look I've been coming here for 30 years and you guys have completely failed. You've wrecked your economies with all this net zero stuff and climate change and energy. and he just started blasting on that and then all the open border stuff. He really let them have it. I mean, he was really the truth. And then there was this uncomfortable like rustling in the audience, you know, as he was like gathering steam and then it's been reported Al Gore started booing at the end. &gt;&gt; Drunk on Chardonnay. Is that true? I can't verify that it was Al Gore, but I definitely heard somebody. And I think it was Al Gore who was &gt;&gt; just like the two guys from the Muppets in the Balcony, Statatler and whoever. And it was just like Waldorf and Statler. &gt;&gt; Well, look, I mean, &gt;&gt; yeah, Walder and Statler. &gt;&gt; This whole climate change thing was Al Gore's big hoax going back to the '9s. And &gt;&gt; well, did he win an Oscar? He won an Oscar. I mean, and even absolutely Bill Gates has acknowledged that this is not an existential threat. It can be dealt with, &gt;&gt; you know. So I think the bloom is off the rose in terms of that whole agenda &gt;&gt; I'd say at Davos. Although to be sure I'd say most people there still probably agree with Al Gore. I mean they haven't changed their policies. I'm talking about the European countries even though it's wrecking their economies. &gt;&gt; They are beholden to this net zero idea. &gt;&gt; It feels like they're in transition and scrambling to try to figure out where they what direction they should go in. they don't feel they can trust America or that we're not their like reliable partner. We're not going to bail them out. We're not going to protect them, etc. And they're going to have to sort of get together as the mids mid-tier economies, economies 5 through 20, and they're going to just have to build their own voting block, their own trading block, and uh go it alone and spend their own money. &gt;&gt; They have it. It's called the EU. It's just yeah &gt;&gt; has not been very successful because I think of their own policies, their energy policies and the open border policies. &gt;&gt; I'm not sure what going it alone means if you don't have &gt;&gt; best-in-class AI or best-in-class weapons. &gt;&gt; Absolutely. I I think it means they're going to have to start investing in those things, buying weapons, making weapons means much of anything. &gt;&gt; Yeah. I I think it means Canada and the EU and all these other countries that feel, hey, we're we're we're gonna have our rug pulled by the Trump administration. We're just going to have to band together and create commerce. And we saw China and uh Canada do a big deal. And I think that's them just trying to say, hey, we have some sovereignty here. We're going to do a partnership with Canada and bring BYD cars in here. And hey, they'll be our big trading partner. So, you know, that's the reaction I think on the other side. &gt;&gt; But they don't &gt;&gt; they don't what &gt;&gt; they don't have as many degrees of freedom as they think they do because great powers define the international system, not mid-tier powers. And a bunch of second or third tier powers cannot redefine the international system even if they band together. And I think at the end of the day, the Europeans, they understand the importance of the United States and specifically they want to keep the US in Europe. I mean, they're desperate to keep NATO together and to keep the US interested and present in Europe because just remember European history before the Americans were there. It's like hundreds of years of wars and constantly fighting each other. &gt;&gt; Exactly. &gt;&gt; Culminating in World War I, World War II, basically the total self-destruction of Europe. And the most peaceful period they've ever experienced has been post 1945 when the Americans are there as the great pacifier. So, they do not want us leaving. And I think they're willing to make large concessions to the US to ensure that we stay there, even though they'll probably grumble about it. But I think that to that end, I think that what President Trump was saying is, look, you guys got to share in the burden here. We've been paying for this whole thing. &gt;&gt; Yeah. No, it makes total sense. And they got their spending up to 3% for NATO and they're going to go to five. So, mission accomplished on that. &gt;&gt; Yes. And on Greenland, he's like, look, you know, we we we was the best part of the speech. He's like, &gt;&gt; &quot;Do you want me to talk about &gt;&gt; for you? &gt;&gt; You got to do something for us. This has got to be a choice.&quot; &gt;&gt; He gets 45 minutes in. He's like, &quot;Do you want me to talk about Greenland? I could talk about it. I You ready? I'm going to talk about it right now.&quot; And then he starts talking about it. But then he starts calling it Iceland. And everybody in the room's like, &quot;Wait, he wants Iceland, too?&quot; And the whole buzz was Trump's going to take Iceland and Greenland. And then, you know, he sort of backed down, but we're not going to invade. And then there was like a sort of sigh of relief. But do you think they actually thought Sachs that he was going to invade Greenland to take it? &gt;&gt; Well, I mean, I don't know. I think that obviously they thought it was a possibility, but the president took the use of force off the table. And yeah, I think you did feel a sigh of relief there. I think you're right. But I think that also, I mean, we don't know the details. They haven't been publicly reported, but there was some sort of meeting at Davos that was convened, I think, by the NATO Secretary General, Mark Ruda, where they negotiated an acceptable compromise on the issue. So, I'm sure we'll find out the details in due course, but it's safe to say that President Trump got enough of what he wanted that he was satisfied with what they worked out. &gt;&gt; I don't know how they just like it's like Lucy with the uh football and Charlie. It's like he's gonna pull the football. Like he's just anchoring the ne negotiations at a military invasion and takeover. He obviously is just going to go for a lease. It's like they they don't know it in year, you know, whatever we're in now, year five or six, year six of Trump being president. Like they should get it by now. He just anchors things at an impossible insane level and then he falls back to whatever he really wanted. It's a classic negotiating technique. All right, we got a lot on the docket. Let's get to work. Everyone wants to hear four venture capitalists and investors talk about the horrific situation in Minneapolis. So, here we go. Uh, for background, I mean, Jason, &gt;&gt; for background, last month, the DHS started an operation called Metro Surge, sending 3,000 federal agents into Minnesota to crack down on illegal aliens. Over the last 3 weeks, two motans were tragically killed in altercations with federal agents. January 7th, 37year-old Renee Good was shot to death by an ICE agent. This incident involved Good accelerating her car, which was surrounded by agents at the time. We're still waiting for the final investigation on this one, but apparently three shots, one through the front windshield, perhaps two through the side. Uh, all these details are still being investigated. Then tragically on January 24th, Alex Prey, also 37, was shot and killed by two border patrol agents, not ICE. Prey was an ICU nurse at the local VA hospital. There's a ton of frame by frame breakdowns available. New York Times and Wall Street Journal did a good job on these. So, I think maybe it's best for us to focus here on maybe the aftermath of all this and the resolution, but you guys can feel free to chime in on the frame by frame breakdowns if you like. In five parts, Steven Miller tweeted that Freddy was an assassin trying to murder federal agents. A source told Axis that Gnome said, &quot;Everything I've done, I've done at the direction of the president and Stephen, Steven being Steven Miller. Greg Bovino uh has been removed from duty and had his social media accounts turned off. President Trump has pivoted, evolved and put Tom Hman in charge.&quot; Quote, &quot;Tom is tough but fair and will report directly to me,&quot; Trump wrote. And uh at the time of this taping, which is on Thursday's uh there was a press conference this morning and here is what Tom Holman said and after 30 seconds we'll go to you Saxs for your reaction to all this. &gt;&gt; No agency organization is perfect. President Trump and I along with others in administration have recognized that certain improvements could and should be made. That's exactly what I'm doing here. So if we get these agreements in place, that means less agents on the street, more agents in the jail. Matter of fact, I have staff from CBP and from ICE working on a draw down plan. What does that look like based on the cooperation? &gt;&gt; Sax, your thoughts? &gt;&gt; Well, first let me say the deaths of Renee Good and Alex Petty are regrettable and and tragic. So are the deaths of Lincoln Riley, Joselyn Nungare, Rachel Morren, Victoria Harwell, Ivory Smith, and too many others to mention who are murdered by criminal illegal aliens. And the media won't ever tell you their names. But President Trump was hired by the American people to do a job, which is to seal the border and deport criminal aliens so that more of these tragedies do not occur in the future. And this is a popular policy. Over 55% of the American people say they want all illegal aliens deported and over 90% want criminal aliens removed. And by criminal aliens, I'm referring to the ones who commit additional crimes after they enter the country illegally. Now, this policy is working. Uh murders were down 21% last year. It's one of the best years in record. And in most states, the process is smooth and doesn't make national news. And the reason for that is because local authorities are cooperating with ICE. But Minneapolis has taken a different approach. They've engaged in a campaign of quote massive resistance to federal authorities. So let's talk about what's actually happening there on the ground. I think the first thing to understand is that what's happening is much more than just protests. And obviously I have no problem with people peacefully protesting and making their opinions known, but that's not what's going on here. These are Antifa style operations designed to thwart the enforcement of federal immigration law. They're highly organized. They're communicating in encrypted chat groups. They stalk and dox ICE agents. They follow them around town. They surround them at their hotels. They use their cars to block roads. And they use bullhorns and whistles to alert criminals who are about to be arrested. And remember, ICE is a law enforcement agency. They have warrants to arrest known criminal aliens. Despite this rhetoric of them being like the Gustapo, they are going after specific named individuals for whom they have warrants to arrest. These are dangerous missions and these agitators are interfering and making these missions even more dangerous. Now, the media has tried to portray good and pretty as simply innocent bystanders or people who were peacefully protesting ICE policies. They weren't. They were foot soldiers in these Antifa style operations and most importantly they brought deadly weapons to the fight. So Renee Good hit an officer with her SUV which under a Minnesota law signed by Tim Waltz himself in 2020 justifies the use of deadly force by an officer to defend himself. And Alex Prey was even more reckless. I think we've probably all seen the video by now where he sought confrontation with ICE officials. He was kicking the car. He was in a rage. This wasn't his first time doing this. And any experienced gun owner will tell you that if you're armed and you're dealing with law enforcement, you have to be the world's biggest pacifist because you're putting your life in danger by making them fear that their lives are in danger. And I think the mainstream media didn't tell people these facts. They just presented highly selective camera angles. They even airbrushed and facetuned Prey to make him appear to be a more, I guess, handsome victim, which is truly sick. Now, in a way, I feel sorry for Good and Pretty because they were the victims of a tinder box that was created in Minnesota by the extreme rhetoric and decisions of Tim Waltz, Jacob Fry, and the political establishment. The local police in Minneapolis should have been allowed to keep conditions safe on the street by creating a perimeter and keeping protesters away from ICE officers who were executing lawful warrants of arrest. But the police were told not to. And then the agitators stepped in and they took advantage of this sort of vacuum of authority to physically intervene. So I think it was almost inevitable that some sort of tragedy was going to result from this abdication of public safety. Now why would Walson Frey want to risk such tragedies with their massive resistance? I think there's two reasons for this. This is the last point I want to make. First, they are desperate to change the subject from the billions of dollars of fraud that they allow to occur on their watch. Remember we had Nick Shirley on the show just a few weeks ago talking about the 8 billion so that was stolen by Somali fraudsters and this campaign of resistance in Minneapolis has done a really good job making everyone forget about that. But I think there's a second and bigger reason that applies and I think it applies to to national Democrats which is they want to thwart mass deportations because illegal immigrants are a vital part of their power base. And you can see this in the 2030 aortionment forecast which just came out. Illegal aliens count towards the census which occurs every decade. And the census determines the aortionment of congressional seats and electoral votes. And what you see in these maps is that citizens of blue states have been migrating to red states because those blue states are failing. And as a result of that, blue states are expected to lose nine house seats and electoral votes because of the changing population numbers. Illegal aliens in blue states have been propping up those numbers. And so, for example, in the last election, President Trump would have won an additional nine electoral votes if we had an accurate counting. So, look, this is not about principle. This is bare knuckle politics. The Democrats are playing for keeps. They don't really care how many innocent Americans get hurt or killed in the process. This is about thwarting a popular policy of deportations and sealing the border which the American people voted for. So don't let the media fool you. &gt;&gt; Freedber Chimath, you want to give your opinion on these two tragic deaths &gt;&gt; or not? I mean, I don't feel you're obligated to comment on this if you don't want to. &gt;&gt; I'm happy to. Nick, I sent a clip into the chat. &gt;&gt; Deported all immigrants who are here illegally, 55% of the New York Times, Marquette, 64%. CBS News, 57%. ABC News with a slightly different question, 56%. So, what you're seeing essentially here is a very clear indication that a majority of Americans, in fact, when they're asked this blunt question, which I believe gets at the underlying feelings, do in fact want to deport all immigrants who are here illegally. There's no arguing with these different numbers cuz they're all essentially the same across four different pollsters. &gt;&gt; I think Sax is right that there's a very very vocal minority. But if we just put that aside, it's important right now to just stick to the facts. Democracy is supposed to be the will of the majority, but also defense and protection for the minority. In this example, the will of the majority is pretty clear. As the CNN clip just showed, everybody wanted the southern border shut and the northern border shut and a structured path to deal with illegal immigration. David's right that that creates a cascade of second and third order effects that have huge implications with respect to the Democrats and their ability to have and curate power. I don't know whether this is what's motivating them or not. I don't want to speculate on that. But the conceptual problem and the conceptual desire of Americans is undisputable. I think that's why Donald Trump won. Now I think though we have to explore the tactics. I think the reality is that both of these two deaths were complete and total tragedies and it has created such an upswell that it has the potential to spin out of control. And if it does that, it risks his ability to continue doing his job and delivering on the conceptual promise that everybody wants. The other thing is that I think that he has otherwise, the president, done an incredible job up until now. The fact that Tom Hman is going there is a really good thing. He was the same person that was awarded a medal by Obama for how he managed Obama's deportation process. It's time to just get control of the process and dial down the temperature because the structural things that they are doing are correct. There are people here that broke the law. There are criminals that are here illegally. We need to remove them because that is the will of the majority. Now we just need to find a way of doing it that creates some freedom to operate for all of law enforcement so that these tragedies stop. That's my two cents. &gt;&gt; Okay. Freedberg, would you like to comment on this or pass? I &gt;&gt; I'll comment. Can I just ask you to react to what Sach said? Do you agree or disagree with his point about the Democrats needing to remove people because they do count in the census and they increase the seats in the House. &gt;&gt; Elon's talked about this a whole bunch as well that the immigration is being done to boost the voter roles. I don't know enough about the census specifically because that occurs x number of years and uh if it's accurate at all. So I'll leave that aside and do some research on it. In terms of importing people for votes, this uh strategy does not make a lot of logical sense. And so many workingclass people voted Trump into office and so many specifically uh Hispanic people, Mexican people are all voting for Trump now because he's a populist and he appeals to to that group. So whatever if Biden and the Democrats were doing that for that reason, that makes no sense. And also those people would have to become citizens in order to vote and that's a 20 30year process. So, you know, they'd be playing an incredibly long game uh on that front &gt;&gt; unless there is cheating in the voting. &gt;&gt; Yeah. &gt;&gt; And we've talked about that as well here. &gt;&gt; For example, if there's no voter ID, you know, &gt;&gt; you know, the Heritage Foundation, I think David, you worked there at some point, right? Did an internship. &gt;&gt; That uh organization, that think tank has done tremendous research into this. They have a database. I think they've collected now 3,000 cases of voter fraud over a 40-year period. So really the whole concept of voter forward being able to tilt a presidential election is just ridiculous and has not been proven. I think Trump filed about 50 or 60 lawsuits and lost all 50 of them. So there's there's no credence to that. Uh but you know I have a couple of thoughts broader on what we've seen and I do actually agree with you David. There should be voter ID everywhere. I I don't think anybody should be able to vote without a driver's license. If you can't or ID if you can't take the time to get ID why should you vote? It doesn't make a lot of sense right. &gt;&gt; Why do you think that's such a push on the other side though Jal? Like what's the motivation for not having IDs? &gt;&gt; The stated &gt;&gt; if it's not about getting people to vote that aren't allowed to vote. &gt;&gt; The stated reason, which I don't believe, is that it's more democratic and you want to get as many people to vote as possible. So, but I don't agree with that. I think everybody should have ID. &gt;&gt; Yeah. Like you got to have ID to buy a beer, right? So, &gt;&gt; yeah. I mean, to get on a plane to I mean, even to ride a train, like you need an ID. I don't I don't understand. &gt;&gt; I think the answer speaks for itself. I mean, uh, to me it's obvious the reason why you prohibit, by the way, it's not just saying you can vote without an ID. They actually prohibit the people administering the polls from checking. There's only one reason to do that. You want to allow cheating, obviously. And one of the things that Doge found was that there were lots of illegal aliens being added to the social security roles. Now, they weren't necessarily collecting social security, but in a lot of &gt;&gt; they were actually paying into it. So, it was quite the opposite. My point is that when you get an SSN number, in a lot of states, all you have to do is check a box when you get a driver's license, which they also give to illegal aliens in order to be added to the voter roles. &gt;&gt; So, they are finding illegal aliens on voter roles. But look, regardless of where you are on cheating in elections, the census just counts total population and then they aortion house seats and electoral votes based on total population, including illegal aliens. And there is data on this. Trump would have won the last election by an additional nine electoral votes if these changes had been made before the last election in 72. &gt;&gt; Do you think uh &gt;&gt; so just to put a pin on that point? It's really important because what it means is that a future Republican would not need to somehow crack the Democrats blue wall of Pennsylvania, Michigan, and Wisconsin. Right? So nine electoral votes is a lot and it does put the presidency that much further out of reach for future Democratic candidates. It provides a powerful incentive and it's again it's not just the electoral votes. It's also these House seats. California would have lost House seats. I think it was like four if this had been recognized because again there's been a huge migration. Jake, you know this better than anybody since you're one of the people who've left. The state is so badly mismanaged that people have been fleeing blue states in droves. And the fact that you've got illegal aliens then replacing that population mass that effect and allows these blue states to maintain their level of representation in the house. That is a huge incentive. &gt;&gt; Freeberg, do you support the use of ICE agents DHS to go into these cities and do you think they were too violent? And do you agree with that strategy that Trump's now seems like pulling back? &gt;&gt; I mean, I know the answer. We talked about it on private chat. Yeah. &gt;&gt; Okay. I'll be very matter of fact. &gt;&gt; Yeah. &gt;&gt; Neither of these people should be dead. It's sad that it happened. &gt;&gt; Who's to blame? &gt;&gt; Neither of these people should have been doing what they were doing. Federal law enforcement agents should not wear masks. They should identify themselves when asked. Ideally, they wear body cams like local law enforcement does. I like body cams. I actually like watching videos of police wearing body cams on YouTube and Tik Tok. I think it's a very important way to hold law enforcement accountable to their work. There may be reasons why federal law enforcement can't do that. If they're undercover and whatnot, they shouldn't have to identify themselves, but I think those are important ground rules. I think that federal law enforcement should have warrant or probable cause. They should not be allowed to randomly ask people for papers. I don't agree with that. If that's what they're doing, and I'm not saying that that's what I agents are doing. &gt;&gt; No, no, that's what they were doing. That's all been proven. &gt;&gt; I don't know that. I don't know if that's been proven. &gt;&gt; Wearing masks, not identifying themselves. &gt;&gt; That's not what I'm talking about. Hold on a second. Hold on a second. There have been a lot of claims made that they're going around quote rounding people up asking for ID and papers. I don't know if that's true. I have personally not been convinced of that. You may have seen something else, but I've not been convinced of that. But if that is what they're doing, there is no probable cause and there's no warrant attached to that. That should not be allowed. I think the mechanism for fixing that, the mechanism for addressing that is number one to go to federal courts and file injunctions, which is actively happening through the legal process. And there's a due process that then would take place that would determine whether these agents are following the law and doing things by the rule of law or are not. And if they're not, injunctions will be imposed and they can't do what they're doing. Short of that, no one should take it into their own hands to obstruct law enforcement, federal or local law enforcement. I think that that is wrong. I don't think that individuals that disagree with a law or disagree with the actions of law enforcement officers should obstruct their actions. The correct path is number one to protest peacefully, number two to go to the courts and file injunctions, and number three to change the law. To go to your local, &gt;&gt; vote for someone and change the law. I think it's totally okay for protesters to not like the law. There's a million laws and regulations and other [ __ ] that I fundamentally disagree with, but those don't give me the right to impose myself physically in the driveway with a car blocking law enforcement from doing their job. If law enforcement was arresting a non-ilgal immigrant murderer and someone did that, would people be up in arms? It's because people disagree with the law. And if you disagree with the law, you've got to change the law. And that's okay. Now, right now, it is the case that it is the law that if someone came here without going through the immigration process as defined by the law, then they are technically here illegally. That is the law. That's what it is. And so, there is a course or a path or a point of view on how do you enforce that law and that is what people are having disagreements over. And again, file injunctions if you don't like the methods, change the law if you don't like the law. My personal view on immigration, just so I can wrap this all up, I don't see how we're going to do this in a humane and just way of removing people from this country who have been here for a period of time and have paid taxes and have been good contributors to this country. I don't know how you're going to do it. I don't know how you're going to do it without inciting a civil war. So I think the compromise has to be that there has to be a path to permanent residency and eventually citizenship for individuals who have been in the United States for some period of time who have followed other laws who have paid taxes and who have not taken advantage of public services that they are not supposed to use. With those conditions met there should be some path otherwise we're going to have civil war. I think that separate to this there's a bigger point to be made and Ray Dalio has made this and I will restate it and you guys can roll your eyes at this but I think that there's a deep emotional driver to all of this. People look at this and they get incited. They get activated emotionally. Everyone I speak to is activated emotionally over this issue and we have to ask ourselves the important question of why are people activated so emotionally right now? What is going on? What is the fundamental root cause of this? What is the fundamental root cause of this prediction model that Ray Dalio has talked about saying that there's a 35 to 40% chance of a civil war in the United States based on predictions that he made in 2020. And it is rooted fundamentally in the fact that everyone to some degree feels some amount of oppression right now. And this is a manifestation of that oppressed feeling. That oppression comes from the fact that the world is racing ahead and people are not a part of it. that people feel like they're being left behind and they see victims of that world and they want to act. And I think we need to pay attention to that and be very cognizant of it because I think fundamentally it is an inevitability that there will be a civil war if we don't recognize it and address it and find some paths of compromise to solve these problems. &gt;&gt; Okay, I'm done. &gt;&gt; Uh, superb. &gt;&gt; So, it ship it. Episode done. I mean, &gt;&gt; let's go home. &gt;&gt; So, yeah, I'm strongly in agreement with you. the police should um not be interfered with. I said that here and I said it on Twitter. You should stay home if you can't peacefully protest and also agree with you strongly. The police uh should be trained to deescalate, wear body cameras, not wear masks, be trained properly, all that stuff. But I I want to make three points here. And the first is, you know, President Trump has surrounded himself with a lot of really competent people, yourself included, David Rubio, Bessant, Lutnik, Kushner. We had a lot of them here. my pal Chris Wright, but he's also surrounded himself with very inexperienced sick offense that he picked based on their loyalty to him and that group needs to go because that group is sinking his second term. I'll put on that list Steven Miller, Christy Gnome, Cash Patel, and Pam Bondi. Uh these folks have been a disaster for Trump. They're not qualified to be in the positions they're in, and they have caused a lot of chaos. And back in August, I explained exactly how unpopular these, you know, ICE actions were. And I implored people, hey, stay home if you cannot be peaceful. If you want to record people, that's fine. But I predicted as well that somebody's going to get killed. And that these agents were acting just without training. They were not deescalating. And they were in fact provoking a lot of this which I think was part of the concept that Steven Miller had was to provoke these kind of reactions. Nick, you can pull up my first chart. You remember back in October was talking about Trump's sinking approval rating back when these ICE issues were happening. The Epstein files weren't being released. Then in November, I brought this up again. Trump's uh net approval rating hit 13%. And then same chart coming up now. He's uh 18%. What's And you know, a lot of people like to use this term taco. Trump always chicken out chickens out. I don't like when people use that because I think they're trying to goat him on. I think he needs to react to these plummeting ratings here. If you could play the Tillis quote, Nick, &gt;&gt; if I were in her position, I can't think of any point in pride over the last year. She's got to make her own decision or the president does. But she has taken this administration into the ground on an issue that we should own. We should own the issue of border security and immigration. But they have destroyed that for Republicans. Something that got the president elected. They have destroyed it through their incompetence. David Miller is in the same boat. this guy after doing the stupid comments he made about Greenland getting the president in a difficult circumstance is one of the people that came out publicly and said that this guy was a terrorist before he had even talked with anybody on the ground and that's clearly not the case now. So I mean it's just I mean Steven Miller never fails to live up to my expectations of incompetence. &gt;&gt; And here's Marowski uh with a similar opinion another Republican. &gt;&gt; Senator, do you have faith in Christine Noam as DHS secretary? I I've already made a statement on that. &gt;&gt; Oh, I I wasn't there for it. &gt;&gt; Yeah, I I said that I've lost confidence in her. &gt;&gt; Do you think that President Trump should remove her from the position? Do you think she should resign? &gt;&gt; Obviously, up to the president. Um I think we would be we would be better served with leadership. &gt;&gt; At the end of the day, Americans don't like to see this violence. They don't like cruelty. They don't like chaos. The reason Trump lost to Biden, who's not a very strong candidate, is because of the general chaos people felt with the immigrant ban the last time and people do want to see the border closed and that has had a great effect I think on the country and I think that's fantastic and he should take a victory lap for that. But leadership starts at the top. President Trump put Steven Miller in charge of all this. Uh I think he's a bad actor. I think these people who he hired were picked strictly to do lawfare and to do this kind of sadistic violent behavior to feed a MAGA base that's not going to keep Trump in office and in fact going to cause him to lose the second uh to lose the midterms. So the easy solution which I brought up here over and over again is if you want to stop having people come into this country, you have to look at why they're coming to this country. They're coming to this country because they want to have a better life. That's why they're coming here. And that's why they'll pay a coyote 10 or 20 or $30,000 and risk their own safety coming across that border where many people die and abused and are abused. So if you want to stop this, all you have to do is go to the business owners and find them. And if they keep hiring illegal aliens, you keep finding them and you put them in jail. But you don't see Steven Miller doing that. Why? Well, because a lot of those businesses are the voters who put Trump in office. They're Republicans. Yeah. Sure, there's Democrat owned business owners as well, but you can solve this whole problem without sending mass agents in to beat the [ __ ] out of people, to be violent, to provoke these kind of reactions. And that doesn't absolve people driving their cars into the police. That's horrible and it's a terrible tragedy. And these people, as I've said three or four times, uh should not go out and protest. But you could solve this. If there are no job opportunities for illegal aliens, they will stop coming here. finally agree with your point strongly, Freedberg, that we should be compassionate to the people who have been here 10, 20 years and paid their taxes. I've said this here many times. They should be given a path. America is a country built by immigrants for immigrants, including the three of you, my bestie immigrants. &gt;&gt; And we all came here illegally or our parents did &gt;&gt; and we didn't seek into the country. &gt;&gt; Yeah, absolutely. &gt;&gt; Now, you kind of gave this diet trial. I don't know why you've picked Steven Miller as the vessel of your hatred. It's like you've &gt;&gt; No, it's not hatred. I based on &gt;&gt; There's been a transference. There's been a transference of your &gt;&gt; trans just based on his TDS to Steven Miller derangement syndrome or something like that. &gt;&gt; If he if his actions were if his actions were kind and compassionate &gt;&gt; if if his if he was compassionate towards immigrants and uh recognize the importance of immigrants to this American story, I would be fine with it. It's just based on his behavior. David, I don't have TDS. You can name call all you want. I'm just basing it on your behavior. &gt;&gt; You've decided to do this ad homonym against him. And look, the big picture is that threat is basing it on their I'm basing it on their qualifications. &gt;&gt; You're not really presenting You're not really presenting evidence. You're showing what other people think. You're showing polling and then what a few senators think. So basically, you're trying to build a Hold on. You're Hold on. Let me finish my point. You're basically trying to build a case against him by using the opinions and declarations of other people rather than actually presenting evidence and building that case other than just a name call and say that he's not compassionate. The big picture is that I would say that Steven Miller has been more correct about immigration than you have been over the last several years. For years, you were denying that we had an open border problem, did you not? &gt;&gt; No. No. for one year when the spike went up I said let's get the data on this because it doesn't make sense that it would triple year-over-year and then we found out that it was in fact and with the new information I changed my opinion and said yeah this is obviously happening that was literally what happened &gt;&gt; well I don't know I mean during the buying years I remember we had a bunch of debates about this and you were saying &gt;&gt; my position was let's see the data cuz the data if you look I I I played this chart on the podcast many times we showed like the last four years and then there was a spike and I said this doesn't make any sense How did this spike happen? It's just my opinion. You don't have to have a you can have a difference of opinion. &gt;&gt; Let me go back to the origin of the chaos in Minnesota. You don't see this happening in other states. And I just want to point out one of the major reasons why. So in other states, you know, take your pick. When you have an illegal alien and they get arrested, they get handed over to ICE or Border Patrol for deportation. That's what's happening in Minnesota. the politicians have given an instruction not to do that. So, literally, you have criminals, people who've been arrested. There's a case that was just um posted today of an illegal alien in Minneapolis who killed an innocent mother and severely injured two others backto back in uh drunk driving crashes. He did not have a valid driver's license, did not have insurance. He was arrested for vehicular homicide. ICE asked him to be turned over to them and the authorities in Minneapolis refused to do that and they eventually released him. So that puts ICE in the position of having to go out and find these guys and arrest them. That's how all these operations started. And you make it sound like these guys are the Gestapo and they just randomly round people up. That's not what's happening. They have arrest warrants with the names of specific people that they're going after. There are arrest warrants, but there were also Steven Miller told them to just go round people up at specific locations. So, &gt;&gt; there is that as well. And they've been trying to hit numbers that they just can't possibly hit. You have to ask yourself &gt;&gt; why didn't we have you have to ask yourself, I think, Sax, why didn't we have this kind of violence and unmarked people, breaking people's rights under Obama who did far more? &gt;&gt; I'll tell you why. You want to know why? Because Obama said that we should deport. Ultimately, our nation, like all nations, has the right and obligation to control its borders and set laws for residency and citizenship. And no matter how decent they are, no matter their reasons. The 11 million who broke these laws should be held accountable. &gt;&gt; I'm saying he was able to do it without sending masked agents in to beat the &gt;&gt; No, the Democratic party changed. That's what happened 15 years ago. the position that Obama Can I just talking about the policing? You ask me a question. Why wasn't the policeing? Why didn't this happen under Obama? Let me tell you the reason why the policing happen years ago. The Democrat party was still somewhat rational on this issue and it was basically a bipartisan position to support deportations of illegal aliens and not to have an open border. The Democrat party changed their point of view on this issue. Now, &gt;&gt; that's not what I'm talking about policing. &gt;&gt; Let me finish my point. Tom Hman has said over and over again, we don't want to go out on the street. It's dangerous. They don't want to be put in this position. ICSE has an incredibly dangerous job to do. Border Patrol has an incredibly dangerous job to do. By the way, the reason why they wear masks is because they're getting doxed by these groups who are organizing with each other on signal and they're being followed around. They're being stalked. They're being chased to their hotels and these guys follow them around. They block their cars by um interposing vehicles. They're interfering in in official operations. They're interfering with arrest and they're getting in physical altercations with them. This is very dangerous for eyes. They don't want to be in this position. But the reason why all of this is happening is because Tim Waltz and Jacob Fry and the rest of these guys, Lieutenant Governor, they are saying, &quot;Do not cooperate with ICE. Do not cooperate with the border patrol.&quot; Telling local authorities in Minnesota, when you arrest an illegal alien, do not turn them over to ICE. It's better to release them. These people are such anti-ICE zealots that they would rather release a killer onto the street than keep them in jail where they can be turned over to ICE. And that's the whole reason why they don't want to keep them in jail is because that would create a central repository, if you will, for ICE to go collect all these illegal aliens. Last year there was something like 470 convicts, okay? Illegal aliens, criminals. And I'm not talking about that their crime wasn't breaking into the country. I'm talking about subsequent crimes that were released in Minnesota so they they cannot be rounded up by ICE and deported. These people are zealots and that is the source of the problem. And then on top of that, you've got rhetoric by people like Tim Walls and the lieutenant governor calling ICE and Border Patrol agents who are just trying to do their jobs the Gestapo and Nazis and riing people up. And I think that people like Alex Pretty or Renee Good, they're victims of this type of rhetoric. These are left-wing activists who are in this leftwing echo chamber and they're baronating in all this rhetoric that's portraying law enforcement officials as Nazis and they're embibing this constantly to the point where they're full of rage and anger. You see that video of Alex Pretty attacking Border Patrol. This is two weeks before he gets killed. He's kicking the car. He's like foaming at the mouth. He's in some sort of rage. This is someone who's emotionally disturbed and he's got a a gun on his waistband, which look, every gun owner knows that you have to be exceedingly careful if you are carrying a weapon and then you are dealing with law enforcement. The last thing you want to do is go out looking for an altercation with them, which is what he did. And by the way, I think it's completely tragic that the two of them were killed. I don't want to see that happen. But again, they are victims of this political environment, this chaos that's being whipped up by the elected officials in Minnesota. This would not be happening if they simply turned over the illegal aliens in their custody to ICE and Border Patrol, which is all that they're asking for. &gt;&gt; I've said on this podcast like many times, but none of you will respond to it. Why are they doing this instead of just finding the business owners? Wouldn't that be a better policy? And you're in the White House, you're around these people. Why don't you tell them why don't we do that? Why don't we just give fines to business owners who hire illegals and stop the incentive to come here? &gt;&gt; How do you know that they hired illegals? How would you know that? And if you know that, then you know the individual. &gt;&gt; It's super simple. You would just go do uh a little investigation. You go to the car wash, you videotape it like any other detective or any other federal agent or a local police officer would, and then you track those people down. You very quietly go pick one or two of them up on the way home. You ask them for their papers or you go to the owner and say, &quot;Hey, we have these six people. Here's their pictures. Here's the pictures of the six people. Show us their papers. Show us their social security number.&quot; That's all you have to do. Freeberg, &gt;&gt; here's my here's my here's my proposal. Okay, Jacob, you may have a point there, Jacob. You may have a point, but I'm not sure &gt;&gt; that it's good enough in the case of an illegal alien who's already been in this country committing crimes. Okay. Yeah. So, if you have if you hold on, if you have a drunk driver &gt;&gt; who has killed people through drunk driving and they're they're captured, I think they should be turned over and deported immediately. &gt;&gt; But if you want to do big numbers, like big numbers, you could just go to a farm, you could go to a car wash, you could go to a restaurant, you take pictures of everybody coming in and out for their shift, you go to the business owner and say, &quot;We have these people. They can you show us their uh paychecks and their pay stubs?&quot; &gt;&gt; All right. So, we'll agree to disagree on some of this and uh we'll agree that this is a tragedy and that we need leadership to calm these things down. Uh and we need to agree on a very reasonable immigration policy which is not what we have right now in my opinion. All right. Claudebot has gone viral overnight. I have been claed. I have been one claude. I am all in on this. Um this is an open-source project created by a gentleman named Peter Steinberger. He's an Austrian developer and entrepreneur. So, what is it? It's uh basically an open-source personal assistant. Uh think like Siri or maybe Jarvis. Have any of you guys used it yet? Has anybody installed it yet? &gt;&gt; I did. I spent 15 minutes and I saved 15% on my car insurance. &gt;&gt; Are you joking? Sure. You installed it and then asked it to go. &gt;&gt; You know what's so funny, Nick? I posted this. The number of people that didn't understand that that was a joke. &gt;&gt; No, I understand it's a joke. Yeah, &gt;&gt; I know you did. Yeah. Yeah. &gt;&gt; But there was a bunch of people that were like, &quot;Really? How did you do it? Can I do it?&quot; And then some people were like, &quot;Wait, only 15%.&quot; And other people were like, &quot;Wait, you set it up yourself. You didn't have somebody help you.&quot; &gt;&gt; This is the great This is This is literally the greatest thing ever. Have you installed it yet, Freedberg? &gt;&gt; No, I don't want to give &gt;&gt; Okay. &gt;&gt; an open source tool access to all my emails and messages. &gt;&gt; Have you done it yet? &gt;&gt; No, because I'm concerned about the security issues. But I want to I want to do it, but I'm concerned about the security. &gt;&gt; I've spent the last 72 hours doing this. I'm going to explain to you what we did at the company. It's mind-blowing. Okay, so it is basically think like an open- source Siri or Jarvis, right? You get into an interface and you can talk to a virtual assistant and it does things for you just like chat GPT or XAI might. But the way it works is you kind of load a virtual machine if you know what that is or you can put it on your Mac Mini. You run this like a server. Then you start authenticating it with your services. So, Gmail, Notion, Slack, WhatsApp, maybe even your password manager. Super dangerous. Nobody like Claude or XAI or Microsoft would ever allow you to do this because it's so dangerous, obviously. &gt;&gt; But we did this and we put it to work. Here's what we did. I created a virtual podcast producer and we made this persona and we created a new Gmail account, Sachs, a new notion account, a new WhatsApp account, everything. We created like basically an a virtual employee. We put it together and we made it the producer for my new This Week in AI podcast. There's a little plug in there for it. So, we had it start doing research on guests. So, we said, &quot;Hey, research guests, right?&quot; just like you can do in an LLM. Then we connected it to um the existing feeds like the podcast feeds and the database of like people we've had on the show and like who's booked. Then we said make a CRM Sachs for all potential guests and suggest other guests and it vibe coded a CRM for itself. &gt;&gt; Can you show this? &gt;&gt; I said hey do some research on this. Right. So then we start a thread with it with the actual producer Oliver who's working on this. So I said, &quot;Do this guest research.&quot; It did all this guest research. Now, this is as good as like Nick or Lisa would do at a first pass, but it did it instantly. And then I made a prompt for it to like, and I was using this prompt, by the way, David, when we were in Davos for our guests. And so this like gets the company's name, the founder information, you know, look for their competitors, all the stuff that a producer would do over a day or two. Do a timeline, give suggested questions, etc. So I teach it this. So, here's producer X and it says, &quot;Got it. That's what uh that's a great guest research.&quot; And I said, &quot;Okay, do the guest research on this person.&quot; It comes back with that. And it gave me its media appearances, all the stuff I would want to do for research. Then I said, &quot;Um, uh, I think this would be &gt;&gt; You're interacting with it like an agent.&quot; So, how is it different than an agent? &gt;&gt; Watch. Okay, we're getting to it. So, then I said, um, producer X, email Alex and let him know that I want to have him on this weekend in AI. CC me and CC Oliver. And it says, uh, I couldn't verify the email, but I'm going to try this one. And it wrote this email. It sent the email. Then after it sent it, &gt;&gt; did he respond? &gt;&gt; So here's the email to the guest. &gt;&gt; Oh my god. &gt;&gt; Hey, Alexis wanted me to reach out. He'd love to have you on this weekend to discuss ExoLabs, your work on distributed inference on the recent he like did this like little thing. Let me know if you'd be interested. &gt;&gt; So then &gt;&gt; it's [ __ ] crazy. He says, &gt;&gt; this guy says, &quot;Sounds great. I'll be in touch.&quot; &gt;&gt; Then I said, &quot;Here's what I want you to do. I want you,&quot; and we created a group called replicants. So now we've made five replicants at the company. And today is like replicant day or replicant week. So I told it every time you do work for me in the replicant room, say what you did and then also put it on your calendar. So now it's putting on its calendar what it did in timestamps. So he said I did some guest research here and then yesterday I did this booking report and that I'm doing this AI around it that I'm doing this &gt;&gt; and then I set up a and then producer lawn had it set up a ticker because we did this email ticker and did all that work &gt;&gt; and the crazy thing is you can start talking to it directly so I can have a direct conversation with producer X and it learns and now it knows everything. So we started one to be an SDR and then we are giving it access to our CRM there. But it built its own CRM, David. It's building its own SAS tools to solve its problems. Every time you add it to something, it gets smarter. So now we've we're making a LinkedIn for this persona. &gt;&gt; The LinkedIn is going to start adding people. People don't know that these aren't humans. &gt;&gt; Yeah, &gt;&gt; it's insane. &gt;&gt; All right, let me give you a few thoughts. So, I was paying close attention to the whole Claudebot freakout, you know, over the weekend. And, you know, again, I wanted to set it up really badly, but I was too afraid of the security issues to do it, but I was watching everyone else do it. And look, I think the takeaways for me are number one that this will be the year of the rise of the personal AI assistant. Until now, AI has mainly come in the form factor of a chatbot and it's been used as a research tool. Some people have &gt;&gt; search. Yeah, &gt;&gt; it's better search. There's kind of this niche case of, you know, fantasy chat bots where people actually like somehow talking to a chatbot, but really it's about better web search and it's about research. Now, we're moving to a completely different form factor, which is again this AI assistant that's an agent that can do things for you and it's going to get better and better at doing different tasks for you. Everyone's going to have this amazing AI agent. Now, you know who who is the beneficiary of this market opportunity? I mean there's obviously going to be some startups like the guy who's doing Claudebot which I guess he renamed it was it moldbot or &gt;&gt; it's called Maltbot now because Claude got upset &gt;&gt; right so in any event it's Maltbot now but I think it's a tremendous opportunity for Google because the question is who has your data Google has all my email my calendar my documents which is exactly the stuff that I would want to integrate with Cloudbot if I believed it was safe obviously I've already made the determination that Google is safe because they already have all my data so I think Google is in a tremendous position to offer a personal AI assistant that's connected with your email and calendar for those of us who are using G Suite and so on. Obviously, there'll be opportunities for other companies as well, but I think this is going to be such a big product that it may become the dominant form factor of AI, meaning more popular than the the sort of researchoriented chatbot. Speaking from the policy point of view, I think it's going to change the policy debate around AI because so much of the policy debate over AI is about fighting the last set of battles around social media &gt;&gt; question and answer modality. &gt;&gt; It's sort of treating AI as like a form of social media. I mean, I hear policy makers say all the time, what are you going to do to protect kids against predators? And it's like, well, wait, what predators are you talking about? This is not social media, right? I mean, I understand if you want to protect kids against, I don't know, a fantasy chatbot that's recommending they do bad things, but it's just it's very different, right? This is just a completely different modality. And I think that the rise of agents will make that clear that we're dealing with something that's not social media. Maybe there's some analogies to the previous set of policy battles that were fought over social media, but I think it's quite different. So, I think Claudebot is a breakthrough. I think it's a really interesting proof of concept and we'll also have to see exactly who who the big winners are here. &gt;&gt; I think what's interesting is that if I said to you, hey, there's going to be an AI personal assistant. You would have some point of view in your mind on what that means. It's like, oh, it can, you know, change my calendar, tell me about my travel, read my email for me, summarize that that kind of stuff. But what I think people don't realize, it's almost like the first time you use FSD in your Tesla or the first time you used an iPhone, you realize that it's so much more that it widens the aperture of what's possible that it's not just the assistant in the way that you might otherwise be thinking about it, but it's like this super worker. And the super worker, like to Jal's point, it it does both scheduling, calendaring, ideation, knowledge work, creates new code, creates CRM tools, books your travel, it does everything. And then if you start thinking about having an army of super workers, you're like, &quot;Oh my god, what's possible?&quot; And now think about what Elon's doing. Yesterday, Elon shut down the Model S and Model X production lines in Fremont. &gt;&gt; And by the way, you know, a year ago, I'd never owned a Tesla. And I was so blown away with FSD that in the last week, we bought two more Model X's. So now everyone in my family, we're all on Tesla because the experience is so good. But they're shutting down the Model S, Model X lines to make Optimus. So if you've got the same thing that Jal's talking about and experiencing, but you've actually got it in physical form in addition to digital form, that's the future where everyone has a super workforce and you put two optimists in your garage and they build and run a business for you and then everyone becomes an entrepreneur. &gt;&gt; What was my prediction, Jason? What of my prediction? &gt;&gt; Oh, that they would merge all these companies and then this week &gt;&gt; it's happening. &gt;&gt; It's happening apparently. Um or I shouldn't say it's happening. We don't know it's happening, but beep &gt;&gt; boop. Between this and my copper prediction, &gt;&gt; I'm about to retire. &gt;&gt; You know, copper's up 26% in a month. &gt;&gt; What is that? Is an annualized. &gt;&gt; Check this out. S &gt;&gt; I we invited somebody from Craft Ventures on and I forgot who's from Craft Ventures who's going to be on this week in AI. So, I said, &quot;Can you tell me who's doing it?&quot; And it said, &quot;Oh, I couldn't find it.&quot; I said, &quot;Oh, it's in the notion database.&quot; So, it goes and it found it. Oh, Mike Robinson from Craft Ventures is on February 24th. And then there's also Brian from Craft or whatever. And he was on a different liquidity round table. And I said, &quot;Hey, can you email Mike and I want to do a pre-show call with him?&quot; Guessed his email, sent it. I said, &quot;C Heidi, I haven't given him an access to my stuff.&quot; And it said, &quot;Oh, do you want to get his top three times, all this stuff?&quot; It's crazy what's going on here, folks. I would say out of 50 hours a producer does a week, this does 40 of them. And of what an SDR does, this does 95%. This is going to be crazy because it keeps learning. Now, your API bill is going to be nuts. The first day we did this, we hit like a $100. I think it's going to be like $1,000 a day in API calls. So, my team's like, &quot;Hey, this is a lot of API calls because people are going crazy inside Slack and making all of these agents.&quot; &gt;&gt; Good news, Kimmy 2.5. Yeah. &gt;&gt; Uh we are ordering Mac um studios, the really powerful studios with like max memory on and they're putting Kimmy. Is that the new open source one? &gt;&gt; Yeah, we're putting Kimmy on them. We're going to have free free for and then it according to people online like 95% of all these queries can be done for free with Kimmy. &gt;&gt; You'll save 90%. &gt;&gt; It's crazy. &gt;&gt; It's really crazy. &gt;&gt; This is independent by the way of whatever LLM you want to use. You can swap out LLMs. You could route it to different LLMs. You can do whatever you want with it. I tried to have it create Reddit accounts for me. Like, I want a Reddit account to go do research. And it's like, I can't do that. It's against the term of service. I don't think people understand how important this Kimmy K2.5 moment is. &gt;&gt; Wait, what is Kimmy K2? &gt;&gt; Just to set some context, right? So, I think the last few years we've all kind of lived in a world of what I would call blackbox AI, meaning you go to your favorite chatbot, basically, you put in a prompt and you press enter, right? And all of those things go to proprietary models and they're excellent. OpenAI has some, Anthropic has some, Google has some, and they all give you back an answer. Super powerful. But the important thing that we don't know to care about right now is that all of that stuff is gated. What does that mean? You don't own the keys. You don't own the blueprints. You have no idea what's actually going on. And what Claudebot demonstrated this week is you're one terms of service update away from everything breaking, right? Because at one point, Anthropic didn't like what was going on and they said, &quot;No, this is not allowed.&quot; So, we've talked about this battle between open source and closed source. So all of the models that that have been winning, the blackbox models are closed source models. But open source is important because it's transparent. It gives everybody their own sovereignty whether you're a company and frankly really more importantly whether you're a country. It gives you control of your own speed. It gives you a lot of execution control. You can audit the weights of the models. It allows you to host it on your own hardware. And the most important thing is the data never leaves your control. So that's why open source was really important, but it was always kind of like an underdog and it wasn't particularly good. So this week you wake up, you go to the office and Kim K 2.5 is important. So this is why it's so important because it was incredibly profound. It's a trillion parameter mixture of expert model. When you farm out work, the proprietary models keep that agentic layer kind of secret. Kimmy Cape 2.5 was like okay look here's this thing called agent swarm it's a technology that we built that's also now public and it allows you to create all you know 100 sub aents and what that allows you to do is basically solve any complicated multi-step problem in parallel so I think this is the moment now if I had to make a prediction I think there is the clear shot across the bow of closed source and I think open source can Why? &gt;&gt; Because when Kimmy K2.5 is accessible, &gt;&gt; it democratizes something this trillion parameter reasoning &gt;&gt; that right now you could not otherwise get. Now you can do vision to code, you can do massive context windows. It's really unbelievable and it's available to everybody. &gt;&gt; And to just build on it, you're exactly correct and I think it's a very big deal. You can run it. This is the other thing. What What have the two limiting factors been here? &gt;&gt; I'm not a big fan of like these locally hosted models. I think it's all [ __ ] and janky. It's all like kids mucking around. &gt;&gt; I think running these things locally are stupid and janky. &gt;&gt; The whole point of open source is to go and take them into these huge data centers is to use nextgen silicon. And we talked about this last week or the week before where again post Grock what I think will happen is you're going to see an explosion of decode silicon. If you take these next generation systems and you marry them to open source you're going to cut the cost of AI by 90%. And when you do that you know Jason your bill is going to be 10 bucks a day. &gt;&gt; Yeah. &gt;&gt; And you're just not going to stop. Here's the interesting thing for me like back to this like running it like there are now people who figured out how to daisy chain the Mac studios. So people are you can see there's two stack there. People are starting to stack these. This is commodity hardware running open source. The there's two advantages to this one. We have control of all the data. We have it on our own hardware. We can run it infinitely. And these things are only getting better. the M5 chips coming to these Mac studios and if you want more power you stack it up and then the open source models are getting easier. So what's going to happen is 90% or 95% of our jobs are going to go to this local hardware will control it. We don't have to worry about our information going up to Samman I canled I canled all of my OpenAI accounts. $25,000 gone. It's a matter of time until the big model makers create an incremental revenue stream for guys like you, Jason, to license back all your prompt and response data. And you'll probably make enough to pay for the all the costs of hosting and running these models. Anyways, &gt;&gt; yeah. Anyway, I this is like feels like &gt;&gt; it's a very big moment. &gt;&gt; It's a big mo it feels like a very big moment. And then, you know, let alone if when do you think freeberg we get it on here? When do we get it on our phones? when we'll be running Kimmy. &gt;&gt; We're not even talking about a lot of the architectural changes that are happening that we've talked about in the past. There papers that indicate we could probably go down by 70 to 100x in terms of compute need in how the the model actually runs. So yeah, ultimately these things end up on the on the iPhone running locally and you don't need to go to the cloud. By the way, just having our nation's AIS are on the line here, I think this has a dramatic effect. If you play all of these paps out on the way a lot of these states have written their idiotic legislation where those those legislative approaches encompass strictly the view of AI being a chatbot interface run by a single company running a model in a data center. And if all of the models end up running locally on machines in open source, different contexts, different use cases, all of like we said from the beginning, all of the [ __ ] idiotic, dumbass takes by legislators on what they think they need to regulate and how go out the window or they create a lot of confusion on what's actually going on in the real world and it puts us at risk. I strongly endorse the effort by those in the administration to pass a federal AI preeemption law that avoids all of this nonsense in the states and the local governments. And I think that this evolution of AI from being centrally hosted in data centers by closed models through chatbot interfaces. All of those layers break. You start to recognize very quickly why you need to have federal preeemption on this stuff because people get way too ahead of it and it's going to limit innovation. &gt;&gt; Yeah. Did you just Well, obviously I agree I agree with that. And again, just to build on that point, you can't underestimate how much the policy debate in Washington has indexed on a single use case. &gt;&gt; That's what I'm saying. &gt;&gt; Which was a niche fantasy chatbot application, &gt;&gt; right? &gt;&gt; Yeah. That's all that people know. Yeah. That's all they know. &gt;&gt; That turned into horror stories. I mean, legit horror stories. But &gt;&gt; again, that is just not the predominant use of AI. And it's certainly not going to be in a year. &gt;&gt; One way to think about it is imagine if the internet came out and the only thing that happened on the internet for the first three years was like pornography websites and like just like and then people are like, &quot;Okay, we got to regulate the internet.&quot; And it's like, &quot;Hold on a second. This is an idiotic use case. Perhaps we should take a zoom out and think about what the internet could enable and all of the other use cases.&quot; And legislation needs to be crafted with a bigger view. And perhaps it's a little too early to make those judgment calls. &gt;&gt; Yeah. You got to wonder what impact this is going to have on the valuations of these companies. I mean, if you can run this stuff locally on your own hardware, you can bracket or put it in a virtual machine and open source wins, it's going to change the economics of everything. And the people I'm talking to in startups, which is where you always see the most efficient use of technology, they're all using Kimmy and what was the other? &gt;&gt; Let me ask you a question about that actually, Chimoth. So Kimmy K2 powerful model but it is a Chinese model or at least it originates in China obviously once it's open source people can fork it and make their own but does that concern you particularly around the use case of coding because what if there was a secret prompt in the model &gt;&gt; zero day attack or something &gt;&gt; well it's like something built into the model that's secret that could inject corrupted code and you know so much code's now being written by these models in such volume that I mean humans in the beginning were checking it all. But now even the founder of Cloudbot said he he doesn't even check all the code because he can't personally supervise all of it. So as AI coding becomes a bigger and bigger percentage. Right now it's probably what like 50%. As it goes to 99% no one's even checking it. You have to really worry about prompt injections. &gt;&gt; Exactly. &gt;&gt; Code injections. &gt;&gt; I think you're bringing up an important point. Right now we overly rely on eval to tell us how good a model is and I don't think we've developed a standard to I mean the the big model companies do it internally but the safety teams who are responsible for red teaming these models don't really work as a broad coalition. Everybody has their own version of what they do to make sure that their own models are good and performant. I think that there's an opportunity because somebody has to be able to take an open source model. Let's just say you're France and you're like, &quot;Wow, where am I in this whole AI race? I'm nowhere. We have a bunch of applications that we want to develop and we need our own sovereign AI stack.&quot; And so, okay, we'll take Kimmy K 2.5. What do they do to your point Sachs to get complete assurance that that model is reliable and safe under all weather conditions? Honestly, the answer is I don't know what they would do right now. But that's the opportunity because somebody has to be able to say, &quot;Okay, look, we're going to sandbox it in the following way. We're going to run it under all these race conditions. We're going to get to all these corner cases so that we can tell you that it's actually good to go and then you can use it.&quot; But how long will that take? And then by that time, are we on Kimmy K3? And then what is France supposed to do? I don't know. So these are complicated questions. But yeah, we do need an entire reimagining of how you red team some of these open source models, obviously. &gt;&gt; And there's open source models being made here in the US too, by the way. &gt;&gt; Exactly. No, no. Meta is doing them. &gt;&gt; Wherever it comes from, &gt;&gt; you're going to need AI models to look for corrupted code and to do the security eval. And there's going to have to be continuous monitoring. The good thing about open source is that when one person discovers a bug or a flaw or whatever, they share it with the community and then it gets patched globally. &gt;&gt; The the other problem with open source though right now is you can't really fork it and make it your own. Why? Because there's so much investment by Moonshot. That's the company that makes Kim K2 in that example that you'll have so much drift in one version your fork will be worthless. So why would you do it? So again, it goes back to sachs. We have to be able to say okay Moonshot will provide Kimmy K2 and every update thereafter but now we need to stress test it and we need to redte team it and we need to be able to say that this thing is bulletproof and right now there is no clean way of going to a third party vendor to do that in a quick reliable cycle and that's a business opportunity for somebody just speaking of the business opportunity I think there are a number of American &gt;&gt; AI companies that are working on open source models it is a gap that we have &gt;&gt; so and that is an opportunity I mean, look, if you're running critical infrastructure, I don't think you want to be using a Chinese model, period, on that critical infrastructure. &gt;&gt; I think if you can get it validated by an American company that's trustworthy, then it's no longer really a Chinese model. Chinese contributed some really great ideas and now there's a branch that you can use. I think it's just hard to ignore how good this stuff is now. &gt;&gt; It feels like this was a turning point this week. Okay, I want to get to one more important story. But dollar has dropped as gold and silver and copper have ripped. Dollar index is down 10% in the last year. Hit its lowest level in four years on Tuesday. Trump was asked if the dollar declined too much. His quote, &quot;No, I think it's great.&quot; Wall Street thinks Trump wants a weaker dollar to boost US manufacturing and exports. Obviously, we have a weaker dollar. That means the stuff from the US is cheaper. Foreign stuff becomes more expensive. Uh and um we have a situation here, Freeberg, that you've talked about. Money printing has increased to $2.5 trillion a year. Trump wants to print an additional 500 billion more. That would bring us close to 3 trillion uh for the military and uh money has poured into gold and silver which have way outperformed the S&amp;P. Shout out Vinnie Lingham. Freeberg, your thoughts on dollar devaluation and what we're seeing? Yeah. So, people talk about the market going up, but I'll use an analogy. If you live on an island and there's two huts on the island and there's a bunch of shells that people are using for trade, each house is going to be worth a certain number of shells. And then if people went and found a whole bunch of more shells, the price per house would go up in number of shells. But there's just more shells in in the supply. And effectively, you've inflated everything. And that's effectively what's gone on with the US fiscal condition. We've talked about this many times, but I think it's always worth a rehash. In a democracy like we have for the past 250 years, without adequate constitutional constraints, it has always been the case that over time spending goes up. Government spending goes up. And this is because in a democracy, people ask for their government to do more every year. And as they ask for their government to do more every year, the government agents who are elected say, &quot;Okay, here you go.&quot; And they spend more. Eventually, when the borrowing capacity gets unlocked, which is what happened in the United States when we went off the gold standard, you borrow like crazy. You print money to to fund those borrowing costs and you use that fundamentally to drive the next voting cycle, which is to give people more and more of what they want. But eventually, the bill comes to you and in the United States, the bill is coming to you. Let's start by looking at the money supply chart. This is the M2 money supply chart, showing the rapid rise in dollars in supply as a function of the central bank of the United States. the Federal Reserve making loans to banks ultimately to fund federal spending. I mean really an extraordinary number. And if you look at the M2 money supply chart going back to 1960, 1955 and you can see postcoid we were hoping that we would have resolved and sort of reduced the money supply by some amount but co really created this accelerating mechanism and you know we're back on track in the last couple of years to increasing the money supply. And so over time the the US dollar gets devalued as there are simply more dollars in the market and US treasuries gets challenged. So if we take a look here around the world central banks have decided that they no longer want to hold US treasuries. And so this is the value of gold versus treasuries in central banks in their inventory. So we are now seeing that for the first time in history or &gt;&gt; no no hold on that's not accurate. It's not like they're selling. This line just shows it's stable per se, right? It's more that the incremental buying is in real assets. &gt;&gt; Yeah. But this dollar the dollar value is also adjusted. So fundamentally I mean one way to think about this is the relative value of central bank holdings around the world. We now see gold eclipsing US treasuries. So now gold is a larger share of the holdings. Yeah. So now gold is the largest share of the holdings of central banks. If you look at the next chart which is just over the past year as JCL pointed out this is the dollar index. So it's the dollar against a basket of foreign currencies has declined you know from a index of about call it 109 down to 96 today. This chart actually looks at so what is the US stock market trading at and instead of trading it in US dollars what if you just looked at the US stock market the total value in ounces of gold. And so if we had the gold standard still and if we functionally converted stock market value from dollars back into gold, you can see that the stock market in the United States over the past years, so this is about seven and a half years going back to the pre-COVID era, is actually down, down pretty substantively from the pre-COVID era. So stock markets are fundamentally down. Everyone's cheering, clapping, bouncing up and down. Stock markets are up. Stock markets are up. And I'm going to tell you why this is important in a minute. Uh, and everyone's, you know, jumping up and down saying, &quot;Great, the stock market's up.&quot; The stock market's up in dollar denominated terms. But if you look at the stock market relative to gold, it's actually down. And the sell-off is not just in the stock market relative to gold, but you can actually look importantly at the metric that we all should care the most about, which is US Treasury yield. So, this is the 30-year. So, the 30-year yield is now at 4.9%. The average US government's cost to borrow today is 3.3%. So if we end up needing to roll all of the US government debt, assuming we take on no new debt, which we know is not the case, $39 trillion of debt outstanding, the federal government level today, and it had to get refinanced at this rate, we would have an incremental annual cost to service the debt, just the interest on the existing debt, of roughly $700 billion a year. incremental cost to service existing debt as interest rates climb from 3.3 to 5%. And so fundamentally, this is about 70% of the current defense budget. It's about 10% of the overall federal budget. It's a significant percentage of US GDP, about 3% of US GDP. It's a substantial number and it creates the spiraling problem that we're in. Now, I just want to make one final point. So, there's this dolorization moment. It's always worth having a reflection on it, but I just want to tie it back to Minnesota, Donald Trump, and socialism. And I think it's important for us to just highlight that if you own assets like we do, the four of us, we own stocks, we own real estate, we own other assets. As the dollar devalues and everything inflates in value, our asset prices go up and we get wealthier and wealthier and wealthier. The majority of Americans do not own assets. They are net asset negative. As a result, they live off of income and they do not benefit from the ddollarization like asset holders do. And this is what is ultimately fueling populism in the United States. And the populism in the United States is what is driving socialism. And the response to those behaviors is what Donald Trump elected to some degree. and the response to the Donald Trump actions is what's driving the civil unrest in Minnesota and other places. And I fundamentally believe that much of the unrest, the civil unrest, and ultimately this divide in this country is driven by the fact that ddollarization because of excess government spending ultimately leads a majority of people in this country to feeling oppressed and left behind because they're seeing a few people in the country accelerate their net worth like all of us here. And there's no way for them to catch up because they don't actually own assets. So, I'll be I'll I'll be honest with you guys and make a confession. I was kind of at I was at the gym this morning on the treadmill. &gt;&gt; You were at the gym. &gt;&gt; Yeah. And while I was there, I was actually thinking about the wealth tax stuff that's going on in California. And I I wonder if it may be an inevitability in order to keep the United States from going into civil war. I mean that very wholeheartedly. Like I just don't know if there's a way of solving this fiscal problem without a functional redistribution of wealth. And the question is can you do it violently or nonviolently? And if there's a nonviolent path, I think that's probably the preferable path. &gt;&gt; Did you ever think about do you ever think about violently picking up some of those weights? We can leave that that in or take it out. &gt;&gt; The problem with that is look, you know, where this California wealth tax is going, right? It's not going to the quote unquote people. It's going to these special interests who've been looting the state for decades. &gt;&gt; Audit everything before you raise taxes. It's very simple, folks. Audit everything. &gt;&gt; I mean, if the money is going to waste, foreign abuse, and special interest, then how do you solve the divide problem? I mean, I guess the ones that the special interests are capable of organizing are able to extract, but doesn't actually solve the problem. In fact, everything gets worse because &gt;&gt; yeah, &gt;&gt; those government special interests generally rig the system in their favor in a way that actually raises the cost for everybody. &gt;&gt; So you look at California, everything's performing worse. &gt;&gt; Can we pour one out for David Freeberg's favorite government program? &gt;&gt; Free beer. &gt;&gt; Free beer. &gt;&gt; Free homeless for 55 homeless people suffering from the shakes. And they spent $6 million on it, but it's over. 6 million a year. 6 million a year for 60 bo hobos to get beer because they had the shakes. The dream is over. The dream is over. Freeberg. No more free beer. &gt;&gt; By the way, I think Sax's point is the right point, which is the resolution to this isn't a fair and reasonable redistribution of assets. It's fundamentally a moment of extraordinary theft. When there's this massive movement of capital like this through a centralized system like the government, there's no free market transition of capital and as a result you end up most seeing a large percentage of it go into into theft back into the hands of a few who were really good at capturing that money as it comes out of the government's coffers. That's a good point. &gt;&gt; You've diagnosed this many times. I mean, look, Texas and Florida do a better job for their population collecting half as much in taxes per capita as California does. And having no income tax or capital gains tax, &gt;&gt; that's because if you commit fraud here, we put you in a firing squad. That's how it works. Right to the firing squad. We're going to go to the range. Let's go to the Come by Sachs. Come by the ranch. I have a shooting range. &gt;&gt; Matt Matt Mayan entered the California guminatorial race. He's running for governor. &gt;&gt; He's running California. &gt;&gt; Yeah. &gt;&gt; Uh what does it mean? Who is he? Explain to the audience why this is important. &gt;&gt; Well, he's the mayor of San Jose. &gt;&gt; Chats are on fire. &gt;&gt; Yeah. He's much more of a moderate and he's not a union captured candidate. &gt;&gt; Oh, here's your poly. &gt;&gt; Yeah, here's poly market. So, Matt Mayan announced this morning very late entry to the gubernatorial race. Katie Porter is kind of the, you know, the output of the Democratic machine. Tommy Styer is the billionaire climate change advocate. But Swallwell, you know, the congressman from the east, &gt;&gt; hasn't met a virtue. He doesn't want a signal. &gt;&gt; He's literally like Christmas lights blinking. &gt;&gt; Tom St. &gt;&gt; Well, look, if if if Maym, is that the right way to pronounce his name? &gt;&gt; Yeah. Matt Mayan. &gt;&gt; Matt Mayan. California has a jungle primary, right? So, everyone's running at the same time. There's no separate lanes for Democrats or Republicans. If he ends up top two, let's say it's him and Swallwell, I think he'll win because all the Republicans will vote for him. They'll go for the more centrist candidate. &gt;&gt; We had a political strategist over for dinner. There's a version where the top two people could be both Republicans actually right now. Current course and speed, the top two vote getters are trending to be both Republican. &gt;&gt; Yeah. &gt;&gt; I like the Karen. The Karen's the most entertaining. She's the one who said, &quot;Get out of my shot.&quot; That's the one. &gt;&gt; Katy Porter. &gt;&gt; The get out of my shot one. &gt;&gt; Yeah. She's tumbling. She's tumbling. Yeah, she's not doing well. She's coming down in the she's not likable and she seems and I think everyone sees right through her as being, you know, effectively captured by California institutions, &gt;&gt; but Maym and Swallwell much more independent, but I think it's going to be a battle between the two of them. &gt;&gt; Just to go back to your point, Jamas, so what's interesting is there are two Republicans running for governor of California. Chad Biano, who's a sheriff, and then Steve &gt;&gt; Steve Hilton, &gt;&gt; who's a political commentator, used to be on Fox News. They're both at about 15%. Which actually puts them in the lead or close to the lead because the field's so fragmented right now. Exactly. The problem that they have is so I guess if the field stayed permanently fragmented, yes, they could be in the &gt;&gt; Could you imagine the melting mines? &gt;&gt; Yeah. But that's not going to happen as much as I would like it to because the Democratic field's going to coalesce and if it doesn't coales the the party machine will get together and they'll tell a bunch of people to drop out and get real &gt;&gt; and they'll they'll basically shift it. So the problem with the Republicans is and it actually be better if there was one Republican instead of two because they need to be at like 30%. Right. &gt;&gt; Right. &gt;&gt; But you know they're each going to get 15%. They they have more of a ceiling is what I'm trying to say. Right. &gt;&gt; Yeah. That's true. That is true. &gt;&gt; Yeah. It's only the final two that make it into the general. So, if the rest of the Dems literally stay equal and there's five Dems that are at like 5 to 10% each, you could end up seeing the two Republicans in the top two spots. &gt;&gt; I don't think that's going to happen. I don't think you're going to end up with five Dems at at 10%. I think what could happen though actually is it's probably better that you have two Republicans rather than one because the disaster would be let's say you end up with Swallwell and one of the Republicans in the runoff, then Swallwell definitely wins. That's what always happens, right? &gt;&gt; The best chance for Matt Mayan is that it's him versus either a Democrat or a Republican actually. &gt;&gt; And then if he's up against a Democrat, a more liberal Democrat like Swallwell, the Republicans will support Maym and he'll win. And if he's up against a Republican, then he'll also win because all the Democrats will support him. And remember, this is like a plus 20 or 30 blue state. So, &gt;&gt; and by the way, he is such a good guy. He's been such an effective mayor of San Jose. you know, he cleared up homelessness. I mean, his polic actual work running something stands out amongst the rest of these folks. So, I think he's got a real shot, Sachs, at moving up real fast, even though he's coming. &gt;&gt; Well, yeah. Look, if he plays his cards right, he could win. I mean, look, I think there is a a path here for California restoration. You get someone like Matt Mayan as governor, maybe Rick Caruso takes another shot at running for mayor in LA. &gt;&gt; I think everyone recognizes that. &gt;&gt; Get in the game. &gt;&gt; I think everyone recognizes that Karen Bass has been a disaster. She, you know, aided and abetted the &gt;&gt; burning down of the Palisades. And then if you defeat the wealth tax so that the &gt;&gt; Wait, did Trump take over, &gt;&gt; you could have a path? &gt;&gt; Did Trump just take over the rebuild in Pacific Palisades with an executive order? Did I see that in my feed? &gt;&gt; He did. Yeah. Well, you know, there's been a very small number of permits granted. I mean, how long has it been like 18 months now? &gt;&gt; Sound [ __ ] &gt;&gt; Wait, so Sax, would you move back if there was a great restoration in California? I feel like somehow it's just not going to happen. You know, it's like &gt;&gt; it's over. Not in our lifetime. &gt;&gt; I feel like the fact that you're telling me that Ma May is this great candidate and, you know, the whole tech world's going to get behind him. I'm sure that's true. And then somehow I think it's the reason he's going to lose and I think, you know, we'll end up with Swallwell, who I think &gt;&gt; you're so jaded, Sax. I think that somehow &gt;&gt; well Swall is considered to be kind of a lightweight and a pretty dim bulb. &gt;&gt; Friend of the pond, Eric Swall, come back anytime. Who interviewed him with me? &gt;&gt; Jim, was that you? Did you interview him with me? &gt;&gt; Yeah, I did. Yeah, you did. Yeah. Yeah. Yeah. &gt;&gt; Yeah. &gt;&gt; I mean, just manifestly not up to the challenges that are going to beset California in the coming years. You need someone really stellar to clean things up after the fiscal insanity of the Newsome years. &gt;&gt; Well, look, I'll restate this on the show very clearly. California has a trillion dollar fiscal cliff coming up because of the pension obligations. I don't know if you guys know this. I'll make this one point very important to know. They once tried to change the benefits, the pension benefits, and they lost a court case. And so there's precedent in California state court that you cannot change pension benefits on the date that someone was hired. You can never change their future benefits. Really important to know. So all of the benefits that have been given to every California public employee up to this day or that they've been promised, they are promised for life and you're not allowed to take them out and they cannot be disposed of except for some form of bankruptcy. And there is no mechanism by which the state can declare bankruptcy. So fundamentally and functionally, there are two ways that California can be saved. Number one, you pass an amendment to the Constitution to fix this pension liability problem. And number two is the state has the ability to declare bankruptcy. Everything else is all about how long are you keeping the state alive for. &gt;&gt; Well, I I I support that that change to bankruptcy law. I just think it'll be hard to get. But, you know, we could try. And look, if you had a governor of California come to the federal government and say, &quot;We would like this,&quot; then there's a much higher chance of it happening. But actually, I think one question I think one question &gt;&gt; for you to ask in evaluating this is what does Gavin Newsome want? Because obviously he's running for president in 2028, you know, who would he like to replace him as governor? You know, and obviously he doesn't want a Republican because that'd be a rebuke. You know, does he want Swall? Does he want someone who will appear even weaker and stupider than him? or does he want someone like Maym who will be appearing &gt;&gt; to clean up the state? I think he might want the idiot &gt;&gt; although it's a it's a tough it's a tough tough choice for him. &gt;&gt; She'll just scream. I like &gt;&gt; Well, here's the thing is he also doesn't want the state to fall apart while he's running for president cuz he might be blamed for that too. &gt;&gt; So, does he want someone like Compton like Maym who's criticized him because Maym has criticized Newsome on things like homelessness, right? &gt;&gt; So, Gavin is super thin skin. &gt;&gt; They can get past that. &gt;&gt; I don't know. But Gavin is super thin skinned, so I'm sure he resents that. So probably the machine gets behind Swallwell, even though it probably means a train wreck for the state. But we'll see. &gt;&gt; Yeah, that's doesn't seem like a good idea. Yeah, I think he would want. &gt;&gt; But this is a big opportunity for tech to flex his political muscles to see. &gt;&gt; Yeah, let's see. Let's see. &gt;&gt; Let's see if they can do it. Let's see if they can do it. &gt;&gt; It's not just tech. It's anyone that doesn't want an establishment governor. And I think that includes Hollywood. It includes agriculture. It includes large swats of the state's economy. So, let's see what happens. &gt;&gt; All right, everybody. The AllIn event series continues. All-in Summit Los Angeles September 13th to 15th. Tickets are now taking applications. So, join us in September. And if you are a venture capitalist, LP in funds, sovereign wealth fund, um, endowment, we're going to be having our first all-in liquidity event May 31st to June 3rd. Go to allin.com/events to apply for a ticket to liquidity as well. And we'll see you all next time on the world's greatest podcast, the All-In podcast. Go ahead and subscribe to our 1 million subscriber channel on YouTube. That's right. YouTube just broke 1 million. All right. Love you, besties. Bye-bye. &gt;&gt; Bye-bye. Love you, Bruce. &gt;&gt; We'll let your winners ride. Rainman David &gt;&gt; and it said &gt;&gt; we open sourced it to the fans and they've just gone crazy with it. &gt;&gt; Besties are my dog taking your driveways. &gt;&gt; Oh man, my habitasher will meet the ugly. We should all just get a room and just have one big huge orgy because they're all just useless. It's like this like sexual tension that they just need to release somehow. &gt;&gt; Your feet. &gt;&gt; We need to get merch. &gt;&gt; I'm going all in. I'm going all in.
                    </div>
                </div>
                
            </article>
            

            <article class="video-card">
                <div class="video-header">
                    <h2 class="video-title">
                        <a href="https://www.youtube.com/watch?v=EV7WhVT270Q" target="_blank" rel="noopener">State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI | Lex Fridman Podcast #490</a>
                    </h2>
                    <div class="video-meta">
                        <span class="channel-pill"><img class="channel-icon" src="https://yt3.ggpht.com/ytc/AIdro_ljfMy9kUR1PH9VRf-XsTsPqFMgORC_zodOQVEAm4hx36lC=s240-c-k-c0x00ffffff-no-rj" alt="" loading="lazy"><span class="channel-name">Lex Fridman</span><span class="channel-subs">(4.9M)</span></span>
                        <span class="meta-sep">·</span><span>265:13</span>
                        <span class="meta-sep">·</span><span>498.9K views</span>
                        <span class="meta-sep">·</span>
                        <span>2026-01-31</span>
                    </div>
                    <div class="video-tags"><span class="tag tag-person">Jensen Huang</span> <span class="tag tag-company">NVIDIA</span> <span class="tag tag-company">OpenAI</span> <span class="tag tag-event">Keynote</span></div>
                </div>
                <div class="tldr">AI development is marked by intense global competition, with significant technical advancements driven by scaling compute, data quality, and innovative post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR).</div>
                <div class="summary-section">
                    <div class="summary-preview" id="preview-1">AI development is marked by intense global competition, with significant technical advancements driven by scaling compute, data quality, and innovative post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR). While models are becoming incredibly...</div>
                    <div class="summary-full" id="full-1">
                        <h3>TL;DR</h3>
<p>AI development is marked by intense global competition, with significant technical advancements driven by scaling compute, data quality, and innovative post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR). While models are becoming incredibly powerful and useful, the industry grapples with the economic viability of ever-larger systems, the ethical integration of AI into society, and profound questions about human agency and the nature of intelligence.</p>
<h3>Global AI Competition and Open-Weight Dynamics</h3>
<ul>
<li><strong>DeepSeek's Catalyst Moment:</strong> The early 2025 release of DeepSeek-R1, demonstrating near state-of-the-art performance with allegedly less compute, ignited intense global competition in AI research and products.</li>
<ul>
<ul>
<li>This sparked a movement in China, leading to numerous strong open-weight models from companies like Zhipu AI (GLM), MiniMax, and Kimi Moonshot.</li>
</ul>
</ul>
<li><strong>China's Open-Weight Strategy:</strong> Chinese companies are releasing open-weight models, often with unrestricted licenses, to build international influence and participate in the U.S. AI market despite security concerns hindering API subscriptions.</li>
<ul>
<ul>
<li>This trend is expected to continue for a few years, driven by government incentives and the pursuit of market share, though eventual consolidation is anticipated due to high development costs.</li>
</ul>
</ul>
<li><strong>U.S. Market Leaders and Counter-Efforts:</strong> While Chinese open models gain traction, U.S. models like Claude 3.5 Opus, Gemini, and ChatGPT maintain perceived superiority in output quality and user adoption.</li>
<ul>
<ul>
<li>Anthropic is recognized for its strong focus on coding capabilities and stable organizational culture.</li>
<li>Google leverages its immense scale and TPU infrastructure, aiming to advance Gemini despite OpenAI's incumbent advantage.</li>
<li>Initiatives like the American Truly Open Models (ATOM) project seek to bolster U.S. open-weight AI development to compete with the rising Chinese ecosystem.</li>
</ul>
</ul>
<li><strong>Fluid Technological Advantage:</strong> The rapid pace of releases creates a "leapfrogging" effect, where the most recent model often appears superior, driven by readily shared ideas and continuous architectural tweaks across companies.</li>
<ul>
<ul>
<li>Differentiation will increasingly hinge on budget and hardware access rather than proprietary technological ideas, as researchers frequently move between labs.</li>
</ul>
</ul>
</ul>
<h3>Technical Advancements and Scaling the AI Frontier</h3>
<ul>
<li><strong>Consistent Core Architecture:</strong> Despite rapid progress, the fundamental Transformer architecture (derived from GPT-2's decoder) remains the bedrock of state-of-the-art LLMs.</li>
<ul>
<ul>
<li>Key innovations are often clever tweaks: Mixture of Experts (MoE), Multi-head Latent Attention, Grouped-query Attention, and Sliding Window Attention, which enhance efficiency and capability.</li>
<li><strong>Mixture of Experts (MoE)</strong> allows models to be much larger by selectively activating "experts" (sub-networks) for specific input tokens, making them more efficient than dense models.</li>
</ul>
</ul>
<li><strong>Scaling Laws Continue to Drive Progress:</strong> The predictable relationship between compute/data and model performance is still strong across all stages of AI development.</li>
<ul>
<ul>
<li><strong>Pre-training:</strong> While still the foundation for raw intelligence and expensive (millions to billions), costs are pushing labs towards optimizing data quality (e.g., synthetic data, advanced OCR) and focusing on post-training for capability unlocks.</li>
<li><strong>Post-training:</strong> This stage has seen major breakthroughs, offering significant capability enhancements more cost-effectively than brute-force pre-training.</li>
<ul>
<ul>
<li><strong>Reinforcement Learning with Verifiable Rewards (RLVR)</strong> is a major innovation, training models by rewarding accurate solutions to verifiable tasks (e.g., math, code).</li>
<ul>
<ul>
<li>RLVR amplifies desirable behaviors like step-by-step reasoning and self-correction, directly improving accuracy and enabling sophisticated tool use.</li>
<li>It is compute-intensive but scales well, offering predictable performance gains with increased compute.</li>
</ul>
</ul>
<li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> remains crucial for refining model style, tone, and alignment, providing the "finishing touch" that makes models user-friendly (e.g., ChatGPT's appeal).</li>
</ul>
</ul>
<li><strong>Inference-time Scaling:</strong> Involves allocating more compute during a model's use, enabling deeper "thinking" or multi-step reasoning before generating an output.</li>
<ul>
<ul>
<li>This capability, often enabled by RLVR, represents a significant "step function" change in model abilities and user experience (e.g., OpenAI's o1).</li>
</ul>
</ul>
</ul>
</ul>
<li><strong>Expanding Context Windows:</strong> Context lengths are rapidly increasing (e.g., up to millions of tokens), driven by compute and data, with architectural innovations (sparse attention, recursive models) aiming for greater efficiency.</li>
<li><strong>Emerging Architectural Alternatives:</strong> Text diffusion models, inspired by image generation, are being explored for their potential to generate text in parallel, offering speed advantages for certain tasks, though full generality and tool integration remain challenges.</li>
</ul>
<h3>Human &amp; Societal Dimensions of AI Adoption</h3>
<ul>
<li><strong>Evolving User Interaction:</strong> Users gravitate to models based on specific features, brand familiarity, or "muscle memory," switching when models perform poorly.</li>
<ul>
<ul>
<li>The future likely involves individuals using multiple specialized LLM subscriptions for diverse personal and professional tasks.</li>
</ul>
</ul>
<li><strong>The Challenge of AI "Voice" and Personalization:</strong> LLMs currently struggle to replicate nuanced human "voice" due to RLHF's tendency to average preferences, leading to generic outputs.</li>
<ul>
<ul>
<li>Personalized AI, capable of "understanding" users, presents ethical considerations, particularly concerning sensitive topics like mental health.</li>
<li>Fear of legal liability and user backlash drives companies to "take the edge off" models, potentially limiting their creative or challenging capacities.</li>
</ul>
</ul>
<li><strong>AI-Generated Content ("Slop") and Trust:</strong> The proliferation of AI-generated content (e.g., code, text) necessitates a human-in-the-loop for curation and verification to maintain quality and trust.</li>
<ul>
<ul>
<li>The distinction between raw AI output and human-vetted content is becoming increasingly critical.</li>
</ul>
</ul>
<li><strong>Impact on Work, Learning, and Agency:</strong></li>
<ul>
<ul>
<li><strong>Programming:</strong> AI tools are making software development more efficient and enjoyable, with senior developers increasingly shipping AI-generated code. However, this raises concerns about how future experts will develop foundational skills if AI handles complex tasks.</li>
<li><strong>Learning:</strong> LLMs offer powerful learning aids (e.g., infinite exercises, customized explanations) but require discipline to avoid superficial understanding.</li>
<li><strong>Job Displacement:</strong> The automation enabled by AI will lead to job losses, creating significant individual suffering that society must address.</li>
</ul>
</ul>
<li><strong>Data Rights and IP in the AI Era:</strong> High-profile lawsuits (e.g., Anthropic's $1.5 billion settlement for copyrighted books) underscore the legal complexities of training data acquisition and the urgent need for compensation schemes for creators.</li>
<ul>
<ul>
<li>The use of proprietary, domain-specific data (e.g., medical, legal) is expected to fuel future AI capabilities, creating new "moats" for specialized models.</li>
</ul>
</ul>
</ul>
<h3>Future Trajectories and Industry Outlook</h3>
<ul>
<li><strong>AGI Timelines and "Jagged" Progress:</strong> Definitions of AGI ("remote worker," "superhuman coder") remain debated, with current predictions for widespread automated programming (superhuman coder) pushed back to 2031.</li>
<ul>
<ul>
<li>AI is likely to be "jagged," excelling in some domains while struggling in others, making a truly complete "superhuman coder" difficult to achieve.</li>
<li>Significant breakthroughs beyond current LLM/RL paradigms may be needed for true AGI, but these are inherently unpredictable.</li>
</ul>
</ul>
<li><strong>Tool Use as a Critical Frontier:</strong> Deep integration of external tools (web search, code interpreters, APIs) is paramount for expanding LLM capabilities and mitigating hallucinations.</li>
<ul>
<ul>
<li>This is currently more advanced in closed models, but open-source tooling is rapidly evolving to enable broader adoption.</li>
<li>Security concerns (e.g., granting LLMs access to personal emails) pose substantial trust barriers for advanced tool use.</li>
</ul>
</ul>
<li><strong>Continual Learning and Personalized Memory:</strong> The ability for AI models to continually adapt and learn from new data and user feedback is a major research problem.</li>
<ul>
<ul>
<li>Current methods involve frequent global model updates or in-context learning, but personalized, on-device weight updates (e.g., Apple Foundation models) represent a long-term goal.</li>
<li>Memory in LLMs primarily relies on context windows, with architectural tweaks (LoRA adapters) offering partial solutions with inherent trade-offs.</li>
</ul>
</ul>
<li><strong>Industry Consolidation and Business Model Evolution:</strong></li>
<ul>
<ul>
<li>The AI sector is seeing increasing consolidation, with large licensing deals impacting the startup ecosystem by favoring top talent over widespread equity distribution.</li>
<li>Easy access to private funding for U.S. frontier labs (OpenAI, Anthropic, xAI) reduces the immediate need for IPOs, contrasting with Chinese companies like MiniMax and Zhipu AI filing for public offerings.</li>
<li>The long-term profitability of the LLM API market is uncertain, potentially pushing companies towards hardware or integrated products.</li>
<li>Meta's Llama project, initially a pioneering open-weight effort, faced internal challenges and misaligned incentives, highlighting the complexities of sustaining open-source leadership.</li>
</ul>
</ul>
<li><strong>Hardware Supremacy and Innovation:</strong> NVIDIA's dominance in AI compute is sustained by its CUDA ecosystem and rapid innovation, though competitors (Google's TPUs, Amazon's Trainium) are emerging.</li>
<ul>
<ul>
<li>Specialized chips for inference (e.g., Groq's acquisition, Vera Rubin project) aim to optimize specific computational needs for cost-efficiency.</li>
<li>Jensen Huang's focused leadership is seen as a key driver for NVIDIA's continued success and the broader AI build-out.</li>
</ul>
</ul>
</ul>
<h3>Notable Quotes</h3>
<blockquote>
"I don't think any company in 2026 will have access to a technology that no other company has. And that is mainly because researchers are frequently changing jobs, changing labs. They rotate. So I don't think there will be a clear winner in terms of technology access. However, I do think the differentiating factor will be budget and hardware constraints." – Sebastian Raschka on <strong>AI technology differentiation</strong>
"What reinforcement learning, this RLVR, is very good at is amplifying these behaviors because they're very useful in enabling the model to think longer and to check its work. And I agree that it is very beautiful that this training allows the model to learn to amplify this in a way that is just so useful in making the final answers better." – Nathan Lambert on <strong>RLVR and model reasoning</strong>
"I think the real question... is when are we going to see a big, obvious leap in economic impact? Because currently, there hasn't been an obvious leap in the economic impact of LLM models, for example." – The interviewer on <strong>economic impact of LLMs</strong>
"Personally, I have a hard time reading things where I see it's obviously AI generated. I'm sorry. It might be really good information, but I have a certain, 'Nah, not for me.'" – Sebastian Raschka on <strong>AI-generated content and human preference</strong>
</blockquote>
                    </div>
                    <button class="toggle-btn" id="sum-btn-1" onclick="toggleSummary(1)">
                        Read more <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M6 9l6 6 6-6"/></svg>
                    </button>
                </div>
                
                <div class="transcript-section">
                    <button class="transcript-toggle" id="trans-btn-1" onclick="toggleTranscript(1)">
                        View transcript
                    </button>
                    <div class="transcript-content" id="transcript-1">
                        The following is a conversation all about the state-of-the-art in artificial intelligence, including some of the exciting technical breakthroughs and developments in AI that happened over the past year, and some of the interesting things we think might happen this upcoming year. At times, it does get super technical, but we do try to make sure that it remains accessible to folks outside the field without ever dumbing it down. It is a great honor and pleasure to be able to do this kind of episode with two of my favorite people in the AI community: Sebastian Raschka and Nathan Lambert.

They are both widely respected machine learning researchers and engineers who also happen to be great communicators, educators, writers, and X posters. Sebastian is the author of two books I highly recommend for beginners and experts alike: *Build a Large Language Model from Scratch* and *Build a Reasoning Model from Scratch*. I truly believe that in the machine learning/computer science world, the best way to learn and understand something is to build it yourself from scratch. Nathan is the post-training lead at the Allen Institute for AI and author of the definitive book on Reinforcement Learning from Human Feedback. Both of them have great X accounts, great Substacks. Sebastian has courses on YouTube, Nathan has a podcast. And everyone should absolutely follow all of those.

This is the Lex Fridman podcast. To support it, please check out our sponsors in the description, where you can also find links to contact me, ask questions, get feedback, and so on. And now, dear friends, here's Sebastian Raschka and Nathan Lambert.

So I think one useful lens to look at all this through is the so-called DeepSeek moment. This happened early in 2025, when the open-weight Chinese company DeepSeek released DeepSeek-R1 that, I think it's fair to say, surprised everyone with near state-of-the-art performance, with allegedly much less compute for much cheaper. Since then, the AI competition has gotten insane, both on the research level and the product level. It's just been accelerating. Let's discuss all of this today, perhaps starting with some spicy questions if we can.

Who's winning at the international level? Would you say it's the set of companies in China or the set of companies in the United States? Sebastian, Nathan, it's good to see you guys. So Sebastian, who do you think is winning?

- Winning is a very broad term. You mentioned the DeepSeek moment, and I think DeepSeek is definitely winning the hearts of those who work on open-weight models because they share them as open models. Winning, I think, has multiple timescales: today, next year, and in 10 years. One thing I know for sure is that I don't think any company in 2026 will have access to a technology that no other company has. And that is mainly because researchers are frequently changing jobs, changing labs. They rotate. So I don't think there will be a clear winner in terms of technology access. However, I do think the differentiating factor will be budget and hardware constraints. I don't think the ideas will be proprietary, but rather the resources needed to implement them. And so I don't currently see a winner-takes-all scenario. I can't see that at the moment.

- Nathan, what do you think?

- You see labs putting different energy into what they're trying to do. To mark the point in time when we're recording this, the hype over Anthropic's Claude 3.5 Opus model has been absolutely insane. I've used it and built things in the last few weeks, and it's almost gotten to the point where it feels like a bit of a meme in terms of the hype. And it's kind of funny because this is very organic. If we go back a few months, we can recall the release date and notes when Google's Gemini 1.5 was released, and the marketing and &quot;wow&quot; factor of that release seemed super high. But then, at the end of October, Claude 3.5 Opus was released, and the hype has been growing, even though Gemini came out before it. And it feels like people don't really talk about it as much, even though when it came out, everyone was saying, &quot;This is Gemini's moment to retake Google's structural advantages in AI.&quot; And Gemini is a fantastic model, and I still use it. The differentiation is just lower. And I agree with Sebastian; what you're saying with all these, the idea space is very fluid, but culturally, Anthropic is known for betting very hard on code—which is the &quot;Claude Code&quot; thing—and it's working out for them right now. So I think that even if the ideas flow pretty freely, so much of this is bottlenecked by human effort and the culture of organizations, where Anthropic seems to be presenting as the least chaotic. It's a bit of an advantage if they can maintain that for a while.

But on the other side of things, there's a lot of technology from China where there are way more labs than just DeepSeek. So DeepSeek kicked off a movement within China, similar, I'd say, to how ChatGPT kicked off a movement in the U.S. where suddenly everything had a chatbot. There are now tons of tech companies in China that are releasing very strong frontier open-weight models, to the point where I would say that DeepSeek is kind of losing its crown as the preeminent open model maker in China. The likes of Zhipu AI with their GLM models, and MiniMax's and Kimi Moonshot's models, especially in the last few months, have shone more brightly. The new DeepSeek models are still very strong, but that could look back as a big narrative point where in 2025 DeepSeek came and provided a platform for many more Chinese companies to release fantastic models and operate in this new way. So these models from Chinese companies are open-weight, and depending on this trajectory, the business models of American companies could be at risk. But currently, a lot of people are paying for AI software in the U.S., but historically, in China and other parts of the world, people don't pay much for software.

- So some of these models like DeepSeek have the love of the people because they are open-weight. How long do you think the Chinese companies will keep releasing open-weight models?

- I would say for a few years. I think that, like in the U.S., there isn't a clear business model for it. I have been writing about open models for a while, and these Chinese companies have realized it. So I get inbound from some of them. And they're smart and realize the same constraints: many top U.S. tech companies and other IT companies won't pay for an API subscription to Chinese companies due to security concerns. This has been a long-standing habit in tech, and the people at these companies then see open-weight models as a way to influence and participate in the huge, growing AI expenditure market in the U.S. And I think the government will see that this builds a lot of international influence in terms of technology uptake, so there will be many incentives to keep it going. But building these models and conducting the research is very expensive, so at some point, I expect consolidation. However, I don't expect that to be a story of 2026, where there will be more open model builders throughout the year than there were in 2025. And many of the notable ones will be in China.

- You were going to say something?

- Yes, you mentioned DeepSeek losing its crown. I do think, to some extent, that's true, but we also have to consider that they are still, I would say, slightly ahead. And it's not that DeepSeek got worse; it's just that the others are using ideas from DeepSeek. For example, you mentioned Kimi: same architecture, and they're training it. And then again, we have this leapfrogging where they might, at some point, be a bit better because they have the more recent model. And I think this comes back to the fact that there won't be a clear winner; it will just be like that. One person releases something, another comes in, and the most recent model is probably always the best.

- Yes, we'll also see that Chinese companies have different incentives. For example, DeepSeek is very secretive, whereas some of these startups, like MiniMax and Moonshot AI, are not. Those two have literally filed IPO paperwork and are trying to gain Western mindshare and conduct a lot of outreach. I don't know if these incentives will change model development because DeepSeek, famously, is built by a hedge fund, Highflyer Capital, and we don't know exactly what they use the models for or if they care about this.

- They're secretive in terms of communication, but not secretive in terms of the technical reports that describe how their models work. And we should also address the Claude 3.5 Opus hype: there's the layer of something being the darling of the X echo chamber, the Twitter echo chamber, and the actual number of people using the model. I think it's probably fair to say that ChatGPT and Gemini are focused on the broad user base that just wants to solve problems in their daily lives, and that user base is gigantic. So the hype about coding may not be representative of the actual use.

- I would also say that many usage patterns are, as you said, about name recognition, brand, and so on, but also almost muscle memory, where, for instance, ChatGPT has been around for a long time. People just got used to using it, and it's almost like a flywheel: they recommend it to other users, and so forth. One interesting point is also the customization of LLMs. For example, ChatGPT has a memory feature, right? So you may have a subscription and use it for personal stuff, but I don't know if you'd want to use that same thing at work. Because there's a boundary between private and work. If you're working at a company, they might not allow that, or you may not want to. And I think that's also an interesting point, where you might have multiple subscriptions. One is just for clean code; it contains none of your personal images or hobby projects. It's just for work, and then the other is your personal one. So I think that's also something where there are two different use cases, and it doesn't mean you only have to have one. I think the future involves multiple ones.

- What model do you think won 2024, and what model do you think is going to win '25?

- I think in the context of consumer chatbots, it's a question of whether you're willing to bet on Gemini over ChatGPT. In my gut, I'd say that feels like a bit of a risky bet because OpenAI has been the incumbent, and there are so many benefits to that in tech. I think the momentum, if you look at 2024, was on Gemini's side, but they were starting from such a low point. And RIP Bard and these earlier attempts at getting started. I think huge credit goes to them for powering through the organizational chaos to make that happen. But it's also hard to bet against OpenAI because they always come off as so chaotic, yet they're very good at landing things. And I think, personally, I have very mixed reviews of o1, but it must have saved them so much money, with its high-profile feature being a router where most users are no longer charging their GPU costs as much. So I think it's very hard to dissociate the things I like about models versus the things that will actually be a general public differentiator.

- What do you think about 2025? Who's going to win?

- I'll say something, even though it's risky: I think Gemini will continue to make progress on ChatGPT. I think Google's scale is a factor, especially when both of these are operating at such extreme scales. Google has the ability to separate research and product better, whereas you hear so much about OpenAI being chaotic operationally and chasing the high-impact thing, which is a very startup culture. And then, on the software and enterprise side, I think Anthropic will continue to have success, as they've again and again been set up for that. Obviously, Google Cloud has a lot of offerings, but I think this Gemini name brand is important for them to build. Google Cloud will continue to do well, but that's a more complex thing to explain in the ecosystem because it's competing with Azure and AWS, rather than solely on the model provider side.

- So in infrastructure, you think TPU gives them an advantage?

- Largely because the margin on NVIDIA chips is insane, and Google can develop everything from top to bottom to fit their stack without paying this margin. They've also had a head start in building data centers. So all of these things, which have both high lead times and very hard margins on high costs, give Google a historical advantage there. And if there's going to be a new paradigm, it's most likely to come from OpenAI, where their research division has again and again shown this ability to land a new research idea or product. Like Deep Research, Sora, o1 thinking models—all these foundational things have come from OpenAI, and that has to be one of their top traits as an organization. So it's hard to bet against that, but I think a lot of this year will be about scale and optimizing what could be described as low-hanging fruit in models.

- And clearly there's a trade-off between intelligence and speed. This is what GPT-5 was trying to solve behind the scenes. It's like, does the broad public actually want intelligence, or do they want speed?

- I think having a nice variety, or the option to have a toggle, is actually ideal. For my personal usage, most of the time when I look something up, I use ChatGPT to ask a quick question and get the information I want fast. For most daily tasks, I use the quick model. Nowadays, I think the auto mode is pretty good, where you don't have to specifically say &quot;thinking&quot; or &quot;non-thinking.&quot; Then again, I also sometimes want the Pro mode. Very often, what I do is, when I have something written, I put it into ChatGPT and say, &quot;Hey, do a very thorough check. Are all my references correct? Are all my thoughts correct? Did I make any formatting mistakes? Are the figure numbers wrong?&quot; or something like that. And I don't need that right away. It's okay—I finish my work, maybe have dinner, let it run, and then come back and go through it. I think this is where it's important to have this option. I would go crazy if I had to wait 30 minutes or even 10 minutes for each query.

- That's me! I'm sitting over here losing my mind that you use the router and the non-thinking model. I'm like, &quot;How do you live with that?&quot; That's my reaction. I've been heavily relying on ChatGPT for a while. I never touch non-thinking. I find its tone, and its propensity for errors—it just has a higher likelihood of errors. Some of this stems from when OpenAI released o1-preview, which was the first model to do this deep search and find many sources and integrate them for you. So I became accustomed to that. So I will only use GPT-4o thinking or Pro when I'm performing any sort of information query for work, whether that's a paper or some code reference that I need. And I will regularly have five Pro queries going simultaneously, each looking for one specific paper or feedback on an equation or something.

- I have a fun example where I needed the answer as fast as possible for this podcast before I was leaving on a trip. I have a local GPU running at home, and I wanted to run a long RL experiment. Usually, I unplug things because you never know what might happen when you're not at home. I accidentally unplugged the GPU. My wife was already in the car, and it was like, &quot;Oh, dang!&quot; Basically, I wanted a Bash script as fast as possible that runs my different experiments and the evaluation. And it's something I know; I learned how to use the Bash interface or Bash terminal, but in that moment, I just needed like 10 seconds—&quot;give me the command!&quot;

- So I used the non-thinking fastest model. It gave me the Bash command to chain different scripts together. And then the thing is, you have the &quot;tee&quot; command where you want to route this to a log file. Off the top of my head, I was just in a hurry; I could have thought about it myself.

- By the way, I don't know if that's a representative case: wife waiting in the car... you have to run, unplug the GPU, generate a Bash script. This sounds like a movie, like Mission Impossible.

- I use Gemini for that. So I use 'thinking' for all the information-heavy stuff, and then Gemini for fast things or stuff that I could sometimes Google. It's good at explaining things, and I trust that it has this kind of background knowledge, and it's simple. And the Gemini app has gotten a lot better. It's good for those sorts of things. And then for code and any sort of philosophical discussion, I use Claude Opus 3.5. Always with extended thinking, too. Extended thinking and inference-time scaling are just ways to make the models marginally smarter. And I will always hedge on that side when the progress is very high, because you don't know when that will unlock a new use case. And then sometimes I use Grok for real-time information or finding something on AI Twitter that I knew I saw, and I need to dig up, and I just fixated on. Although when Grok-3 came out, Grok-3 Super Heavy, which was their Pro variant, was actually very good, and I was pretty impressed with it. Then, through muscle memory, I just lost track of it with the ChatGPT app open. So I use many different things.

- Yeah, I actually do use Grok-3 Heavy for debugging. For hardcore debugging that the other models can't solve, I find that it's the best. It's interesting because you say ChatGPT is the best interface. For me, for that same reason—though this could be just momentum—Gemini is the better interface. I think it's because I fell in love with their &quot;best needle in the haystack&quot; feature. If I ever put something in that has a lot of context but I'm looking for very specific kinds of information to make sure it tracks all of it, I find that Gemini, for me at least, has been the best. So it's funny with some of these models: if they win your heart over for one particular feature, on a particular day, for a specific query or prompt, you're like, &quot;This model's better.&quot; And so you'll just stick with it for a bit until it does something really dumb. There's like a threshold effect: some smart thing happens, and you fall in love with it. Then it does something dumb, and you're like, &quot;You know what? I'm going to switch and try Claude or ChatGPT,&quot; and all that kind of stuff.

- This is exactly it. You use it until it breaks, until you have a problem, and then you change the LLM. And I think it's the same way we use anything, like our favorite text editor, operating systems, or browser. I mean, there are so many browser options: Safari, Firefox, Chrome. They're relatively similar, but then there are edge cases, maybe extensions you want to use, and then you switch. But I don't think anyone types the same thing, like a website, into different browsers and compares them. You only do that when the website doesn't render or something breaks. So that's a good point: you use it until it breaks, and then you explore other options.

- Regarding the long context thing, I was also a Gemini user for this, but the GPT-4o release blog had crazy long context scores, where many people were like, &quot;Did they just figure out some algorithmic change?&quot; It went from, like, 30% to 70% or something in this minor model update. So it's also very hard to keep track of all of these things, but now I look more favorably at GPT-4o's long context. So it's just like, &quot;How do I actually get to testing this?&quot; It's a never-ending battle.

- Well, it's interesting that none of us have talked about the Chinese models from a usage perspective. What does that say? Does that mean the Chinese models aren't as good, or does that mean we're just very biased and U.S.-focused?

- I do think that's currently the discrepancy between the model and the platform. I think the open models are more known for their open weights, not for their platform yet.

- There are also many companies willing to sell you open-model inference at a very low cost. With things like OpenRouter, it's easy to look at multi-model options. You can run DeepSeek on Perplexity. I think all of us sitting here are like, &quot;We use OpenAI GPT-4 Pro consistently.&quot; We're all willing to pay for the marginal intelligence gain. And these models from the U.S. are better in terms of outputs. I think the question is: will they stay better for this year and for years to come? As long as they're better, I'm going to pay to use them. There's also analysis showing that the way Chinese models are served—you could argue this is due to export controls or not—is that they use fewer GPUs per replica, which makes them slower and leads to different errors. It's speed and intelligence. If these things are in your favor as a user, many users in the U.S. will go for this. I think that is one thing that will spur these Chinese companies to compete in other ways, whether it's free or substantially lower costs, or it will breed creativity in terms of offerings, which is good for the ecosystem. But the simple fact is, the U.S. models are currently better, and we use them. I've tried these other open models, and I'm like, &quot;Fun, but I don't go back to them.&quot;

- We didn't really mention programming. That's another use case that many people care about. I use basically half-and-half Cursor and Claude because I find them to be a fundamentally different experience and both useful. What do you guys use? You program quite a bit, so what's the current vibe?

- So, I use the Codeium plugin for VS Code. You know, it's very convenient; it's just a plugin, and it's a chat interface that has access to your repository. I know that Claude is, I think, a bit different. It is a bit more agentic. It touches more things; it does the whole project for you. I'm not quite there yet where I'm comfortable with that, because maybe I'm a control freak, but I'd still like to see what's going on. And Codeium is kind of the sweet spot right now, where it's helping me but not taking completely over.

- I should mention one of the reasons I use Claude: to build the skill of programming with English. I mean, the experience is fundamentally different. You're... as opposed to micromanaging the details of the code generation process, and looking at the diff (which you can do in Cursor if that's the IDE you use), and changing, altering. Looking and reading the code, and understanding the code deeply as you progress, versus just thinking in this design space and guiding it at this macro level, which I think is another way of thinking about the programming process. Also, we should say that Claude just seems to be a better utilization of Claude Opus 3.5 somehow.

- It's a good side-by-side comparison. You can have Claude open, you can have Cursor open, you can have VS Code open, and select the same models on all of them, then ask questions. It's very interesting. Claude is way better in that domain; it's remarkable.

- Alright, we should say that both of you are legitimate on multiple fronts: researchers, programmers, educators, and Twitterers. And on the book front, too. So Nathan, hopefully, has an RLHF book coming out at some point soon.

- It's available for preorder, and there's a full digital preprint. I'm making it pretty and better organized for the physical version, which is why I do it, because it's fun to create things that you think are excellent in physical form, when so much of our life is digital.

- I should say, according to Perplexity here, Sebastian Raschka is a machine learning researcher and author known for several influential books. A couple of them that I wanted to mention, which are books I highly recommend: *Build a Large Language Model from Scratch* and the new one, *Build a Reasoning Model from Scratch*. I'm really excited about that. Building things from scratch is one of the most powerful ways of learning.

- Honestly, building an LLM from scratch is a lot of fun, and it's also a lot to learn. And as you said, it's probably the best way to learn how something really works, because you can look at figures, but figures can have mistakes. You can look at concepts and explanations, but you might misunderstand them. But if there is code, and the code works, you know it's correct. There's no misunderstanding; it's precise. Otherwise, it wouldn't work. And that's the beauty behind coding: it doesn't lie. It's math, basically. Even with math, I think you can have mistakes in a book that you would never notice. Because you're not running the math when you're reading the book, you can't verify this. And what's nice with code is you can verify it.

- Yeah, I agree with you about the *Build a Large Language Model (From Scratch)* book. It's nice to tune out everything else—the internet and so on—and just focus on the book. But, you know, I read several history books; it's just less lonely somehow. It's really more fun. For example, on the programming front, I think it's genuinely more fun to program with an LLM. And I think it's genuinely more fun to read with an LLM. But you're right; that distraction should be minimized. So you use the LLM to enrich the experience, maybe add more context. The rate of &quot;aha moments&quot; for me, on a small scale, is really high with LLMs.

- 100%. I also want to correct myself: I'm not suggesting *not* to use LLMs. I suggest doing it in multiple passes. Like, one pass just offline, in focus mode, and then after that... I mean, I also take notes, but I try to resist the urge to immediately look things up. I do a second pass. It's just more structured this way. Sometimes things are answered later in the chapter, and sometimes it just helps to let it sink in and think about it. I would highly recommend using LLMs when reading books. For me, it's just not the first thing to do; it's the second pass.

- By way of recommendation, I'll say I do the opposite. I like to use the LLM at the beginning to lay out the full context of what this world I'm now stepping into is. But I try to avoid clicking out of the LLM into the world of Twitter and blogs. Because then you're down this rabbit hole. You're reading somebody's opinion, there's a flame war about a particular topic, and all of a sudden you're in the realm of the internet and Reddit. But if you're purely letting the LLM give you the context of why this matters and what the big-picture ideas are... Books themselves are good at that, but not always.

- This is why I like the ChatGPT app: it gives the AI a home on your computer where you can focus on it, rather than it just being another tab in my mess of internet options. And I think Claude Code does a good job of making that a joy. It seems very engaging as a product designed to be an interface through which your AI will then go out into the world. It's something intangible compared to OpenAI's models: it just feels warm and engaging, whereas they can often be as good but just feel a little bit rough around the edges. Whereas Claude Code makes it fun to build things, particularly from scratch, where you don't have to worry because you trust it will make something. This is good for websites, refreshing tooling, and similar tasks, which I use it for, or for data analysis. For my blog, we scrape Hugging Face to keep track of download numbers for every dataset and model over time now. And Claude was just like, &quot;Yeah, I've made use of that data, no problem.&quot; And I was like, &quot;That would have taken me days.&quot; And then I have enough situational awareness to be like, &quot;Okay, these trends obviously make sense.&quot; But that's just a wonderful interface where you can have an intermediary and not have to do the awful low-level work that you would typically have to do to maintain different web projects.

- Alright. So we just talked about a bunch of the closed-weight models. Let's talk about the open ones. So, tell me about the landscape of open LLM models. Which ones are interesting? Which stand out to you and why? We already mentioned DeepSeek.

- Do you want to see how many we can name off the top of our heads?

- Yeah, without looking at notes.

- DeepSeek, Kimi, MiniMax, Z.ai, Moonshot. We're just naming Chinese ones.

- Let's throw in Mistral AI, Gemma—OLMo, the open-source model by AI2. Actually, NVIDIA had a really cool one, Nemotron 340B. There's a lot of stuff, especially at the end of the year. Qwen might be the one—

- Qwen was the obvious name I was going to say. I was trying to get through... You can get at least 10 Chinese and at least 10 Western. I think that... OpenAI released their first open model since GPT-2. When I was writing about OpenAI's open model release, they were like, &quot;Don't forget about GPT-2,&quot; which I thought was really funny because it's just such a different time. But GLM-4 is actually a very strong model and does some things that the other models don't do very well. I think that, selfishly, I'll promote a bunch of Western companies. Both in the U.S. and Europe have these fully open models. I work at the Allen Institute for AI, where we've been building OLMo, which releases data and code. And now we have actual competition from people trying to release everything so that others can train these models. There's the Institute for Foundation Models and LM360, which had their K2 models of various types. Apertus is a Swiss research consortium. Hugging Face has SmolLM, which is very popular. And NVIDIA's Nemotron has started releasing data as well. And then Stanford's Meerkat project is creating a pipeline for people to open a GitHub issue, implement a new idea, and then have it run in a stable language modeling stack. So this space—that list was way smaller in 2024; I think it was just AI2. So it's a great thing for more people to get involved and to understand language models, which doesn't really have a Chinese company that has an analog.

While I'm talking, I'll say that the Chinese open language models tend to be much bigger, and that gives them higher peak performance as MoEs. Many of these models that we like a lot, whether it was Gemma and Nemotron, have tended to be smaller models from the U.S., which is starting to change. In the U.S. and Europe, Mistral Large 3 came out in December, which was a giant MoE model, very similar to DeepSeek's architecture. And then startups like RCAI, and both Nemotron and NVIDIA, have teased MoE models way bigger than 100 billion parameters—in the 400 billion parameter range—coming in Q1 2026. So I think this balance is set to change this year in terms of what people are using Chinese versus U.S. open models for, which I'm personally going to be very excited to watch.

- First of all, huge props for being able to name so many of these. Did you actually name LLaMA?

- No.

- I feel like...

- RIP.

- This was not on purpose.

- RIP LLaMA. Alright. Can you mention some interesting models that stand out? You mentioned Qwen 2.5 is obviously a standout.

- I would say the year is almost bookended by DeepSeek V3 and R1, and then, on the other hand, in December, DeepSeek V3. Because what I like about those is they always have an interesting architectural tweak—an architectural tweak that others don't have. Otherwise, if you want to go with familiar but really good performance, Qwen 2.5 and, as Nathan said, also Jamba. And I think what's interesting about Jamba is that it's the first public or open-weight model that was really trained with tool use in mind, which I do think is a bit of a paradigm shift because the ecosystem wasn't quite ready for it. By &quot;tool use,&quot; I mean that the LLM is able to do a web search or call a Python interpreter. And I do think it's a standout because it's a huge unlock. Because one of the most common complaints about LLMs is, for example, hallucinations, right? In my opinion, one of the best ways to solve hallucinations is not to always try to remember information or make things up. For math, why not use a calculator app or Python? If I ask the LLM, &quot;Who won the soccer World Cup in 1998?&quot; instead of just trying to memorize, it could do a search. I think mostly it's still a Google search. So ChatGPT and GPT-4o would do a tool call to Google, maybe find the FIFA website, and find that it was France. It would get you that information reliably instead of just trying to memorize it. So I think it's a huge unlock, which I think right now is not yet fully utilized by the open-source, open-weight ecosystem. Many people don't use tool call modes because it's a trust thing. You don't want to run this on your computer where it has access to tools that could wipe your drive or whatever. So you want to containerize that. But I do think that is a really important step for the upcoming years to have this ability.

- First of all, thank you for defining what you mean by &quot;tool use.&quot; I think that's a great thing to do in general for the concepts we're talking about. Even things as well-established as MoEs. You have to say that means Mixture of Experts, and you kind of have to build up an intuition for people about what that means, how it's utilized, and what the different flavors are. So what does it mean that there's such an explosion of open models? What's your intuition?

- If you're releasing an open model, you want people to use it; that is the first and foremost thing. And then after that come things like transparency and trust. I think when you look at China, the biggest reason is that they want people around the world to use these models, and I think many people will not pay for software, but they might have computing resources to run it. I think there can also be data that you don't want to send to the cloud. So the number one thing is getting people to use models, use AI, or use your AI who might not be able to do it without having access to the model.

- I guess we should state explicitly: we've been talking about these Chinese models and open-weight models. Oftentimes, the way they're run is locally. So it's not like you're sending your data to China, or to whoever developed it in Silicon Valley, or whoever developed the model.

- Many American startups make money by hosting these models from China and selling them. It's called selling tokens, which means somebody will call the model to do some piece of work. I think the other reason is for U.S. companies, like OpenAI, which is so GPU-deprived. They're at the limits of their GPUs. Whenever they make a release, they're always talking about, &quot;Our GPUs are hurting.&quot; And I think in one of these GPT-4o release sessions, Sam Altman said, &quot;We're releasing this because we can use your GPUs. We don't have to use our GPUs, and OpenAI can still get distribution out of this,&quot; which is another very real thing, because it doesn't cost them anything.

- And for the user, I mean, there are users who just use the model locally, like they would use ChatGPT. But for companies, I think it's a huge unlock to have these models because you can customize them, you can train them, and you can add more data post-training. Like, specialize them into, let's say, legal, medical models, whatever you have. And you mentioned Llama; the appeal of the open-weight models from China is that the open-weight models also have even friendlier licenses. I think they are just unrestricted open-source licenses, whereas if we use something like Llama or Gemma, there are some strings attached. I think there's an upper limit in terms of how many users you can have. And then if you exceed, I don't know, so and so many million users, you have to report your financial situation to, let's say, Meta or something like that. And I think while it is a free model, there are strings attached, and people do like things where strings are not attached. So I think that's also one of the reasons, besides performance, why the open-weight models from China are so popular: because you can just use them. There's no catch in that sense.

- The ecosystem has gotten better on that front, but mostly downstream of these new providers offering such open licenses. That was funny when you pulled up Perplexity and said, &quot;Kimi-k2-thinking hosted in the U.S.&quot; Which is just like an exact... I've never seen this, but it's an exact example of what we're talking about, where people are sensitive to this. But Kimi-k2-thinking and Kimi-k2 are very popular models. People say they have very good creative writing and also excel at some software tasks. So it's just these little quirks that people pick up on with different models that they like.

- What are some interesting ideas that some of these models have explored that you can speak to, ideas that are particularly interesting to you?

- Maybe we can go chronologically. I mean, there was, of course, DeepSeek. DeepSeek R1 came out in January 2025, if we just focus on this year. However, this was based on DeepSeek-V3, which came out the year before, in December 2024. There are multiple things on the architecture side. What is fascinating is you can still... I mean, that's what I do with my from-scratch coding projects. You can still start with GPT-2, and you can add things to that model to transform it into this other model. So it's all still kind of like the same lineage... it's a very close relationship between them. But off the top of my head, what was unique about DeepSeek was the Mixture of Experts. I mean, they weren't inventing Mixture of Experts. We can maybe talk a bit more about what Mixture of Experts means. But just to list these things first before we dive into detail. Mixture of Experts, but then they also had Multi-head Latent Attention, which is a tweak to the attention mechanism. This was, I would say, in 2025, the main distinguishing factor between these open-weight models. Different tweaks to make inference or KV cache size... We can also define KV cache in a few moments. But to kind of make it more economical to have long context, to shrink the KV cache size. So what tweaks can we do? And most of them focused on the attention mechanism. There is Multi-head Latent Attention in DeepSeek. There is Grouped-query Attention, which is still very popular. It wasn't invented by any of those models; it goes back a few years. But that would be the other option. Sliding Window Attention—I think OLMo 2 uses it, if I remember correctly. So there are these different tweaks that make the models different. Otherwise, I put them all together in an article once, where I just compared them. They are very surprisingly similar. It's just different numbers in terms of how many repetitions of the Transformer block you have in the center. And just little knobs that people tune. But what's so nice about it is that it works no matter what. You can tweak things. You can move the normalization layers around to get some performance gains. And OLMo is always very good in ablation studies, showing what moving something around does to the model. Does it make it better or worse? But there are so many, let's say, ways you can implement a Transformer and still make it work. The big ideas that are still prevalent are Mixture of Experts, multi-head latent attention, sliding window attention, and grouped-query attention. And then, at the end of the year, we saw a focus on making the attention mechanism scale linearly with inference token prediction. So there was Qwen 2.5, for example, which added a gated delta net. It's kind of inspired by State Space Models, where you have a fixed state that you keep updating. But it essentially makes this attention cheaper, or it replaces attention with a cheaper operation.

- And it may be useful to step back and talk about Transformer architecture in general.

- Yeah, so maybe we should start with GPT-2 architecture. The Transformer that was derived from the &quot;Attention Is All You Need&quot; paper. The &quot;Attention Is All You Need&quot; paper had a Transformer architecture that had two parts: an encoder and a decoder. And GPT just focused on the decoder part. It is essentially still a neural network, and it has this attention mechanism inside. And you predict one token at a time. You pass it through an embedding layer. There's the Transformer block. The Transformer block has attention modules and a fully connected layer. And there are some normalization layers in between. But it's essentially neural network layers with this attention mechanism. So coming from GPT-2, when we move on to GPT-3, there is, for example, the Mixture of Experts layer. It wasn't invented by GPT-3; it's a few years old. But it is essentially a tweak to make the model larger without consuming more compute in each forward pass. So there is this fully connected layer, and if listeners are familiar with multi-layer perceptrons, you can think of a mini multi-layer perceptron—a fully connected neural network layer inside the Transformer. And it's very expensive because it's fully connected. If you have 1,000 inputs and 1,000 outputs, that's one million connections. And it's a very expensive part in this Transformer. And the idea is to kind of expand that into multiple feedforward networks. So instead of having one, let's say you have 256. But that would make it way more expensive, because now you have 256, but you don't use all of them at the same time. So you now have a router that says, &quot;Okay, based on this input token, it would be useful to use this fully connected network.&quot; And in that context, it's called an expert. So a Mixture of Experts means you have multiple experts. And depending on what your input is—let's say it's more math-heavy—it would use different experts compared to, let's say, translating input text from English to Spanish. It would maybe consult different experts. It's not as clear-cut to say, &quot;Okay, this is only an expert for math and for Spanish.&quot; It's a bit more fuzzy. But the idea is essentially that you pack more knowledge into the network, but not all the knowledge is used all the time. That would be very wasteful. So during token generation, you are more selective. There's a router that selects which tokens should go to which expert. It adds more complexity; it's harder to train. There's a lot that can go wrong, like collapse and everything. So I think that's why OLMo still uses dense... I mean, you have OLMo models with Mixture of Experts, but dense models, where &quot;dense&quot; means... So, it's jargon. There's a distinction between dense and sparse. So Mixture of Experts is considered sparse because we have a lot of experts, but only a few of them are active. So that's called sparse. And then dense would be the opposite, where you only have one fully connected module, and it's always utilized.

- So maybe this is a good place to also talk about KV cache. But actually, before that, even zooming out: fundamentally, how many new ideas have been implemented from GPT-2 to today? How different are these architectures, really?

- Like the Mixture of Experts. The attention mechanism in Llama 3, that would be the Group Query Attention mechanism. So it's a slight tweak from multi-head attention to Group Query Attention, so we have that. I think they replaced LayerNorm with RMSNorm, but it's just a different normalization and not a big change. It's just a tweak. The nonlinear activation function—for people familiar with deep neural networks, I mean, it's the same as changing sigmoid with ReLU. It's not changing the network fundamentally. It's just a little tweak. And that's about it, I would say. It's not really fundamentally that different. It's still the same architecture. So you can convert from one to the other by just adding these changes, basically.

- It fundamentally is still the same architecture.

- Yep. So for example, you mentioned my book earlier. That's a GPT-2 model in the book because it's simple and very small—124 million parameters, approximately. But in the bonus materials, I do have OLMo from scratch, Llama 3 from scratch, and other types of from-scratch models. And I always start with my GPT-2 model and just tweak—well, add different components, and you get from one to the other. It's kind of like a lineage, in a sense.

- Can you build up an intuition for people? Because when you zoom out and look at it, there's so much rapid advancement in the AI world. And at the same time, fundamentally, the architectures haven't changed. So where is all the turbulence, the turmoil of advancement, happening? Where are the gains to be had?

- So there are the different stages where you develop or train the network. You have pre-training. Now, back in the day, it was just pre-training with GPT-2. Now you have pre-training, mid-training, and post-training. So, I think right now we are in the post-training focus stage. I mean, pre-training still gives you advantages if you scale it up to better, higher-quality data. But then we have capability unlocks that were not there with GPT-2, for example. ChatGPT is basically a GPT-3 model. And GPT-3 is the same as GPT-2 in terms of architecture. What was new was adding the supervised fine-tuning and the reinforcement learning with human feedback. So it's more on the algorithmic side rather than the architecture.

- I would say that the systems also change a lot. I think if you listen to NVIDIA's announcements, they talk about things like, &quot;You now do FP8; you can now do FP4.&quot; And what's happening is these labs are figuring out how to utilize more compute to put it into one model, which lets them train faster, and that lets them put more data in. And then you can find better configurations faster by doing this. So you can look at, essentially, the tokens per second per GPU as a metric that you look at when you're doing large-scale training. And you could get... you can go from, like, 10K to 13K by turning on FP8 training, which means you're using less memory per parameter in the model. And by saving less information, you do less communication, and you can train faster. So all of these system-level things underpin much faster experimentation on data and algorithms. It's this kind of loop that keeps going, and it's kind of hard to describe when you look at the architecture and they're exactly the same. But the codebase used to train these models is going to be vastly different—and you could probably... the GPUs are different, but you probably train GPT-NeoX-20B much faster in wall-clock time than GPT-2 was trained at the time.

- Yeah. Like you said, in the Mixture of Experts, they had, for example, this FP4 optimization, where you get more throughput. But I do think this is... for speed, this is true, but it doesn't give the model new capabilities. It's just: how much can we make the computation coarser without suffering in terms of model performance degradation? But I do think... I mean, there are alternatives popping up to the Transformer. There are text diffusion models—a completely different paradigm. And there is also... I mean, although text diffusion models might use Transformer architectures, it's not an autoregressive Transformer. And also Mamba models. It's a State-Space Model. But they do have trade-offs, and currently, there's nothing that has replaced the autoregressive Transformer as the state-of-the-art model. For state-of-the-art, you would still go with that, but there are now alternatives for the cheaper end—alternatives that are kind of making compromises—but it's not just one architecture anymore. There are little ones coming up. But if we talk about the state-of-the-art, it's pretty much still the Transformer architecture, autoregressive, derived from GPT-2, essentially.

- I guess the big question here is: we talked quite a bit about the architecture behind the pre-training. Are the scaling laws holding strong across pre-training, post-training, inference, context size, data, and synthetic data?

- I'd like to start with the technical definition of a scaling law, which kind of informs all of this. A scaling law is the power law relationship between... You can think of the x-axis, which is kind of what you are scaling (a combination of compute and data, which are somewhat similar), and the y-axis, which is the held-out prediction accuracy over next tokens. We talked about models being autoregressive. It's like if you keep a set of text that the model hasn't seen, how accurate will it get when you train? And the idea of scaling laws came when people figured out that that was a very predictable relationship. And I think that technical term is continuing, and then the question is: what do users get out of it? And then there are more types of scaling, where OpenAI's o1 was famous for introducing inference-time scaling. And I think less famously for also showing that you can scale reinforcement learning training and get this kind of log x-axis and then a linear increase in performance on the y-axis. So there are kind of these three axes now: traditional scaling laws are talked about for pre-training, which is how big your model and dataset are. Then there's scaling reinforcement learning, which is like how long you can do this trial and error learning that we'll talk about.

Here is the cleaned-up portion of the transcript:

We'll define more of this, and then there's inference-time compute, which is letting the model generate more tokens on a specific problem. So, I'm kind of bullish, but they're all still really working. However, the low-hanging fruit has mostly been taken, especially in the last year, regarding reinforcement learning with verifiable rewards (RLVR) and inference-time scaling. This scaling is precisely why these models feel so different to use. Previously, you would get that first token immediately. Now, they'll go off for seconds, minutes, or even hours, generating these hidden thoughts before giving you the first word of your answer. And that's all about this inference-time scaling, which represents a wonderful step function in how the models change abilities. This has enabled tool use and much better software engineering, which we were talking about. And when we say &quot;enabled,&quot; this is almost entirely downstream of the fact that reinforcement learning with verifiable rewards training just let the models pick up these skills very easily. This allowed the models to learn.

If you look at the reasoning process when models generate a lot of tokens, what they'll often be doing is trying a tool, looking at what they get back, then trying another API, seeing what they get back, and checking if it solves the problem. The models, when trained, very quickly learn to do this. And then, at the end of the day, that provides a general foundation where the model can use CLI commands very nicely in your repo, handle Git for you, move things around, organize things, or search to find more information. A year ago, sitting in these chairs, we didn't really think of models doing this. So, this is something that has just happened this year and has totally transformed how we think of using AI, which I think is very magical. It's such an interesting evolution and unlocks so much value. But it's not clear what the next avenue will be in terms of unlocking such capabilities. I think we'll get to continual learning later, but there's a lot of buzz around certain areas of AI, yet no one knows when the next step function will really come.

- So, you've actually said quite a lot of profound things very quickly. It would be nice to unpack them a little bit. You say you're basically bullish on every version of scaling. So, can we just start at the beginning? Regarding pre-training, are we implying that the low-hanging fruit on pre-training scaling has been picked? Has pre-training hit a plateau, or is even pre-training still something you're bullish on?

- Pre-training has gotten extremely expensive. I think scaling up pre-training also implies serving a very large model to users. So, I think it's been loosely established that models like GPT-4 and similar ones were around one trillion parameters at their biggest size. There are many rumors that they've actually gotten smaller as training has become more efficient. You want to make the model smaller because your serving costs then decrease proportionately. For these models, the cost of training them is really low relative to the cost of serving them to hundreds of millions of users. DeepSeek had this famous number: about five million dollars for pre-training at cloud market rates. In the OLMo-1 section 2.4 paper, we detailed how long we had the GPU clusters sitting around for training, which included engineering issues and multiple seeds. It was about two million dollars to rent the cluster to deal with the headaches of training a model. So, these models are pretty accessible; many people could get one to 10 million dollars to train a model. However, the recurring costs of serving millions of users are truly billions of dollars of compute. I think you can look at a thousand-GPU rental, for which you can pay 100 grand a day. And these companies could have millions of GPUs. You can see how much these things cost just to sit idle. So, that's a big thing, and it makes you wonder: if scaling is actually giving you a better model, is it going to be financially worth it? We will slowly push it out as AI solves more compelling tasks, such as Claude 3 Opus, GPT-4.5, and making Claude Code just work for things. I launched this project called the ATOM project (American Truly Open Models) in July. That was like a true vibe-coded website, and I had a job to make plots and stuff. Then I came back to refresh it in the last few weeks, and it's like Claude 3 Opus versus whatever model at the time just crushed all the issues it had from building in June and July. It might be a bigger model; many things go into this, but there's still progress coming.

- So, what you're speaking to is the nuance of the Y-axis of the scaling laws: that the way it's experienced versus on a benchmark, the actual intelligence might be different. But still, based on your intuition about pre-training, if you scale the size of compute, will the models get better? Not whether it's financially viable, but purely from the law aspect of it, do you think the models will get smarter?

- Yeah. And I think that this sometimes comes off as almost disillusioned, coming from leadership at AI companies who say, &quot;It's held for 13 orders of magnitude of compute, so why would it ever end?&quot; So, fundamentally, I think it's pretty unlikely to stop. Eventually, we won't even be able to test the bigger scales because of all the problems that come with more compute. I think there's a lot of talk about how 2026 is the year when very large Blackwell compute clusters—gigawatt-scale facilities for hyperscalers—are coming online. These were all contracts for power and data centers that were signed and sought out in 2022 and 2023, before or right after ChatGPT. So, it took this two-to-three-year lead time to build these bigger clusters for training the models. While there's obviously immense interest in building even more data centers than that. So, that's kind of the crux of what people are saying: these new clusters are coming. The labs are going to have more compute for training; they're going to utilize this, but it's not a given. I've seen so much progress that I expect it, and I expect slightly bigger models. I would say it's more like we'll see a $2,000 subscription this year. We've seen $200 subscriptions, and that could 10X again. These are the kinds of things that could come, and they're all downstream of a bigger model that offers just a little bit more cutting edge.

- So, it's reported that xAI is going to hit that one-gigawatt scale early in 2026, and a full two gigawatts by year-end. How do you think they'll utilize that in the context of scaling laws? Is a lot of that for inference? Is a lot of that for training?

- It ends up being all of the above. I think all your decisions when training a model come back to pre-training. If you're going to scale RL on a model, you still need to decide on an architecture that enables this. We were talking about other architectures, using different types of attention, and also mixture-of-experts models. The sparse nature of MoE models makes it much more efficient for generation, which becomes a big part of post-training. You need to have your architecture ready so that you can actually scale up the compute. I still think most of the compute is going into pre-training because you can still make a model better, and you still want to revisit this. You still want the best base model you can get. In a few years, that will saturate, and the RL compute will just go longer.

- Are there people who disagree with you and say pre-training is dead, that it's all about scaling inference, post-training, context, continual learning, or synthetic data?

- People vibe that way and describe it in that way, but I think it's not the practice that is happening.
- [Interviewer]: It's just the general vibe of people saying this thing is dead...
- [Guest]: The excitement is elsewhere. The low-hanging fruit in RL is elsewhere. For example, we released our model in November. Every company has deadlines. Our deadline was November 20th, and for that, our run was five days, which, compared to 2024, is a very long time to be doing post-training on a model of 30 billion parameters. It's not a big model. Then in December, we had another release where we just let the RL run for another three and a half weeks, and the model got notably better, so we released it. That's a big amount of time to allocate to something that is going to be your peak for the year.
- [Interviewer]: So the reasoning is...
- [Guest]: There are these types of decisions that happen when training a model where they just can't—they can't leave it forever. You have to keep pulling in the improvements from your researchers. So, you redo pre-training, you'll do this post-training for a month, but then you need to give it to your users; you need to do safety testing. I think there's a lot in place that reinforces this cycle of updating the models. There are things to improve. You get a new compute cluster that lets you do something more stably or faster. You hear a lot about Blackwell having rollout issues. At AI2, most of the models we're pre-training are on 1,000 to 2,000 GPUs. But when you're pre-training on 10,000 or 100,000 GPUs, you encounter very different failures. GPUs are known to break in weird ways, and on a 100,000-GPU run, you're pretty much guaranteed to always have at least one GPU that is down. And you need your training code to handle that redundancy, which is a very different problem. Whereas what we're doing—like, &quot;Oh, I'm playing with post-training on DGX H100s,&quot; or with your book, or with people learning ML—what they're battling to train these biggest models is just massive distributed scale, and it's very different. But that's somewhat different from whether these... that's a systems problem in order to enable the scaling laws, especially at pre-training. You need all of these GPUs at once.

When we shift to reinforcement learning, it actually lends itself to heterogeneous compute because you have many copies of the model. To give a primer on language model reinforcement learning, what you're doing is having two sets of GPUs. One you can call the actor, and one you call the learner. The learner is where your actual reinforcement learning updates are done. These are traditionally policy gradient algorithms; Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) are the two popular classes. On the other side, you're going to have actors that are generating completions, and these completions are what you're going to grade. Reinforcement learning is all about optimizing reward. In practice, what you can do is have many different actors in different parts of the world solving different types of problems. Then you send the results back to a highly networked compute cluster to do the actual learning, where you take the gradients. You need to have a tightly meshed network where you can do different types of parallelism and spread out your model for efficient training. There are just many considerations for every different type of training and serving when you scale. We talked about pre-training, we talked about RL, and then inference-time scaling: how do you serve a model that's thinking for an hour to 100 million users? I'm like, &quot;I don't really know about that, but I know that's a hard problem. In order to give people this intelligence, there are all these systems problems, and we need more compute and more stable compute to do it.&quot;

- But you're bullish on all these kinds of scaling, is what I'm hearing? On inference, on reasoning, even on pre-training?

- Yeah, that's a big can of worms, but there are basically two... The knobs are training and inference scaling where you can get gains. In a world with, let's say, infinite compute resources, you'd want to do all of them. So you have training and inference scaling, and training itself is like a hierarchy: pre-training, mid-training, post-training. Changing the model size, using more training data, or training a bigger model gives you more knowledge within the model. Then the model, let's say, has a better... it's like a better base model. We still call it a foundation model, and it unlocks capabilities, but you don't, let's say, have the model be able to solve your most complex tasks during or immediately after pre-training. You still have these other &quot;unlock&quot; phases, like mid-training or post-training with RL, that unlock capabilities the model has from its pre-training knowledge. And I think, sure, if you do more pre-training, you get a better base model that you can unlock later. But as Nathan said, it just becomes too expensive. We don't have infinite compute, so you have to decide: do I want to spend that compute more on making the model larger? It's a trade-off. In an ideal world, you'd want to do all of them. And I think in that sense, scaling is still pretty much alive. You would still get a better model, but as we saw with GPT-4o, it's just not worth it because you can unlock more performance with other techniques at this moment, especially if you look at inference scaling. That's one of the biggest gains this year with o1, where it took a smaller model further than pre-training a larger model like GPT-4o. So, I wouldn't say pre-training scaling is dead; it's just that there are other more attractive ways to scale right now. But at some point, you will still want to make some progress on pre-training. Also, the thing to consider is where you want to spend your money. If you spend it more on pre-training, it's like a fixed cost: you train the model, and then it has this capability forever. You can always use it. With inference scaling, you don't spend money during training; you spend money later per query. Then it's also about the math: how long is my model going to be on the market if I replace it in half a year? Maybe it's not worth spending 5 million, 10 million, or 100 million dollars on training it longer. Maybe I will just do more inference scaling and get the performance from that. It might cost me two million in terms of user queries. It becomes a question of how many users you have and then doing the math, and I think that's also where it's interesting, where ChatGPT is in a specific position. I think they have many users where they need to go a bit cheaper, where they have that GPT-4o model that is a bit smaller. Other companies, for example, might have customers with different trade-offs. For instance, there was also the Math Olympiad or some of these math problems where ChatGPT or a proprietary model was used, and I'm pretty sure it's just a model that has been maybe fine-tuned a little bit more, but most of it was during inference scaling to achieve peak performance in certain tasks. They don't need that all the time. But yeah, long story short, I do think all of these—pre-training, mid-training, post-training, inference scaling—are still things you want to do. It's just finding... at the moment, this year, it's about finding the right ratio that gives you the best bang for the buck, basically.

- I think this might be a good place to define pre-training, mid-training, and post-training.

- So, pre-training is the classic training: one next-token prediction at a time. You have a big corpus of data. And Nathan probably also has very interesting insights there because of OLMo. A big portion of the paper focuses on the right data mix. So, pre-training is essentially just training a cross-entropy loss, focusing on next-token prediction on a vast corpus of internet data, books, papers, and so forth. It has changed a little bit over the years in the sense that people used to throw in everything they could. Now, it's not just raw data; it's also synthetic data where people rephrase certain things. So, synthetic data doesn't necessarily mean purely AI-made data. It's also taking something from an article—a Wikipedia article, for example—and then rephrasing it as a Q&amp;A question, or summarizing, rewording it, and making better data that way. Because I think of it similarly to humans. If someone, let's say, reads a book compared to a messy—no offense, but, like, a Reddit post or something like that—I do think you learn...

- There's going to be a post about this, Raschka.

- [Guest 1]: Some Reddit data is very coveted and excellent for training; you just have to filter it.
- [Guest 1]: And I think that's the idea: if someone took that and rephrased it in a, let's say, more concise and structured way—I think it's higher-quality data that gets the LLM to the same place. You get the same LLM out of it at the end, but it gets there faster. It trains faster because if the grammar and punctuation are correct, it already learns the correct way versus getting information from a messy source and then learning later how to correct that. So, I think that is how pre-training evolved, and why scaling still works is that it's not just about the amount of data; it's also about the tricks to make that data better for you, in a sense. And then mid-training is... I mean, it used to be called pre-training. I think it's called mid-training because it was awkward to have &quot;pre-training&quot; and &quot;post-training&quot; with nothing in the middle, right? It sounds a bit weird. You have pre-training and post-training, but what is the actual training? So, mid-training is usually similar to pre-training, but it's a bit more specialized. It's the same algorithm, but what you do is focus, for example, on long-context documents. One reason you don't do that during pre-training is because you don't have that many long-context documents. We have a specific phase for this. And one problem with LLMs is also still... it's a neural network, and it has the problem of catastrophic forgetting. So, you teach it something, and it forgets other things. And you want to... I mean, it's not 100% forgetting, but it's like no free lunch; you can't... It's also the same with humans. If you ask me some math I learned 10 years ago, I wouldn't know; I would have to look at it again.

- Nathan was actually saying that he's consuming so much content that he's experiencing a catastrophic forgetting issue.

- Yeah, I'm trying to learn so much about AI, and it's like I was learning about pre-training parallelism, and then I'm like, 'I lost something, and I don't know what it was.'

- I don't want to anthropomorphize LLMs, but I think it's the same kind of sense in how humans learn. Quantity is not always better because you have to be selective. And mid-training involves being selective in terms of quality content at the end. So, the last thing the LLM sees is the quality content. And then post-training encompasses all the fine-tuning: supervised fine-tuning, DPO, reinforcement learning with verifiable rewards, human feedback, and so forth—the refinement stages. And the cost aspect is also interesting, right? For pre-training, you spend a lot of money on that right now; RL costs a bit less. With RL, you don't really, I would say, teach it knowledge. It's more like unlocking existing knowledge; it's more like skill learning—how to solve problems with the knowledge it has from pre-training. There are actually three papers this year (or last year, 2024) on RL for pre-training, but I don't think anyone does that in production.
- [Guest 2]: Toy, toy examples for now.
- [Guest 1]: Toy examples, right. But to generalize, RL post-training is more like skill unlocking, where pre-training is like soaking up the knowledge essentially.

- A few things that could be helpful for people: many people think of synthetic data as being bad for training models. You mentioned the DeepSeek-V3 Almost OCR—which is an Optical Character Recognition paper. Many labs did this; AI2 had one, and Nougat had multiple. The reason each of these labs has these is because there are vast amounts of PDFs and other digital documents on the web that are in formats not easily encoded with text. So, you use this Almost OCR, or DeepSeek OCR (what we called our Almost OCR), to extract what can be trillions of tokens of candidate data for pre-training. Pre-training dataset size is on the order of trillions—it is measured in trillions of tokens. Smaller models from researchers can use something like 5 to 10 trillion. Qwen is documented going up to 50 trillion, and there are rumors that these closed labs can go up to 100 trillion tokens. And just getting this potential data to put in, they have a very big funnel. Then the data you actually train the model on is a small percentage of this. This character recognition data would be described as synthetic data for pre-training in a lab. And then there's also the fact that ChatGPT now gives wonderful answers, and you can train on those best answers, which is also synthetic data. It's very different from early ChatGPT, with lots of hallucination data, when people became grounded in synthetic data.

- One interesting question is, if I recall correctly, OLMo-3 was trained with less data than some other open-weight models—maybe even OLMo-2. But you still got better performance, and that might be one of the examples of how the data helped.

- It's mostly down to data quality. I think if we had more compute, we would train for longer. We'd see that as something we would want to do. Especially with big models, you need more compute, because we talked about having more parameters and knowledge. Essentially, there's a ratio where big models can absorb more from data, and you get more benefit from this. It's like this: in any logarithmic graph in your mind, a small model will level off sooner if you're measuring tons of tokens, and bigger models need more. But mostly, we aren't training that big of models right now at AI2, and getting the highest quality data we can is the natural starting point.

- Is there something to be said about the topic of data quality? Is there still some low-hanging fruit where the quality could be improved?

- It's like turning the crank. Historically, in the open, there's been a canonical &quot;best&quot; pre-training dataset that has moved around between whoever has the most recent or best effort. For example, AI2's Dolma was very early with the first OLMo, and Hugging Face had FineWeb. There's a DCLM project, which has been like a—it stands for DataComp Language Model. There's been DataComp for other machine learning projects, and they had a very strong dataset. Much of it is because the internet is becoming fairly closed off, so we have Common Crawl, which is hundreds of trillions of tokens, and you filter it. It looks like scientific work where you're training classifiers and making decisions based on how to prune down this dataset into the highest quality content that suits your tasks. Previously, language models were tested much more on knowledge and conversational things, but now they're expected to do math and code. To train a reasoning model, you need to remix your whole dataset. There are actually wonderful scientific methods here where you can take your gigantic dataset and sample many tiny things from different sources. So, say you have GitHub, Stack Exchange, Reddit, Wikipedia. You can sample small things from them, train small models on each mix, and measure their performance on your evaluations. You can just do basic linear regression, and it's like, 'Here's your optimal dataset.' But if evaluations change, your dataset changes significantly. So, much of OLMo-3 involved new sources for reasoning to be better at math and code. Then you do this mixing procedure, and it gives you the answer. I think much of that's happened at labs this year, as there are new hot things, whether it's coding environments or web navigation, and you just need to bring in new data. You need to change your pre-training so that your post-training can work better. That's like the constant evolution and redetermination of what they care about for their models.

- Are there fun anecdotes of what sources of data are particularly high quality that we wouldn't expect? You mentioned Reddit can sometimes be a source.

- [Guest 2]: Reddit was very useful. I think PDFs are definitely one.
- [Interviewer]: Oh, especially arXiv.
- [Guest 2]: Yeah, AI2 has run Semantic Scholar for a long time, which you can say is a competitor to Google Scholar with many more features. To do this, AI2 has found and scraped many PDFs for openly accessible papers that might not be behind the closed walled garden of a certain publisher. So, truly open scientific PDFs. If you sit on all of these and process them, you can get value out of them. And I think that much of that style of work has been done by the frontier labs much earlier. You need a pretty skilled researcher that understands how things change models, and they bring it in and clean it. It's a lot of labor. I think at frontier labs, when they scale researchers, much more goes into data. If you join a frontier lab and want to have impact, the best way to do it is just find new, better data. Rather than the fancy, glamorous algorithmic things like figuring out how to make o1—that's the sexiest thought of a scientist: 'Oh, I figured out how to scale RL.' There's a group that did that, but most of the contributions come from...
- [Interviewer]: On the dataset...
- [Guest 2]: ...saying, 'I'm going to make the data better,' or, 'I'm going to make the infrastructure better so that everyone on my team can run experiments 5% faster.'

- At the same time, I think it's also one of the closest guarded secrets what your training data is, for legal reasons. So there's also, I think, a lot of work that goes into essentially hiding what your training data was—tuning the model to not give away the sources because of legal reasons.

- The other thing, to be complete, is that some people are trying to train on only licensed data, whereas Common Crawl is a scrape of the whole internet. So, if I host multiple websites, I'm happy to have them train language models, but I'm not explicitly licensing what governs it. Therefore, Common Crawl is largely unlicensed, which means your consent hasn't really been provided for how to use the data. There's another idea where you can train language models only on data that has been explicitly licensed, so that the governing contract is provided. I'm not sure if Appertus refers to the copyright or the license. I know the reason they did it was for an EU compliance issue, where they wanted to make sure their model fit one of those checks.

- On that note, there's also the distinction in licensing. Some people, like you said, just purchase the license. Let's say they buy a book online, like an Amazon Kindle book or a Manning book, and then use that in training data. That is a gray zone, 'cause you paid for the content, and you might want to train on it. But then there are also restrictions where even that shouldn't be allowed, which is where it gets a bit fuzzy. And I think that is right now still a hot topic. Also, big companies like OpenAI approached private companies for their proprietary data, and private companies are becoming more protective of their data because they know, 'Okay, this is going to be my moat in a few years.' And I do think that's the interesting question: if LLMs become more commoditized and many people learn about LLMs, there will be many more people able to train LLMs. Of course, there are infrastructure challenges. But if you think of big industries like pharmaceutical, law, and finance, I do think they will, at some point, hire people from other frontier labs to build their in-house models on their proprietary data, which will be another unlock with pre-training that is currently not available. Because even if you wanted to, you can't get that data. You can't get access to clinical trials most of the time, or these types of things. So, I do think scaling, in that sense, might still be pretty much alive if you also look at domain-specific applications, because we are still just looking at general-purpose LLMs like ChatGPT, Anthropic, and so forth. They are just general-purpose; they're not even scratching the surface of what an LLM can do if it is specifically trained and designed for a specific task.

- I think on the data thing, this is one of the things that happened in 2025 that we totally forget: Anthropic lost in court and owed $1.5 billion to authors. Anthropic, I think, bought thousands of books, scanned them, and was cleared legally for that because they bought the books; that is kind of going through the system. Then, on the other side, they also torrented some books, and I think this torrenting was the reason the court said they were culpable to pay these billions of dollars to authors, which is just such a mind-boggling lawsuit that just came and went. That is so much money—so much money from the VC ecosystem.

- These are court cases that will define the future of human civilization, because it's clear that data drives much of this, and there's this very complicated human tension. I mean, you can empathize: you're both authors. There's some degree to which you put your heart and soul, your sweat, and your tears into the writing that you do. It feels a little bit like theft for somebody to train on your data without giving you credit.

- And there are also two layers to it, as Nathan said. Someone might buy the book and then train on it, which could be argued as fair or not. But then there are the straight-up companies who use pirated books, where it's not even compensating the author. That is where people got a bit angry about it, specifically, I would say.

- Yeah, but there has to be some kind of compensation scheme. This is moving towards something like what Spotify streaming did originally for music. What does that compensation look like? You have to define those models and think through all of that. One other thing people are generally curious about, and I'd love to get your thoughts on, is: as LLMs are used more and more, if you look at arXiv or GitHub, more and more of the data is generated by LLMs. What do you do in that kind of world? How big of a problem is that?

- The largest problem is the infrastructure and systems, but from an AI point of view, it's kind of inevitable.

- So, it's basically LLM-generated data that's essentially curated by humans, right?

- Yes, and I think many open-source contributors are legitimately burning out. If you have a popular open-source repo, somebody's like, 'Oh, I want to do open-source AI. It's good for my career,' and they just vibe-code something and throw it in. You might get more of this...
- [Interviewer]: I have a...
- [Guest 2]: ...than I do.

- Yeah, I actually have a case study here. I have a repository called mlxtend that I developed as a student around 10 years ago, and it is still a reasonably popular library for certain algorithms, especially for frequent data mining tasks. Recently, two or three people submitted many PRs in a very short amount of time. I do think LLMs have been involved in submitting these PRs. As the maintainer, there are two things: first, I'm a bit overwhelmed. I don't have time to read through it because, especially as an older library, it is not a priority for me. At the same time, I also appreciate it because I think something people forget is that it's not just using the LLM. There's still a human layer that verifies something, and that, in a sense, is also how data is labeled, right? One of the most expensive things is getting labeled data for reinforcement learning from human feedback phases. And this is kind of like that, where it goes through phases, and then you get higher-quality data out of it. So I don't mind it, in a sense. It can feel overwhelming, but I do think there is value in it.

- It feels like there's a fundamental difference between raw LLM-generated data and LLM-generated data with a human in the loop who does some kind of verification, even if that verification accounts for a small percentage of the lines of code.

- I think this goes with anything, where people sometimes also think, 'Oh, yeah. I can just use an LLM to learn about XYZ,' which is true. You can, but there might be a person who is an expert who might have used an LLM to write specific code. There is human work that went into it to make it nice, throwing out the not-so-nice parts to kind of pre-digest it for you, and that saves you time. I think that's the value-add, where you have someone filtering things or even using the LLMs correctly. This is still labor that you get for free. For example, reading an article—let's say a Substack article—I could maybe ask an LLM to give me opinions on that, but I wouldn't even know what to ask. I think there is still value in reading that article compared to me going to the LLM, because you are the expert. You select what knowledge is actually spot-on, what should be included, and you give me this very executive summary. This is a huge value-add because now I don't have to waste three or five hours to go through this myself, potentially getting some incorrect information, and so on. So I think that's also where the future still lies for writers, even though there are LLMs that can save you time.

- It's fascinating to actually watch—and I'm sure you guys do this—the difference between a summary and the original content. Even if it's a page-long summary of page-long content, it's interesting to see how the LLM-based summary takes the edge off. Is it the signal it removes from the content?

- The voice is what I talk about a lot.

- Voice? I would love to hear what you mean by voice, but sometimes there are literally insights. By removing an insight, you're fundamentally changing the meaning of the content. I'm continuously disappointed by how bad LLMs are at really getting to the core insights, which is what a great summary does. Yet, even if I have these extensive, extremely elaborate prompts where I'm really trying to dig for the insights, it's still not quite there. I mean, that's a whole deep philosophical question about what constitutes human knowledge and wisdom and what it means to be insightful, and so on. But when you talk about 'the voice,' what do you mean?

- So, when I write, I think much of what I'm trying to do is take what you think as a researcher—which is very raw. A researcher is trying to encapsulate an idea at the frontier of their understanding and put a feeling into words. I try to do this with my writing, which makes it come across as raw but also high-information in a way that some people will get and some won't. That's the nature of research. I think this is something language models don't do well. They're all trained with RLHF, which is designed to take feedback from many people and average how the model behaves. And I think it's going to be hard for a model to be very incisive when there's that sort of filter in it. This is a wonderful, fundamental problem for researchers in RLHF: it provides so much utility in making the models better, but the problem formulation also has this knot in it that you can't get past. These language models don't have this prior in the deep expression they're trying to achieve. I don't think it's impossible. There are stories of models that really shock people. I would have loved to try Bing Sydney. Did that have more voice? Because it would often go off the rails on people in what is, obviously, a historically scary way—like telling a reporter to leave his wife—which is crazy to potentially put into general adoption. But that's kind of a trade-off: is this RLHF process in some ways adding limitations?

- That's a terrifying place to be for one of these frontier labs and companies, because millions of people are using them.

- There was a lot of backlash last year with GPT-4 getting removed. I've never used the model, but I've talked to people at OpenAI who get emails from users detecting subtle differences in the deployments, even in the middle of the night. They're like, 'My friend is different.' They find employees' emails because they are so attached to this set of model weights and a deployed configuration. We see this with TikTok. I don't use TikTok, but supposedly, in five minutes, the algorithm 'gets' you. It's like it's locked in. Those are language models doing recommendations. I think there are ways you can do this. Within five minutes of chatting, the model just 'gets' you. And that is something people aren't really ready for. I think—don't give that to kids... at least until we know what's happening.

- There's also going to be this mechanism: what's going to happen with these LLMs as they're used more and more? Unfortunately, the nature of the human condition is such that people commit suicide. Journalists will report extensively on the people who commit suicide, and they would very likely link it to the LLMs because they have data about those conversations. If you're really struggling, depressed, or thinking about suicide, you're probably going to talk to LLMs about it. And so journalists will say, 'Well, the suicide was committed because of the LLM.' And that's going to lead to companies, because of legal issues, taking the edge off the LLM. So, it's going to be as generic as possible. It's so difficult to operate in this space because you don't want an LLM to cause harm to humans at that level. But this is also the nature of the human experience: to have a rich, fulfilling conversation—one that challenges you and from which you grow. You need that edge. And that is something extremely difficult for AI researchers on the RLHF front to solve because you're actually dealing with the human condition.

- Many researchers at these companies are so well-motivated; Anthropic and OpenAI culturally want to do good for the world. And it's such a... I'm like, 'I don't want to work on this,' because many people see AI as a health ally, as someone they can talk to about their health confidentially. But then it bleeds into talking about mental health, where it's heartbreaking that this will be the thing where somebody goes over the edge, but other people might be saved. And I'm like, 'I don't...' There are things that, as a researcher training models, I don't want to train image generation models and release them openly because I don't want to enable somebody to have a tool on their laptop that can harm other people. I don't have the infrastructure in my company to do that safely. But... there are many areas like this where it just needs people who will approach it with the complexity and conviction because it's just such a hard problem.

- But also, we as a society—as users of these technologies—need to ensure that we're having the complicated conversation about it versus just fearmongering. 'Big tech is causing harm to humans,' or 'stealing your data,' all that kind of stuff. It's more complicated than that, and you're right. There's a very large number of people inside these companies, many of whom you know and many of whom I know, who deeply care about helping people. They are considering the full human experience of people from across the world—not just Silicon Valley; people across the United States, people across the world—what that means, what their needs are. It's really difficult to design this one system that is able to help all these different kinds of people across different age groups, cultures, mental states, and conditions.

- I wish the timing of AI was different regarding Big Tech's relationship to the average person. Big Tech's reputation was so low, and with how expensive AI is, it's inevitably going to be a Big Tech thing. It takes so many resources, and people say the US is, quote-unquote, 'betting the economy on AI' with this build-out. To have these intertwined at the same time makes for such a hard communication environment. It would be good for me to go talk to more people in the world who hate Big Tech and see AI as a continuation of this.

- And one of the things you recommend—one of the antidotes you talk about—is to find agency in this whole system. As opposed to sitting back powerlessly and consuming the AI 'slop' as it quickly and rapidly takes over the internet. Find agency by using AI to build things, build apps... So, first, that actually helps you build intuition; but second, it's empowering because you can understand how it works and what its weaknesses are. It gives your voice the power to say, 'This is bad use of the technology, and this is good use of technology.' And you're more plugged into the system, so you can understand it better and steer it better as a consumer.

- I think that's a good point about agency. Instead of ignoring it and saying, 'Okay, I'm not going to use it,' I think it's probably long-term healthier to say, 'Okay, it's out there.' Like the internet and computers back when they came out: 'How do I make the best use of it, and how does it help me to up-level myself?' The one thing I worry about, though, is if you just fully use it for something you love to do, that thing you love to do is no longer there. And that could potentially lead to burnout. For example, if I use an LLM to do all my coding, now there's no coding; I'm just managing something that is coding for me. Two years later, if I just do that eight hours a day—having something code for me—do I still feel fulfilled? I mean, is this hurting me in terms of being excited about my job and what I'm doing? Am I still proud to build something?

- So, on that topic of enjoyment, it's quite interesting. We should just throw this in there: there's this recent survey of about 791 professional developers ('professional' meaning 10-plus years of experience).
- [Guest 1]: That's a long time. As a junior developer?
- [Host]: Yeah, in this day and age.
- [Host]: So, there are also aspects on many fronts that are surprising. They break it down by junior and senior developers. But it just shows that both junior and senior developers use AI-generated code in code they ship. So, this is not just for fun or intermediate learning; this is code they ship. And so, 25%—most of them use around 50% or more. What's interesting is that for the category of over 50% of your code that you ship being AI-generated, senior developers are much more likely to do so. But you don't want AI to take away the thing you love. I think this speaks to my experience—these particular results I'm about to say: together, about 80% of people find it either somewhat more enjoyable or significantly more enjoyable to use AI as part of their work.

- I think it depends on the task. From my personal usage, for example, I have a website where I sometimes tweak things. I personally don't enjoy this, so in that sense, if the AI can help me implement something on my website, I'm all for it. It's great. But then, at the same time, when I solve a complex problem—if there's a bug and I hunt this bug and find it—it's the best feeling in the world. You get so much joy; you feel great. But now, if you don't even think about the bug and just go directly to the LLM, you never have this kind of feeling, right? But then there could be the middle ground where you try yourself, can't find it, use the LLM, and then you don't get frustrated because it helps you move on to something you enjoy. So, looking at these statistics, I think the difference is also what is not factored in; it's averaging over all the different scenarios. We don't know if it's for the core task or for something mundane that people would not have enjoyed otherwise. So, in a sense, AI is really great for doing mundane things that take a lot of work. For example, my wife the other day, who has a podcast for book discussions (a book club), was transferring the show notes from Spotify to YouTube, and the links somehow broke. She had in some episodes—because there are so many books—like 100 links, and it would have been really painful to go in and fix each link manually. So I suggested, 'Hey, let's try ChatGPT.' We copied the text into ChatGPT, and it fixed them. Instead of two hours going from link to link, it made that type of work much more seamless. I think everyone has a use case where AI is useful for something that would be really boring, really mundane.

- For me personally, since we're talking about coding, and you mentioned debugging... much of the source of enjoyment for me—more on the Cursor side than the Cloud side—is having a friend, a—what's that called? A pair programmer. It's less lonely. You made debugging sound like this great joy. I would say debugging is like a drink of water after you've been going through a desert for days. So, you skip the whole desert part where you're suffering. Sometimes it's nice to have a friend who can't really find the bug but can give you some intuition about the code. You're together with that friend going through the desert, and then together, you find that drink of water. At least for me, maybe it speaks to the loneliness of the programming experience. That is a source of joy.

- It's maybe also related to delayed gratification. Even as a kid, I liked the idea of Christmas presents—having them, looking forward to them—better than actually getting them. I would look forward to the day, but then it's over, and I'm disappointed. It's similar to food, I think; food tastes better when you're really hungry. And you're right, with debugging, it is not always great. It's often frustrating, but if you can solve it, then it's great. There's a sweet Goldilocks zone: if it's too hard, then you're wasting your time. But I think that is another challenge: how will people learn? In the chart we looked at, we saw that more senior developers are shipping more AI-generated code than junior ones. It's very interesting, because intuitively, you would think it's the junior developers, because they don't know how to do the thing yet. It could mean the AI is not good enough yet, or it could also mean experts are more effective at using it. They know how to use it better, review the code, and trust it more. One issue in the future will be: how do you become an expert if you never try to do the thing yourself? The way I learned is by trying things myself. Like math textbooks: if you just look at the solutions, you learn something, but I think you learn better if you try first. You appreciate the solution differently because you can put it into your mental framework. If LLMs are here all the time, would you actually go through the struggle? Struggle is not nice, right? But if you use the LLM to do everything, at some point you will never really take the next step, and then you might not get that unlock that you would get as an expert using an LLM. So, I think there's a Goldilocks sweet spot where maybe the trick is to make dedicated offline time where you study two hours a day, and then for the rest of the day, use LLMs. But I think it's also important for people to still invest in themselves, in my opinion, to not just LLM everything.

- Yeah, we as a civilization—we each individually—have to find that Goldilocks zone, and in the programming context as developers. Now, we've had this fascinating conversation that started with pre-training and mid-training. Let's get to post-training. A lot of fun stuff in post-training. So, what are some of the interesting ideas in post-training?

- The biggest one from 2025 is learning about reinforcement learning with verifiable rewards. You can scale up the training there, which means doing a lot of this iterative generate-grade loop, and that lets the models learn both interesting behaviors on the tool use and software side. This could be searching, running commands on their own and seeing the outputs. That training also enables this inference-time scaling very nicely. And it just turned out that this paradigm was very nicely linked, where this RL training enables inference-time scaling. But inference-time scaling could have been found in different ways. So, it was a perfect storm where models change a lot, and the way they're trained is a major factor in doing so. And this has changed how people approach post-training dramatically.

- Can you describe RLVR, popularized by DeepSeek R1? Can you describe how it works?

- Yeah. Fun fact, I was on the team that came up with the term RLVR, which is from our Tulu 3 work before DeepSeek. We don't take credit for being the people to popularize scaling RL, but, as an aside, the ability academics get is to name and influence the discourse because the closed labs can only say so much. One of the things you can do as an academic is you might not have the compute to train the model, but you can frame things in a way that ends up being—I describe it as—a community can come together around this RLVR term, which is very fun.

**Speaker 1:** And then DeepSeek are the people who did the training breakthrough, which means they scaled reinforcement learning where the model generates answers, and the completion is then graded for correctness. That accuracy then serves as the reward for reinforcement learning. So reinforcement learning classically involves an agent acting in an environment, and the environment gives back a state and a reward, and you try to maximize this reward. In the case of language models, the reward is typically accuracy on a set of verifiable tasks, whether it's math problems or coding tasks. This also applies to factual domains, which are, in some ways, verifiable, or to constraints on instructions, such as &quot;respond only with words that start with A.&quot; All of these things are verifiable in some way, and the core idea is to find a lot more of these verifiable problems and allow the model to attempt them many times, taking these RL steps—these RL gradient updates. The infrastructure evolved from Reinforcement Learning from Human Feedback (RLHF), where the score they were optimizing was a learned reward model based on aggregate human preferences. So you change the problem domains, and that allowed the optimization to proceed at much larger scales, which essentially kickstarted a major change in what the models can do and how people use them.

**Speaker 2:** What kind of domains is RLVR amenable to?

**Speaker 1:** Math and code are the famous ones. Then there's a lot of work on what is called rubrics, which is related to a concept people might have heard of: &quot;LLM-as-a-judge.&quot; For example, for each problem within my training dataset, I will then have another language model and ask it, &quot;What would a good answer to this problem look like?&quot; And then you can try the problem over and over again and assign a score based on this rubric. So that's not necessarily as verifiable as a math and code domain, but this rubrics idea, along with other scientific problems that might be a little more vague, is where a lot of the attention is focused—where they're trying to push this set of methods into these more open-ended domains so the models can learn a lot more.

**Speaker 2:** I think that's called Reinforcement Learning with AI Feedback, right?

**Speaker 1:** That's the older term coined in Anthropic's Constitutional AI paper. So a lot of these things come in cycles.

**Speaker 2:** Also, just to take one step back for RLVR. The interesting thing about RLVR is that you can ask the LLM, for example, a math question, and while you know the correct answer and allow the LLM to figure it out, as you said, the method it uses... you don't really constrain it much. There are some constraints you can add, such as &quot;use the same language; don't switch between Spanish and English.&quot; But let's say you're pretty much hands-off. You only give the question and the answer, and then the LLM's task is simply to arrive at the right answer. But the beautiful thing that happens in practice is that the LLM will provide a step-by-step description, much like a student or a mathematician would, on how to derive the solution. It will use those steps, and that actually helps the model to improve its own accuracy. And then, as you mentioned, there's inference scaling. Loosely, inference scaling means spending more compute when using the LLM during inference, which, in this context, implies the model would use more tokens. In the R1 paper, they showed the longer they train the model, the longer the responses are. They grow over time. They use more tokens, so it becomes more expensive for simple tasks. However, these explanations help the model improve its accuracy. There are also interesting papers showing that what the model explains does not necessarily have to be correct, or maybe it's even unrelated to the answer, but for some reason, it still helps the model. It's the fact that it is explaining. And I think it's also—again, I don't want to anthropomorphize these LLMs—but it's kind of like how we humans operate, right? If there's a complex math problem, let's say in a math class, you usually have scratch paper and you do it step by step. You cross out things. And the model also self-corrects, and that was, I believe, the &quot;aha moment&quot; in the R1 paper. They called it the aha moment because the model itself recognized it had made a mistake and then, in effect, said, &quot;Ah, I did something wrong, let me try again.&quot; And I think that's just so cool that this falls out of simply giving it the correct answer and letting it figure out how to proceed—that it, in a sense, does what a human would do. Although LLMs don't think like humans, it's an interesting coincidence. The other nice side effect is that it's often great for us humans to see these steps. It builds trust, but also we learn and can double-check things.

**Speaker 1:** There's a lot in here. I think there's been a lot of debate this year on whether language models truly experience these &quot;aha moments.&quot; I think these &quot;aha moments&quot; are somewhat misleading because in pre-training, you've essentially seen the entire internet. So you have definitely seen people explaining their work, even verbally, such as in a transcript of a math lecture where someone might say, &quot;You try this, oh, I messed this up.&quot; And what reinforcement learning, this RLVR, is very good at is amplifying these behaviors because they're very useful in enabling the model to think longer and to check its work. And I agree that it is very beautiful that this training allows the model to learn to amplify this in a way that is just so useful in making the final answers better.

**Speaker 2:** I can also give you a hands-on example. I was training the Qwen 2.5 base model with RLVR on MATH 500. The base model had an accuracy of about 15%. In just 50 steps, within a few minutes using RLVR, the model went from 15% to 50% accuracy. And the model... you can't tell me it's learning anything fundamentally new about math in—

**Speaker 1:** The Qwen example is weird because there've been two papers this year, one of which I co-authored, about data contamination in Qwen—specifically, that they train on a lot of this special mid-training phase that we—

**Speaker 2:** Exactly. Exactly.

**Speaker 1:** —that we spent a minute on because it's unusual.

**Speaker 2:** And so—

**Speaker 1:** —because they train on problems that are almost identical to MATH.

**Speaker 2:** Exactly. And so you can see that, essentially, RL isn't teaching the model any new knowledge about math. You can't do that in 50 steps. So the knowledge is already present from pre-training; you're just unlocking it.

**Speaker 1:** I still disagree with the premise because there's a lot of complexities that you can't prove. One of the things that points to this unusual behavior is that if you take the Qwen 2.5 so-called base model, you could Google &quot;math dataset, Hugging Face,&quot; take a problem, and if you put it into Qwen 2.5 base, all these math problems have words, so it might be a word problem like, &quot;Alice has five apples and takes one and gives three to whoever.&quot; Why people are suspicious of these Qwen-based models is if you change the numbers but keep the words, Qwen will produce—even without external tools—a very high accuracy decimal representation of the answer. This means that, at some point, it was shown problems almost identical to the test set, and it was using tools to get a very high-precision answer, something a language model without tools would never actually achieve. So it's been a significant debate in the research community: how much can you believe these reinforcement learning papers that train on Qwen and measure performance specifically on this math benchmark, given that multiple papers have discussed contamination? I think this is what caused the reputation of RLVR being about formatting, because you can get these gains so quickly and therefore it must already be in the model. But there's a lot of complexity here; it's not really like controlled experimentation, so we don't really know.

**Speaker 2:** But if it weren't true, I would say distillation wouldn't work, right? Distillation can work to some extent, but the biggest problem is research contamination, because we don't know what's in the data. Unless you have a new dataset, it is really impossible. And similarly—you mentioned the MATH dataset, where you have a question, an answer, and an explanation. But even with something simpler like MMLU, which is a multiple-choice benchmark, if you just change the format slightly—for example, if you use a dot instead of a parenthesis or something similar—the model accuracy will vastly differ.

**Speaker 1:** I think that could be a model issue rather than a general issue.

**Speaker 2:** It's not even malicious by the developers of the LLM, as if to say, &quot;Hey, we want to cheat at that benchmark.&quot; It has just seen something at some point. I think the only fair way to evaluate an LLM is to have a new benchmark that was created after the LLM's deployment cutoff date.

**Speaker 2:** Can we lay out what would be the recipe of all the things that go into post-training? And you mentioned RLVR was a really exciting, effective thing. Maybe we should elaborate. RLHF still has a very important role to play. What other ideas are there for post-training?

**Speaker 1:** I think you can approach this sequentially. You could view it as what made o1, which was the first reasoning model, possible, or what the latest models will entail. These stages actually involve similar interventions, where you start with mid-training, and what is rumored to enable o1 and similar models is truly careful data curation, where you provide a broad set of what are called reasoning traces. These are essentially the model generating words in a forward process that reflects, for example, breaking down a problem into intermediate steps and trying to solve them. At mid-training, you need data similar to this so that when you move into post-training, primarily with these verifiable rewards, it can learn. And then what is happening today is you're figuring out which problems to give the model and how long you can train it for and how much inference you can enable the model to use when solving these verifiable problems. As models get better, certain problems are no longer... the model will solve them 100% of the time, and therefore there's very little signal in this. The GRPO equation is famous for this because the reward given to the agent is based on how good a given action (a completion) is relative to the other answers for that same problem. So if all problems yield the same answer, there's no signal. So they're finding harder problems, which is why you hear about things like scientific domains, which are inherently difficult. Getting anything right there, if you have a lab, for instance, it just generates so many tokens, or much harder software problems. The frontier models are all pushing into these harder domains, where they can train on more problems and learn more skills at once. The RLHF link to this is that RLHF has been and still serves as the finishing touch on the models, where it makes them more useful by improving the organization, style, or tone. Different things resonate with different audiences; some people like a quirky model, and RLHF can be effective at enabling that personality. And some people hate the markdown bulleted list format that models often use, but it's actually really good for quickly parsing information. In RLHF, this human feedback stage is very effective for incorporating this into the model at the end of the day. It's what made ChatGPT so magical for people. And that use has actually remained fairly stable. This formatting can also help models improve at math problems, for example. So the border between style and formatting, and the method used to answer a problem, is actually—they are all very closely linked during model training. This is why RLHF can still improve a model's math ability, but verifiable domains offer a much more direct process for doing so because it aligns better with the problem formulation. This is why everything ultimately forms together. But to summarize: Mid-training provides the model with the necessary skills for subsequent learning. RL with verifiable rewards is allowing the model to try many times—investing significant compute into trial-and-error learning across difficult problems. And then RLHF finishes the model, making it easy to use and generally more polished.

**Speaker 2:** Can you comment on the amount of compute required for RLVR?

**Speaker 1:** It's only gone up and up. I think Greg Brockman famously said they use a similar amount of compute for pre-training and post-training. Back to the scaling discussion, they involve very different hardware architectures for scaling. Pre-training is very compute-bound, which relates to the FLOPs discussion—how many matrix multiplications you can get through at once. Because in RL you're generating answers and trying the model in real-world environments, it ends up being much more memory-bound. This is because you're generating long sequences, and attention mechanisms exhibit a quadratic increase in memory as sequences grow longer. So the compute becomes very different. When we talk about a model in pre-training, I think back to the Biden administration executive order, which cited something like 10^25 FLOPs to train a model. If you're using FLOPs in post-training, it's much more complex because the reality is about how many hours and how many GPUs you are allocating. And I think in terms of time, the RL compute is approaching similar timeframes because you simply can't put it all into one system. Pre-training is so computationally dense, with all GPUs communicating and operating extremely efficiently, whereas RL has many moving parts and can take a long time to generate a sequence of a hundred thousand tokens. If you think about GPT-4o taking an hour, consider: what if your training run has to sample for an hour, and you need to ensure that's handled efficiently? So I think in GPU hours or just wall clock hours, the RL runs are probably approaching the same number of days as pre-training, but they probably aren't using as many GPUs at the same time. There are rules of thumb in labs that pre-training runs shouldn't last more than a month because they fail catastrophically. And if you are planning a huge cluster for two months and then it fails on day 50, the opportunity costs are just so big. So people don't want to put all their eggs in one basket. GPT-4 was the ultimate &quot;YOLO&quot; run; nobody had wanted to attempt anything like it before, as it took three months to train, and everyone was shocked that it worked. I think people are a little bit more cautious and incremental now.

**Speaker 2:** So RLVR is, let's say, more unlimited in how much you can train and derive benefit, whereas with RLHF, because it's preference tuning, you reach a certain point where it doesn't make sense to spend more RL budget. Just taking a step back with preference tuning: multiple people can offer multiple explanations for the same thing, and both can be correct. But at some point, you learn a certain style, and it no longer makes sense to iterate on it. My favorite example is if relatives ask me what laptop they should buy, I give them an explanation or ask them, &quot;What is your use case?&quot; They, for example, might prioritize battery life and storage. Others, like us, would prioritize RAM and compute. Both answers are correct, but different people require different answers. And with preference tuning, you're trying to average preferences somehow, by asking data labelers to give you the preferred answer and then you train on that. But at some point, you learn that average preferred answer. And there's no reason to keep training longer on it because it's just a style, whereas with RLVR, you literally let the model solve increasingly complex and difficult problems. And so I think it makes more sense to allocate more budget long-term to RLVR. Right now, we are in &quot;RLVR 1.0&quot; territory, where it's still a simple setup where we have a question and answer but don't do anything with the intermediate steps. There were multiple research papers, by Google for example, on process reward models that also assign scores for the explanation—how correct the explanation is. And I think that will be the next step, let's say &quot;RLVR 2.0&quot; for this year: focusing on the stages between question and answer, and how to leverage that information to improve the explanation and thus improve accuracy. So that's one angle. And there was a DeepSeek Math-V2 paper where they also had interesting inference scaling there, where they had first developed separate models that grade themselves. And I think that will be one aspect. And the other, as Nathan mentioned, will involve RLVR branching into other domains.

**Speaker 1:** The place where people are excited is value functions, which is pretty similar. Process reward models assign a &quot;goodness&quot; score to each intermediate step in a reasoning process, while value functions apply value to every token the language model generates. Both of these have been largely unproven in the language modeling and reasoning model era. People are more optimistic about value functions for whatever reason now. I think process reward models were tried much more in this pre-o1, pre-reasoning model era, and many people experienced significant challenges with them. So I think a lot of it stems from human nature; for example, value models have a very deep history in reinforcement learning. Training value models was one of the core elements that enabled deep reinforcement learning to exist. So right now in the literature, people are excited about trying value models, but there's very little empirical proof for it. And there are negative examples in trying to scale up process reward models. These findings don't always hold true in the future. We came to this discussion by talking about scaling. And the simple way to summarize what you're saying is you don't want to do too much RLHF, essentially because the signal scales. People have worked on RLHF for language models for years, especially with intense interest after ChatGPT. And the first release of a reasoning model trained with RLVR, OpenAI's o1, featured a scaling plot showing that if you increase the training compute logarithmically, you get a linear increase in evaluations, and this has been reproduced multiple times. I think DeepSeek had a plot like this. But there's no scaling law for RLHF where if you logarithmically increase the compute, you get some performance. In fact, the seminal scaling paper for RLHF is *Scaling Laws for Reward Model Overoptimization*. So that's a significant distinction: RLVR and its current and future methods will follow this scaling paradigm, meaning you can let the best runs continue for an extra 10x and achieve a few x performance gain, but you can't do this with RLHF. And that is just going to be field-defining in how people approach them. While I advocate for academics to pursue RLHF, a good way to describe it is this: to achieve the best RLHF, you might not need an extra 10 or 100x of compute, but to achieve the best RLVR, you do. So I think there's a seminal paper from a Meta internship. It's called something like *The Art of Scaling Reinforcement Learning with Language Models*. The framework they describe is ScaleRL. And their incremental experiment involved something like 10,000 V100 hours, equating to thousands or tens of thousands of dollars per experiment, and they do a lot of them. This cost is just not accessible to the average academic. This presents a difficult equilibrium where we're trying to figure out how to learn from each community.

**Speaker 2:** I was wondering if we could take a bit of a tangent and talk about education and learning. If you're someone listening to this who is smart and interested in programming and AI, I presume building something from scratch is a good beginning. Can you take me through what you would recommend people do?

**Speaker 1:** I would personally start by implementing a simple model from scratch that you can run on your computer. The goal is not to have something you use every day for your personal projects. It's not going to be a personal assistant replacing an existing open-weight model or ChatGPT. It's to see exactly what goes into the LLM, what comes out, and how the pre-training works, preferably on your own computer. Then you learn about pre-training, supervised fine-tuning, and the attention mechanism. You get a solid understanding of how things work, but at some point you'll reach a limit because local models can only do so much. The problem with learning about LLMs at scale is that it's exponentially more complex to build a larger model, as the model doesn't simply become larger. You have to think about sharding your parameters across multiple GPUs. Even for the KV-cache, there are multiple ways to implement it. One way to understand how it works is to grow the cache step-by-step by, for instance, concatenating lists, but then that wouldn't be optimal on GPUs. You would pre-allocate a tensor and then fill it in. But that adds another 20 or 30 lines of code. For each thing, you add code. The trick with the book is to understand how the LLM works. It's not going to be a production-level LLM, but once you have that, you can understand the production-level LLM.

**Speaker 2:** So you're trying to always build an LLM that's going to fit on one GPU?

**Speaker 1:** Yes. Most of them do. I have some bonus materials on some MoE models. One or two of them may require multiple GPUs, but the goal is to have it on one GPU. And the beautiful thing is you can self-verify. It's almost like RLVR. When you code these from scratch, you can take an existing model from the Hugging Face Transformers library. That library is great, but if you want to learn about LLMs, I don't think that's the best place to start because the code is so complex; it has to fit so many use cases. Since people use it in production, it's really sophisticated and intertwined. It's not linear to read.

**Speaker 1 (cont.):** It started as a fine-tuning library and then grew to be the standard representation of every model architecture. Hugging Face is the default place to get a model, and Transformers is the software that enables it so people can easily load a model and do something basic with it.

**Speaker 2:** And all frontier labs that have open-weight models have a Transformers version of it, from DeepSeek to GPT-2. That's the canonical weight that you can load there. But even the Transformers library itself is not used in production. People use SGLang or vLLM, and it adds another layer of complexity.

**Speaker 1:** We should say that the Transformers library has around 400 models.

**Speaker 2:** So it's the one library that tries to implement a lot of LLMs, resulting in a huge codebase. It's huge; perhaps millions—

**Speaker 1:** That's crazy.

**Speaker 2:** —hundreds of thousands of lines of code. Understanding the part you want to understand is finding the needle in the haystack. But what's beautiful is you have a working implementation, so you can work backward from it. What I would recommend doing, or what I also do, is if I want to understand, for example, how Llama 3 is implemented, I would look at the weights in the model hub, the config file, and then you can see, &quot;Oh, they used so many layers.&quot; They use, for example, Group Query Attention or Multi-Head Attention in that case. Then you see all the components in a human-readable 100-line config file. And then you start, for example, with your GPT-2 model and add these things. The cool thing here is you can then load the pre-trained weights and see if they work in your model. And you want to match the same output that you would get with a Transformer model, and then you can use that as a—

**Speaker 2 (cont.):** —basically, as a verifiable reward to ensure your architecture is correct. Sometimes it takes me a day. With Llama 3, the challenge was RoPE for the position embeddings. They had a YaRN extension, and there was some custom scaling there, and I couldn't quite match these things, and through this struggle, you truly understand things. But at the end, you know you have it correct because you can unit test it. You can check against the reference implementation, and I think that's one of the best ways to truly learn. To basically reverse-engineer something.

**Speaker 1:** I think that is something that everybody interested in getting into AI today should do. I liked your book. I came to language models from this RL and robotics field. I had never taken the time to just learn all the fundamentals, and this Transformer architecture I described is so fundamental, much like deep learning was something I had to learn in the past, and people need to do this now. I think where a lot of people tend to get overwhelmed is by the question, &quot;How do I apply this to have impact or find a career path?&quot; Because AI language models make these fundamental concepts so accessible, and people with motivation will learn it, and then it's like, &quot;How do I get the cycles to contribute to research?&quot; I'm actually fairly optimistic because the field moves so fast that a lot of times the best people don't fully solve a problem because a bigger, very low-hanging fruit problem appears, so they move on. And I think that a lot of what I was trying to do in this RLHF book is take post-training techniques and describe how people think about them influencing the model, and what people are doing. It's remarkable how many things people just stop studying. So I think people getting narrow after doing the fundamentals is good, as is reading the relevant papers and being engaged in the ecosystem. The proximity that random people have online to leading researchers... No one knows who all the... for example, the anonymous accounts on X in ML are very popular for whatever reason, and no one knows who all these people are. It could just be random people who study the material deeply, especially with AI tools to help them say, &quot;I don't understand this; keep digging into it.&quot; I think that's a very useful approach. But there's a lot of research areas that just require maybe three papers that you need to read, and then one of the authors will probably email you back. But you have to put in a lot of effort into those emails to demonstrate understanding of the field. For a newcomer, I think it would easily take weeks of work to feel like they can truly grasp a very narrow area, but I think going narrow after you have the fundamentals will be very useful to people. For example, I became very interested in character training—how you make the model funny, sarcastic, or serious, and what you do to the data to achieve this. A student at Oxford reached out to me, saying, &quot;Hey, I'm interested in this,&quot; and I advised him. And that paper now exists, and there are perhaps two or three people in the world that were very interested in this. He's a PhD student, which gives you an advantage, but for me, that was a topic I was waiting for someone to say, &quot;Hey, I have time to spend cycles on this.&quot; And I'm sure there's a lot more very narrow things where you might think, &quot;Oh, it doesn't make sense that there was no answer to this.&quot; I think there's just so much information coming that people feel like, &quot;I can't grab onto any of these,&quot; but if you actually stick to one area, I think there are a lot of interesting things to learn.

**Speaker 2:** Yeah, I think you can't try to do it all because it would be very overwhelming, and you would burn out if you tried to keep up with everything. For me, I haven't kept up with computer vision in a long time, and have just focused on LLMs. But coming back to your book, I think this is also a really great book and a really good &quot;bang for the buck&quot; if you want to learn about RLHF. I wouldn't go out there and read RLHF papers because you would spend two years—

**Speaker 1:** Some of them contradict. I've just edited the book, and I was like, there's a chapter where I had to say, &quot;X papers say one thing and X papers say another thing, and we'll see what comes out to be true.&quot;

**Speaker 2:** Just to go through some of the table of contents, some of the ideas we might have missed in the broader picture of post-training. So first of all, you did the problem setup, training overview, what preferences are, preferences data, and the optimization tools, reward modeling, regularization, instruction tuning, rejection sampling, reinforcement learning. Then, Constitutional AI and AI Feedback, reasoning and inference time scaling for use in function calling, synthetic data and distillation, evaluation, and then an open questions section covering over-optimization, style and information, product UX, character, and post-training. So what are some ideas worth mentioning that connect both the educational component and the research component? You mentioned the character training, which is pretty interesting.

**Speaker 1:** Character training is interesting because there's so little research out there, but we talked about how people engage with these models and how we feel good using them because they're positive. But that can go too far; it can be too positive. And it's essentially: how do you change your data and decision-making to make it exactly what you want? OpenAI has something called a Model Spec, which is essentially their internal guideline for what they want the model to do, and they publish this to developers. So essentially, you can know what constitutes a failure of OpenAI's training—where they have intentions they haven't met yet—versus something they actually wanted to do that you don't like. And that transparency is very nice, but all the methods for curating these documents and how easy it is to follow them is not very well known. I think the way the book is designed is that the RL chapter is obviously what people want because everybody hears about it with RLVR, and it's the same algorithms and the same math, but it's just that you can use it in very different contexts. So I think the core of RLHF is how messy preferences are. It's essentially a rehash of a paper I wrote years ago, but this is the chapter that will tell you why RLHF is never fully solvable, because the way RL is set up assumes that preferences can be quantified and that multiple preferences can be reduced to single values. And I think it relates in economics literature to the Von Neumann-Morgenstern utility theorem. That is the chapter where all of that philosophical, economic, and psychological context tells you what gets compressed into doing RLHF. So you have all of this, and later in the book, it's like, you use this RL math to make the number go up. I think that's why it will be very rewarding for people to research, because quantifying preferences is a problem that humans have designed in order to make preferences studyable. But there's fundamental debates. For example, in a language model response, you have different things you care about, whether it's accuracy or style. And when collecting data, they all get compressed into &quot;I like this more than another.&quot; That is happening, and there's a lot of research in other fields regarding how you should actually do this. I think social choice theory is the subfield of economics around how you should aggregate preferences. And I went to a workshop that published a white paper titled, &quot;How can you think about using social choice theory for RLHF?&quot; So I want people who get excited about the math to stumble into and learn this broader context. I keep a fun list of all the tech reports that I like for reasoning models. In Chapter 14, which has a short summary of RLVR, there's a gigantic table where I list every single reasoning model I like. So I think in education, a lot of it needs to be, at this point, what I like—because the language models are so good at the math. For example, the famous paper Direct Preference Optimization, which offers a much simpler way of solving the problem than traditional RL. The derivations in the appendix skip steps of math. For this book, I redid the derivations and I thought, &quot;What the heck is this log trick they use to change the math?&quot; But doing it with language models, they explain, &quot;This is the log trick.&quot; I thought, &quot;I don't know if I like this, that the math is so commoditized.&quot; I think some of the struggle in reading this appendix and following the math is good for learning. And I...

**Speaker 2:** Yeah, so we're actually returning to this often, just on the topic of education. You both have brought up the word &quot;struggle&quot; quite a bit. So there is value in it. If you're not struggling as part of this process, you're not fully following the proper learning process, I suppose.

**Speaker 1:** Some of the providers are starting to work on models for education, which are designed to not give—I haven't used them, but I would guess—all the information at once—

**Speaker 2:** Right.

**Speaker 1:** —and make people work to do this. I think you could train models to do this, and it would be a wonderful contribution. Where, like all the material in the book, you had to reevaluate every decision for it—

**Speaker 1 (cont.):** Which is such a great example. I think there's a chance we'll work on it at AI2, which I thought would be so fun.

**Speaker 2:** It makes sense. I did something like that the other day for video games, for example. I sometimes play video games for my pastime—video games with puzzles. You know, like Zelda and Metroid. And there's this new game where I got stuck, and I really got stuck and thought, &quot;Okay.&quot; I don't want to struggle for two days, so I used an LLM. But then you tell it, &quot;Hey, please don't add any spoilers. Just, you know, I'm here. What do I have to do next?&quot; And the same thing you can do for math, where you can say, &quot;Okay, I'm at this point. I'm getting stuck. Don't give me the full solution, but what is something I could try?&quot; Where you kind of carefully probe it. But the problem here is that I think it requires discipline. A lot of people enjoy math, but there are also many who need to do it for their homework, and then it becomes a shortcut. And yeah, we could develop an educational LLM, but the other LLM is still there, and there's still a temptation to use the other LLMs.

**Speaker 1:** I think a lot of people, especially in college, understand the things they're passionate about—they're self-aware about it, and they understand it shouldn't be easy. I think we just have to develop good taste—talk about research taste, like &quot;school taste&quot; for things that you should be struggling on—and stuff you shouldn't be struggling on. Which is tricky to know, because sometimes you don't have good long-term vision about what would be actually useful to you in your career. But you have to develop that taste, yeah.

**Speaker 2:** I was talking to my fiancee or friends about this, and there's this brief 10-year window where all of the homework and all the exams could be digital. But before that, everybody had to do all the exams in blue books because there was no other way. And now after AI, everybody's going to need blue books and oral exams because everybody could cheat so easily. It's like this brief generation that had a different education system where everything could be digital, but you still couldn't cheat. And now it's just gonna go back. It's just very funny.

**Speaker 2:** You mention character training. Just zooming out to a more general topic, for character training, how much compute was required? And in general, to contribute as a researcher, are there areas where not too much compute is required, where you can actually contribute as an individual researcher?

**Speaker 1:** For the character training, I think this research is built on fine-tuning about seven billion parameter models using LoRA, which is essentially only fine-tuning a small subset of the weights of the model. I don't know exactly how many GPU hours that would take. But it's doable.

**Speaker 2:** Not doable for every academic.

**Speaker 1:** The situation for some academics is so dire that the only work you can do is performing inference where you use closed or open models, get completions from them, and then analyze them to understand the models. And that's very well-suited to evaluation, where you want to be the best at creating representative problems that the models fail on or that show certain abilities. I think you can break through with this approach. I think that the top-tier goal for a researcher working on evaluation, if you want to build career momentum, is for Frontier Labs to adopt your evaluation. You don't need to have every project do this. But if you go from a small university with no compute and you figure out something that Claude struggles with, and then the next Claude model mentions it in its blog post, there's your career rocket ship. I think that's hard, but if you want to scope the maximum possible impact with minimum compute, it's something like that, which involves getting very narrow and understanding where the models are headed. So you need to build a tool that tests where Claude 4.5 will fail. If I'm going to start a research project, I need to think where the models in eight months are going to be struggling.

**Speaker 2:** But what about developing totally novel ideas?

**Speaker 1:** This is a trade-off. I think that if you're doing a PhD, you could also think, &quot;It's too risky to work in language models. I'm going way longer term.&quot; This means asking, &quot;What is the thing that's going to define language model development in 10 years?&quot; I think that I end up being a person that's pretty practical. I went into my PhD thinking, &quot;Eh, I got into Berkeley. Worst case, I get a master's, and then I go work in tech.&quot; And so I'm very practical about it; so I think about the life afforded to people working at these AI companies, the amount of... OpenAI's average compensation is over a million dollars in stock a year per employee. For any normal person in the US, getting into an AI lab like this is transformative for your life. So I'm pretty practical about it; there's still a lot of upward mobility working in language models if you're focused. And the outcomes, for instance, look at these jobs. But from a research perspective, the transformative impact in these academic awards—being the next Yann LeCun, for example—comes from not working on or not caring about language model development very much.

**Speaker 2:** It's a big financial sacrifice in that case.

**Speaker 1:** So I get to work with some awesome students, and they ask, &quot;Should I go work at an AI lab?&quot; And I respond, &quot;You're getting a PhD at a top school. Are you going to leave to go to a lab?&quot; I say, &quot;I don't know.&quot; If you go work at a top lab, I don't blame you. Don't go work at some random startup that might go to zero. But if you're going to OpenAI, I think, &quot;It could be worth leaving a PhD for.&quot;

**Speaker 2:** Let's more rigorously think through this. So where would you give a recommendation for people to do a research contribution? The options are academia: get a PhD. Spend five years publishing. Compute resources are constrained. There are research labs that are more focused on open-weight models, and so working there. Or closed frontier labs, research labs. So OpenAI, Anthropic, xAI, and so on.

**Speaker 1:** There are two gradients: the more closed the environment, the more money you tend to get, but also the less credit you receive. So in terms of building a portfolio of things that you've done, it's very clear what you have done as an academic. Versus if you are going to trade this fairly reasonable progression for being a cog in the machine, which could also be very fun. So I think it's very different career paths. But the opportunity cost for being a researcher is very high because PhD students are paid essentially nothing. So I think it ends up rewarding people who have a fairly stable safety net and realize they can operate in the long term, pursuing very interesting work and aiming for interesting jobs. So it is a fairly privileged position to say, &quot;I'm going to see out my PhD and figure it out after because I want to do this.&quot; And at the same time, the academic ecosystem is getting bombarded by funding cuts and other issues. So there's just so many different trade-offs where I understand plenty of people who say, &quot;I don't enjoy it. I can't deal with this funding search. My grant got cut for no reason by the government,&quot; or, &quot;I don't know what's going to happen.&quot; So I think there's a lot of uncertainty and trade-offs that, in my opinion, favor just taking the well-paying job with meaningful impact. It's not like you're getting paid to sit around at OpenAI. You're building the cutting edge of things that are changing millions of people's relationship to tech.

**Speaker 2:** But publication-wise, they're being more secretive, increasingly so. So you're publishing less and less. And so you are having a positive impact at scale, but you're a cog in the machine.

**Speaker 1:** I think, honestly, it hasn't changed that much. So I have been in academia; I'm not in academia anymore. At the same time, I wouldn't want to miss my time in academia. But what I wanted to say before I get to that part, I think it hasn't changed that much. I was working in, for example, using AI or machine learning methods for applications in computational biology with collaborators, and many people went directly from academia to Google. And I think it's the same thing. Back then, professors were sad that their students went into industry because they couldn't carry on their legacy in that sense. And I think it's the same thing. It hasn't changed, I think, that much. The only thing that has changed is the scale. But cool stuff was always developed in industry that was closed. You couldn't talk about it. And I think the difference now is your preference. Do you like to talk about your work and publish, or do you prefer to be in a closed lab? That's one difference, the compensation, of course. But it's always been like that, I think. So it really depends on where you feel comfortable. And also, nothing is forever. The only thing right now is there's a third option, which is starting a startup. A lot of people are starting startups. Very risky move. But it is a high-risk, high-reward type of situation, whereas joining an industry lab, I think, is relatively safe. Also upward mobility. Honestly, I think once you have been at an industry lab, it will be easier to find future jobs. But then again, it's about how much you enjoy the team and working on proprietary things versus how you like the publishing work. I mean, publishing is stressful. It is. The acceptance rate at conferences can be arbitrary and very frustrating, but there's also a high reward: if you have a paper published, you feel good because your name is on it. You have a high accomplishment, and so on.

**Speaker 1:** I feel like my friends who are professors seem on average happier than my friends who work at a frontier lab. To be totally honest. Because that's just grounding, and—the frontier labs definitely do this 9/9/6—which essentially is shorthand for work all the time.

**Speaker 2:** Can you describe 9/9/6 as a culture that, I believe, was invented in China and adopted in Silicon Valley? What's 9/9/6? It's 9:00 AM to 9:00 PM.

**Speaker 1:** Six days a week.

**Speaker 2:** Six days a week. What is that, 72 hours? Okay. So, is this basically the standard in AI companies in Silicon Valley? More and more, this kind of grind mindset.

**Speaker 1:** Yeah, maybe not exactly like that, but I think there is a trend toward it. And it's interesting. I think it almost flipped. Because when I was in academia, I felt that way because, as a professor, you had to write grants. You had to teach and you had to do your research. It's like three jobs in one, and it is more than a full-time job if you want to be successful. And I feel that now, as Nathan just said, professors, in comparison to a lab, I think have less pressure or workload than at a frontier lab because—

**Speaker 1 (cont.):** I think they work a lot. They're just so fulfilled. By working with students, having a constant runway of mentorship, and a very people-oriented mission. In an era when things are moving very fast and are very chaotic, it's very rewarding to people.

**Speaker 2:** Yeah, and I think at a startup, it's this pressure. It's like you have to make it. It's really important that people put in the time, but it's really hard because you have to deliver constantly. I've been at a startup. I had a good time, but I don't know if I could do it forever. It's an interesting pace, and it's exactly like we talked about in the beginning. These models are leapfrogging each other, and they are constantly trying to take the next step compared to their competitors. It's just ruthless, I think, right now.

**Speaker 1:** I think this leapfrogging nature and having multiple players is actually an underrated driver of language modeling progress, where competition is deeply ingrained in people, and these companies have intentionally created very strong culture. Anthropic, for example, is known to be so culturally committed and organized. We hear so little from them, and everybody at Anthropic seems very aligned. And it's like being in a culture that is super tight—and having this competitive dynamic is something that will make you work hard and create things that are better. So that comes at the cost of human capital, which means you can only do this for so long, and people are definitely burning out. I think I wrote a post on burnout as I've treaded in and out of this myself, especially when trying to be a manager and in full-mode training. It's a crazy job. The book *After Steve* by Patrick McGee talks about how hard the Apple engineers worked to set up supply chains in China, and he said they had &quot;saving marriage&quot; programs, and he mentioned in a podcast, &quot;People died from this level of working hard.&quot; So I think it's a perfect environment for creating progress based on human expense, and the human expense is the &quot;996&quot; that we started this with, where people really do grind.

**Speaker 2:** I also read this book. I think they had a code word for when someone had to go home to spend time with their family to &quot;save the marriage.&quot; It's crazy. Then colleagues would say, &quot;Okay, this is a red alert for this situation. We have to let that person go home this weekend.&quot; But at the same time, I don't think they were forced to work. They were so passionate about the product that they got into that mindset. And I had that sometimes as an academic, but also as an independent person, I experience that sometimes. I overwork, and it's unhealthy. I had back issues, I had neck issues, because I did not take the breaks that I maybe should have taken. But no one forced me to; it's because I wanted to work, because it's exciting stuff.

**Speaker 1:** That's what OpenAI and Anthropic are like now. They want to do this work.

**Speaker 2:** Yeah, but there's also a feeling of fervor that's building, especially in Silicon Valley, aligned with the scaling laws idea—this hype that the world will be transformed in a matter of weeks, and you want to be at the center of it. And I have this great fortune of having conversations with a wide variety of human beings, and from there I get to see all these bubbles and echo chambers across the world. And it's fascinating to see how we humans form them. And I think it's fair to say that Silicon Valley is a kind of echo chamber, a silo and a bubble. I think bubbles are actually really useful and effective. It's not necessarily a negative thing because you can be ultra-productive. It could be the Steve Jobs reality distortion field, because you just convince each other that breakthroughs are imminent, and by convincing each other of that, you make the breakthroughs imminent.

**Speaker 1:** Byrne Hobart wrote a book classifying bubbles. Essentially, one type is financial bubbles, involving speculation, which is problematic. The other type, for which I don't know the specific term, is effectively for &quot;build-outs,&quot; because it pushes people to build these things. And I do think AI is in this, but I worry about it transitioning to a financial bubble, which is...

**Speaker 2:** Yeah, but also in the space of ideas, that bubble means you are creating a reality distortion field, deviating from reality. If you go too far from reality while also working, say, &quot;996&quot; hours, you might miss some fundamental aspects of the human experience, including outside Silicon Valley. This is a common problem in Silicon Valley: it's a very specific geographic area. You might not understand the Midwest perspective, the full experience of all the other humans in the United States and across the world, and you speak a certain way to each other, convincing each other of certain things, which can get you into real trouble. Whether AI is a big success and becomes a powerful technology or it's not, in either trajectory you can get yourself into trouble. So you have to consider all of that. Here you are, a young person trying to decide what you want to do with your life.

**Speaker 1:** The thing that is...

I don't even really understand this, but the SF AI memes have gotten to the point where &quot;permanent underclass&quot; was one of them, which was the idea that the last six months of 2025 was the only time to build durable value in an AI startup or model. Otherwise, all the value will be captured by existing companies, and you will therefore be poor. That's an example of the SF thing that goes so far. I still think young people will be able to tap into it. If you're really passionate about wanting to have an impact in AI, being physically in SF is the most likely place where you'll do this. But it has trade-offs.

- I think SF is an incredible place, but there is a bit of a bubble. And if you enter that bubble—which is extremely valuable—make sure to also get out. Read history books, read literature, visit other places in the world. Twitter and Substack are not the entire world.

- One of the people I worked with is moving to SF, and I need to get him a copy of *Season of the Witch*, which is a history of SF from 1960 to 1985. It goes through the hippie revolution, the gay community taking over the city and that culture emerging, and then the HIV/AIDS crisis and other things. And it's just so recent, and there's so much turmoil and hurt, but also love in SF. No one knows about this. It's a great book, *Season of the Witch*. I recommend it. Many of my SF friends who get out recommended it to me. And I think that's just how it is living there. I lived there and I didn't appreciate this context, and it's just so recent.

- Yeah. Okay, we've talked a lot about many things. Certainly about the exciting things from last year. But this year, one of the exciting things you both mentioned is the scaling of text diffusion models—a different exploration of text diffusion. Can you talk about what that is and the possibility it holds? So, are these different kinds of approaches than the current LLMs?

- Yeah, so we talked a lot about the transformer architecture and the autoregressive transformer architecture, specifically, like with GPT. That doesn't mean others aren't exploring different approaches. People are always on the lookout for the next big thing, because I think it would be almost foolish not to. While the transformer architecture is currently dominant and works best, and there's nothing else out there yet, it's always a good idea not to put all your eggs into one basket. So, people are developing other alternatives to the autoregressive transformer. One example is text diffusion models. Listeners may know diffusion models from image generation, as Stable Diffusion popularized them. There was a paper on generating images. Previously, people used GANs (Generative Adversarial Networks). Then came this diffusion process, where you iteratively denoise an image, resulting in really good quality images over time. Stable Diffusion was a company; other companies also built their own diffusion models. And now people are asking, &quot;Okay, can we try this for text as well?&quot; It doesn't make intuitive sense yet because it doesn't feel like something continuous, like a pixel, that we can differentiate. Text is discrete, so how do we implement that denoising process? It's somewhat similar to Google's BERT models. Going back to the original transformer, there was an encoder and a decoder. The decoder is what we're currently using in GPT and similar models. The encoder is more of a parallel technique, where you fill in multiple tokens in parallel. GPT models perform autoregressive generation, completing a sentence one token at a time. In BERT models, you have a sentence with gaps that are masked out, and one iteration fills in these gaps. Text diffusion is similar: you start with some random text, then iteratively fill in missing parts or refine them over multiple iterations. The key advantage here is that this can process multiple tokens simultaneously. Thus, it holds the promise of greater efficiency. The trade-off, of course, is the quality. It might be faster, but it introduces the dimension of the denoising process. The more steps you perform, the better the text quality becomes. This allows for different scaling approaches. Researchers are exploring if it's a valid alternative to the autoregressive model, potentially offering the same quality for less compute. Currently, papers suggest that achieving the same quality requires increasing the denoising steps, which then consumes the same amount of compute as an autoregressive model. Another downside is that while parallelism is appealing, some tasks, like reasoning or tool use, are not inherently parallel. For example, you might need to ask a code interpreter for an intermediate result. This is tricky with diffusion models. Consequently, some hybrid models exist. But the main idea is how to parallelize it. It's an interesting avenue. Currently, most models in this space are research models, like LaMDA and some others. I've seen some from startups, but there's no diffusion model yet at the scale of Gemini or ChatGPT. However, Google announced they are launching Gemini Diffusion, contextualizing it within their Nano 2 model. They stated that for the same quality on most benchmarks, they can generate content much faster. Regarding &quot;what's next&quot;—I don't think text diffusion will replace autoregressive LLMs, but it will be useful for quick, cheap, and at-scale tasks. Maybe the free tier in the future will be something like that.

- To illustrate why this is so much better: for example, if GPT-5 takes 30 minutes to respond, it's generating one token at a time. The diffusion idea is to essentially generate all those tokens in the completion in one batch, which is why it could be significantly faster. I think it could be well-suited for startups working with code, where you have a codebase and someone who's effectively &quot;vibe coding&quot; and says, &quot;Make this change.&quot; A code diff is essentially a huge reply from the model, but it doesn't require extensive external context, and you can generate it really fast using these diffusion models. One example I've heard of is using text diffusion to generate very long diffs. Doing so with an autoregressive model would take minutes, and such latency for a user-facing product causes significant churn; every second, you lose a lot of users. So I think it's going to grow and have some applications, but I actually thought different types of models would be used for different things sooner than they have been. I think the issue of tool use is what's preventing them from being more general-purpose. Because in Claude and ChatGPT, the autoregressive chain is interrupted by an external tool, and I don't know how to achieve that with a diffusion setup.

- So, what's the future of tool use this year and in the coming years? Do you think there will be a lot of developments there, and how will that be integrated into the entire stack?

- I do think right now it's mostly on the proprietary LLM side, but I believe we will see more of it in open-source tooling. And I mean, it is a huge unlock because then you can truly outsource certain tasks from mere memorization to actual computation. Instead of having the LLM memorize what is 23 plus five, just use a calculator.

- So do you think that can help solve hallucination?

- Not solve it, but reduce it. Still, the LLM needs to know when to make a tool call. And second, the internet isn't always correct. You can do a web search, but if I ask who won the World Cup in, say, 1998, it still needs to find the right website and retrieve the correct information. So, you can still visit an incorrect website and obtain inaccurate information. So I don't think it will fully solve hallucination, but it is improving it in that sense. Another cool paper earlier this year, I think from late December—so not technically 2024, but close—was about the recursive language model. That's an interesting idea to take this even further. Just to explain (as Nathan also mentioned earlier), it's harder to do cool research in academia due to compute budget limitations. If I recall correctly, they did everything with GPT-4, so they didn't even use local models. The idea is, let's say you have a long-context task. Instead of having the LLM solve all of it in one shot, or even in a chain, you break it down into sub-tasks. You have the LLM decide what constitutes a good sub-task, and then recursively call an LLM to solve it. And I think something like that—then adding tools, and for each one, maybe you have a huge Q&amp;A task where each goes to the web and gathers information, and then you pull it together at the end and stitch it back together. I think there will be many unlocks using such approaches, where you don't necessarily improve the LLM itself, but rather how the LLM is used and what it can utilize. One downside with tool use right now is that you have to give the LLM permission to use tools. That will require significant trust, especially if you want to enable features like an LLM answering emails for you, or not even answering, but just sorting or selecting them. I don't know if I would give an LLM access to my emails today, right? I mean, this is a huge risk.

- I think there's one last point regarding tool use. You hinted at this, and we've both approached it in our own ways: open versus closed models use tools in very different ways. With open models, people go to Hugging Face and download the model, then the person will ask, &quot;What tool do I want?&quot; Exa is my preferred search provider, but someone else might prefer a different search startup. When you release a model, it needs to be useful for multiple tools and use cases, which is very hard because you're creating a general reasoning engine model—which is actually what GPT-4 excels at. With closed models, however, you're deeply integrating a specific tool into your experience. I think open models will struggle to replicate some of the things I like to do with closed models, such as referencing a mix of public and private information. Something I keep trying every three to six months is using tools like Cursor on the web, which involves prompting a model to make an update to a GitHub repository I have. That set of secure cloud environments is just so nice for simply saying, &quot;send it off and do this thing, then come back to me.&quot; These will probably help define some of the local open and closed niches. However, I think initially, because there was such a rush to get tool use working, open models were on the back foot, which is somewhat inevitable. There's so much research and so many resources in these frontier labs, but it will be interesting when open models solve this. It's going to necessitate a more flexible and potentially interesting model that might work with this recursive idea to be an orchestrator and a tool-use model, so hopefully, this necessity drives some interesting innovation.

- So, continual learning—this is a longstanding and important problem. I think its importance increases as the cost of training models goes up. Can you explain what continual learning is and how important it might be this year and in the coming years for making progress?

- This relates a lot to the SF zeitgeist surrounding what AGI (Artificial General Intelligence) is, what ASI (Artificial Superintelligence) is, and what today's language models are capable of doing. I think language models can solve many tasks, but a key milestone among the AI community is when AI could essentially replace any remote worker, taking in information and solving digital tasks. The limitation highlighted by people is that a language model will not learn from feedback the same way an employee does. So if you hire an editor, they might make mistakes, but you'll tell them. And if you hired a good editor, they won't make the same mistake again. But language models don't have this ability to modify themselves and learn quickly. So the idea is, if we are to truly achieve a general, adaptable intelligence that can perform in any remote work scenario, it needs to be able to learn quickly from feedback and on-the-job experience. I'm personally more bullish on language models' ability to simply be provided with very good context. You mentioned, perhaps offline, that you can write extensive documents for models where you state, &quot;I have all this information. Here are all the blog posts I've ever written. I like this type of writing. My voice is based on this.&quot; But many people don't provide this to models, and the models weren't designed to handle this amount of context previously. Agentic models are just beginning to emerge. So it's this trade-off: do we need to update the weights of these models with continual learning to make them learn fast? Or, the counterargument is that we just need to provide them with more context and information, and they will *appear* to learn fast simply by having a lot of context and being very smart.

- So, we should mention the terminology here. Continual learning refers to continuously changing the weights so that the model adapts and adjusts based on new incoming information, doing so continually, rapidly, and frequently. The other side of this, which you mentioned, is generally referred to as in-context learning. As you learn, there's a huge context window. You can just keep loading it with extra information every time you prompt the system. I think both can legitimately be seen as learning. It's just a different place where you're doing the learning.

- To be honest, we already have continual learning—the updating of weights—in different flavors. I mean, if you think about it, the distinction here is whether you do that on a personalized custom model for each person, or on a global model scale. And I think we already have that with the progression from GPT-5 to 5.1 and 5.2. It's maybe not immediate, but it's a curated update—a quick, curated update where there was feedback on things they couldn't do, feedback from the community. They updated the weights for the next model, and so forth. So it is, I mean, a flavor of that. A finer-grained example is RLVR; you run it, and it updates. The problem is you can't just do that for each person because it would be too expensive to update the weights for every individual. I think that's the core issue. Unless you get... I mean, even at OpenAI's scale, building the data centers would be too expensive. I think that's only feasible once you have something on the device where the cost falls on the consumer, like what Apple tried to do with their Apple Foundation models, putting them on the phone, and then they learn from the experience.

- A bit of a related topic, but this somewhat anthropomorphized term: &quot;memory.&quot; What are different ideas for the mechanism of how to add memory to these systems, especially personalized memory?

- Right now, it's mostly about context—basically stuffing things into the context and then just recalling that. But again, I think it's expensive because you have to—I mean, you can cache it, but you still spend tokens on that. And second, you can only do so much. I think it's more about preference or style. Many people do that when they solve math problems. You can add previous knowledge and other information, but you also give it certain preference prompts: &quot;do what I preferred last time,&quot; or something similar. But it doesn't unlock new capabilities. For that, one thing people still use is LoRA adapters. These are basically—instead of updating the whole weight matrix, there are two smaller weight matrices that you have in parallel or as overlays, like the delta. But yes, you can do that to some extent, but then again, it's about economics. There were also papers, for example, showing that LoRA learns less but forgets less. It's like, you know, there's no free lunch. If you want to learn more, you need more weights, but that gets more expensive. And then again, if you learn more, you forget more; you have to find that Goldilocks zone, essentially.

- We haven't really mentioned it much, but context length is also implied in this discussion. Is a lot of innovation possible there?

- I think the colloquially accepted understanding is that it's a compute and data problem, where you can... and sometimes, small architectural changes, like attention variants. We talked about hybrid attention models, which are essentially what you get if you have what looks like a state space model within your transformer. And those are better suited because you spend less compute to model the furthest along token. I think that—but those aren't free because they have to be accompanied by a lot of compute or the right data. So how many sequences of 100,000 tokens exist in the world, and where do you get these? It just ends up being quite expensive to scale them. We've quickly gotten to a million tokens of input context length. I would expect it to keep increasing and reach 2 million or 5 million this year, but I don't expect it to go to 100 million. That would be a true breakthrough, and I think such breakthroughs are possible. The continual learning aspect is a research problem where there could be a breakthrough that just makes transformers work much better at this, and cheaply. These things could happen with so much scientific attention. But by continually iterating, it'll be consistent increases over time.

- I think also, looking at the extremes, there's again no free lunch. On one extreme, to make it cheap, you have an RNN that has a single state where you save everything from the previous input. It's like a specific fixed-size mechanism, so you never truly grow the memory. You are stuffing everything into one state, but the longer the context gets, the more information you forget because you can't compress everything into a single state. Then, on the other hand, you have transformers, which try to remember every token—which is great sometimes if you want to look up specific information, but very expensive because the KV cache and the dot product grow. Then, yes, as you said, the Mamba layers kind of have the same problem. Like an RNN, you try to compress everything into one state; you're a bit more selective. But then I think it's like this Goldilocks zone again. With Nemotron-3, they found a good ratio of how many attention layers you need for the global information (where everything is accessible) compared to having these compressed states. And I think that's how we will scale more—by finding better ratios in that Goldilocks zone, striking a balance between making computing cheap enough to run and powerful enough to be useful. And one more plug here: the recursive language model paper is one of the papers that tries to address the long-context problem. What they found is that instead of stuffing everything into this long context, if you break it up into multiple smaller tasks—thus saving memory by having multiple smaller cores—you can actually achieve better accuracy than having the LLM try everything all at once. It's a new paradigm. We will see; there might be other variations of that. So I think with that, we will still make improvements on long context. But then also, as Nathan said, I think the problem for pre-training itself is that we don't have as many long-context documents as other documents. So it's harder to study, essentially, how LLMs behave at that level.

- There are some rules of thumb where you pre-train a language model, like OLMo. We pre-trained at about 8K context length and then extended to 32K with training. There are some rules of thumb where essentially, doubling the training context length takes about 2X compute, and then you can normally 2 to 4X the context length again. So I think a lot of it ends up being somewhat compute-bound at pre-training, which is in this... Everyone talks about this big increase in compute for the top labs this year, and that should reflect in longer context windows. But on the post-training side, there are some more interesting developments: as we have agents, they're going to manage this context on their own. People who use Claude a lot now dread the compaction, which is when Claude takes its entire full 100,000 tokens of work and compacts it into a bulleted list. But what the next models will do—I'm sure people are already working on this—is essentially allow the model to control when and how it compacts. So you can essentially train your RL algorithm where compaction is an action, shortening the history, and then the problem formulation will be, &quot;I want to keep the maximum evaluation scores I've gotten while the model compacts its history to the minimum length.&quot; Because then you have the minimum amount of tokens needed to do this kind of compounding autoregressive prediction. There are actually quite nice problem setups for this, where these agentic models learn to use their context in a different way than just plowing forward.

- One interesting recent example is DeepSeek-V3, where they had a sparse attention mechanism and essentially a very efficient, small, lightweight indexer. And instead of attending to all tokens, it selects: &quot;Okay, what tokens do I actually need?&quot; It almost harks back to the original idea of attention, where you are selective, but attention is always &quot;on&quot;—you might have zero weight on some tokens, but you still process them all. But they are even more like, &quot;Let's just mask that out, or not even perform that operation.&quot; With sliding window attention in OLMo, that's also kind of the same idea. You have a rolling window that you keep fixed, because you don't need everything all the time. Occasionally, some layers might, but it's wasteful. But right now, I think if you use everything, you're on the safe side. It gives you the best bang for your buck because you never miss information. And right now, I think this year will be more about figuring out, as you said, how to be smarter about that. I think right now people want the next state-of-the-art, and the state-of-the-art happens to be the brute-force, expensive approach. And then once you have that, as you said, the goal is to keep that accuracy, but see how we can do it cheaper using tricks.

- Yeah. All this scaling. The reason we get the Claude 3.5 Sonnet model first is because you can train it faster, and you're not hitting these compute walls as soon. They can just try a lot more things and get the model faster, even though the bigger model might actually be better.

- I think we should mention that there's a lot of exciting stuff going on in the AI space. My mind has recently been really focused on robotics, which we almost entirely haven't talked about. There's a lot of work on image generation and video generation. I think it's fair to say that the most exciting research, in terms of its quantity, intensity, and fervor, is in the LLM space. That's why I think it's justified for us to really focus on the LLMs we're discussing. But it would be nice to bring in certain things that might be useful. For example, there's growing excitement around world models. Do you think world models will have any use in the LLM space this coming year? Yes, I do think so. Also, what's interesting with LLMs is that if we unlock more LLM capabilities, it automatically unlocks all other fields because it accelerates progress. Many researchers and engineers use LLMs for coding. So even if they work on robotics, optimizing these LLMs that help with coding pays off. But yes, world models are interesting. It's basically where the model runs a simulation of the world—a little &quot;toy&quot; version of the real thing—which can unlock capabilities regarding data the LLM isn't aware of. It can simulate things. And I think LLMs just happen to work well by pre-training and then performing next-token prediction. But we could do this even more sophisticatedly. So what I'm saying is... there's a paper, I think by Meta, called &quot;World Models.&quot; They basically apply the concept of world models to LLMs again, where instead of just using next-token prediction and verifiable rewards to check answer correctness, they also ensure the intermediate variables are correct. It's kind of like the model is essentially learning a code environment. And I think this makes a lot of sense. It's just expensive to do, but it is making things more sophisticated, modeling the whole process, not just the result. And so it can add more value. I remember when I was a grad student, there was a competition called CASP, I think, where they performed protein structure prediction. They predict the structure of a protein that hasn't been solved yet at that point. So in a sense, this is actually great, and I think we need something like that for LLMs too, where you run the benchmark, but you hand in the results and no one knows the solution. And then after the fact, someone reveals it. But AlphaFold, when it came out, crushed this benchmark. I mean, there were also multiple iterations, but I remember the first one. I'm not an expert in that subject, but the first one explicitly modeled the physical interactions and physics of the molecule. Also the angles, even impossible angles. Then, in the next version, they got rid of this and just used brute-force scaling. I think with LLMs, we are currently in this brute-force scaling phase because it just happens to work. But I think at some point, it might make sense to bring back this modeling. And I think with world models, that's where it might actually be quite cool. And of course, for robotics, that's completely related to world models.

- Yeah. And robotics is very explicit. There's the problem of locomotion or manipulation. Locomotion is much more solved, especially in the learning domain. But there's a lot of value, just like with the initial protein-folding systems, in bringing in traditional model-based methods. So it's unlikely that you can just learn the manipulation or the whole-body manipulation problem end-to-end. That's the dream. But then you realize, when you look at the magic of the human hand and the complexity of the real world, it's really hard to learn this all the way through—the way I guess AlphaFold 2 didn't.

- I'm excited about the robotic learning space; I think it's collectively getting supercharged by all the excitement and investment in language models generally. The infrastructure for training Transformers, which is a general modeling approach, is becoming world-class industrial tooling. Wherever there was a limitation for robotics, it's just much better now. There's much more compute. They take these language models and use them as central units where you can do interesting explorative work around something that already kind of works. And then I see it emerging, kind of like we talked about with Hugging Face transformers and Hugging Face. I think when I was at Hugging Face, I was trying to make this happen, but it was too early. It's like having these open robotic models on Hugging Face, and people being able to contribute data and fine-tune them. I think we're much closer now that the investment in robotics and self-driving cars is related and enables this. Once you get to the point where you can have this sort of ecosystem, someone can download a robotics model, maybe fine-tune it to their robot, or share datasets across the world. And there's some work in this area, like RTX, I think from a few years ago, where people are starting to do that. But I think once they have this ecosystem, it will look very different. And then this whole post-ChatGPT boom is putting more resources into that, which I think is a very good area for research.

- This is also resulting in much better, more accurate, and more realistic simulators being built, closing the sim-to-real gap in the robotics space. But, you know, you mentioned a lot of excitement and investment in the robotics space. The downside of that, which happens in hype cycles, is that I personally believe—and most robotics people believe—that robotics is not going to be solved at the timescale implicitly or explicitly promised. So what happens when all these robotics companies spring up and then don't have a product that works? There's going to be this kind of crash of excitement, which is nerve-wracking. Hopefully something else will come in and keep swooping in so that the continued development of some of these ideas keeps progressing.

- I think it's also related to the continual learning issue, essentially. The real world is so complex, whereas with LLMs, you don't really need to have something learn for the user because there are many things everyone has to do. Everyone maybe wants to, I don't know, fix their grammar in emails or code, or something like that. It's more constrained, so you can prepare the model for that. But preparing a robot for the real world is harder. I mean, you have robotic foundation models, but you can learn certain things like grasping objects. But then again, I think everyone's house is different, you know? It's so different, and that is, I think, where the robot would have to learn on the job, essentially. And I think that, I guess, is the bottleneck right now—how to customize it on the fly, essentially.

- I don't think I can possibly understate the importance of what doesn't get talked about almost at all by robotics folks or anyone: safety. All the interesting complexities we talk about—learning, failure modes, and failure cases. Everything we've been talking about with LLMs failing in interesting ways sometimes... All of that is fun and games in the LLM space. In the robotics space, in people's homes, across millions of minutes and billions of interactions, you're almost allowed to *never* fail. When you have embodied systems deployed in the real world, you just have to solve so many problems you never thought you'd have to solve when simply thinking about the general robot learning problem.

- I'm so bearish on in-home learned robots for consumer purchase. I'm very bullish on self-driving cars, and I'm very bullish on robotic automation—for example, Amazon distribution, where Amazon has built whole new distribution centers designed for robots first, rather than humans. There's a lot of excitement in AI circles about AI enabling automation and mass-scale manufacturing. I do think the path to robots doing that is more reasonable: where it's a thing designed and optimized to do a repetitive task that a human could conceivably do but doesn't want to. But it's also going to take a lot longer than people probably predict. I think the leap from AI singularity to scaling up mass manufacturing in the US, because we have a massive AI advantage, is one that's troubled by many political and other challenging problems.

- Let's talk about timelines, specifically timelines to AGI or ASI. Is it fair, as a starting point, to say that nobody really agrees on the definitions of AGI and ASI?

- I think there's a lot of disagreement, but I've been getting pushback where people say the same thing, which is like a system that could reproduce most digital economic work. The remote worker is a fairly reasonable example. And I think OpenAI's definition is somewhat related to that: an AI that can do many economically valuable tasks. I don't really love that as a definition, but I think it could be a grounding point, because language models today, while immensely powerful, are not this remote worker &quot;drop-in.&quot; And there are things that could be done by an AI that are much harder than remote work, such as finding an unexpected scientific discovery that you couldn't even posit—which would be an example of Artificial Superintelligence. Or taking in all medical records and finding linkages across certain illnesses that people didn't know, or figuring out that some common drug can treat some niche cancer. They would say that is a superintelligence task. So these are natural tiers. My problem with it is that it becomes entwined with the quest for meaning and these religious aspects. So there are different paths you can take.

- And I don't even know if the &quot;remote worker&quot; is a good definition, because what exactly is that? I actually... I don't know if you like the originally titled SITUATIONAL AWARENESS report. They focus more on code and research taste, so the target there is the &quot;superhuman coder.&quot; So they have several milestone systems: superhuman coders, superhuman AI researchers, then superintelligent AI researchers, and then full ASI (Artificial Superintelligence). But after you develop the superhuman coder, everything else follows quickly. There, the task is to have fully autonomous, automated coding. So any kind of coding you need to do to perform research is fully automated. From there, humans would be doing AI research together with that system, and they will quickly be able to develop a system that can actually do the research for you. That's the idea. And initially, their prediction was 2027 or '28, but now they've pushed it back by three to four years to 2031, as a mean prediction. My prediction is probably even beyond 2031, but at least you can, in a concrete way, think about how difficult it is to fully automate programming.

- Yeah, I disagree with some of their presumptions and dynamics on how it would play out, but I think they did good work in defining concrete scenario milestones and telling a useful story. That's why the reach for this SITUATIONAL AWARENESS document transcended Silicon Valley. It's because they told a good story and performed a lot of rigorous work to do this. I think the camp I fall into is that AI is &quot;jagged,&quot; meaning it will be excellent at some things and really bad at others. I think that when they're close to this automated software engineer, it will be good at traditional ML systems and front-end development, which the model excels at. But for distributed ML, the models are actually quite bad because there's so little training data on large-scale distributed learning. This is something we already see, and I think this will just get amplified. It's kind of messier in these trade-offs, and then there's how you think AI research works, and so on.

- So you think a &quot;superhuman coder&quot; is almost unachievable, meaning because of the jagged nature of the thing, you're always going to have gaps in capabilities?

- I think it's assigning completeness to something. The models are kind of superhuman at some types of code, and I think that will continue. And people are creative, so they'll utilize these incredible abilities to fill in the models' weaknesses and move really fast. There will always be this dance for a long time between humans enabling what the model cannot do, and the best AI researchers are the ones who can enable this superpower. And I think those lines, from what we already see—like code for building a website, where you can stand up a beautiful website in a few hours, or perform data analysis—the whole thing is going to keep getting better at these things, and we'll pick up some new code skills along the way. And kind of linking to what's happening in big tech, this Situational Awareness report leans into the singularity idea, where I think research is messy, social, and largely in the data in ways that AI models can't process. But what we do have today is really powerful, and these tech companies are collectively buying into this with billions of dollars in investment. So we are going to get a much better version of ChatGPT and a much better version of Cursor than we already have. It's just hard to predict where that's going, but the bright clarity of that future is why some of the most powerful people in the world are putting so much money into this. There are just small differences. We don't actually know what a &quot;better&quot; version of ChatGPT is, but also, can it automate AI research? I would say probably not, at least in this timeframe. Big tech is going to spend $100 billion much faster than we get an automated researcher that enables a research singularity.

- So, what would your prediction be? Is this even a useful milestone, or is it more than 10 years out?

- I would say less than that on the software side, but longer for things like research.

- Let's just for fun try to imagine a world where all software writing is fully automated. Can you imagine that world?

- By the end of this year, the amount of automated software will be so high. But it will be things like trying to train a model with RL, and you need multiple bunches of GPUs communicating with each other. That will still be hard, but much easier.

- One way to think about this—the full automation of programming—is to consider the fraction of useful code lines written compared to the number of humans in the loop. Presumably, for a long time, humans will be in the loop of software writing. It will just be fewer and fewer relative to the amount of code written, right? With the &quot;superhuman coder,&quot; the presumption is that it goes to zero—the number of humans in the loop. What does that world look like when the number of humans in the loop is in the hundreds, not in the hundreds of thousands?

- I think software engineering will be driven more toward system design and outcomes. I think this has been happening over the last few weeks, where people have gone from a month ago saying, &quot;Oh yeah, agents are kind of slop&quot;—which is a famous Andrej Karpathy quote—to what is a little bit of a meme: the industrialization of software, where anyone can create software. I do think we are closer to that side of things. It takes direction and understanding how the systems work to extract the best from language models. It's hard to accept the gravity of how much is going to change with software development and how many more people can do things without ever looking at the code.

- I think what's interesting is to consider whether these systems will be completely independent. While I have no doubt that LLMs will at some point solve coding, in a sense—like calculators solved calculating, right? At some point, humans developed a tool where you never need a human to calculate a number. You just type it in, and it's an algorithm. In that sense. And I think that's probably the same for coding. But the question is... I think what will happen is, you will just say, &quot;Build that website.&quot; It will create a really good website, and then you might refine it. But will it do things independently? Will humans still be asking the AI to do something—like, will there be a person to say, &quot;Build that website?&quot; Or will there be AI that just builds websites, or something similar?

- I think talking about building websites is...

- Mm-hmm, too simple.

- The problem with websites and the web—HTML and all that kind of stuff—is that it's very resilient to just... slop. It will show you slop; it's good at displaying slop. I would rather think of safety-critical systems, like asking AI to end-to-end generate something that manages logistics, or a fleet of cars, that kind of stuff. It end-to-end generates that for you.

- I think a more intermediate example is something like Slack or Microsoft Word. If organizations allow it, AI could very easily implement features end-to-end and do a fairly good job for things you want to try. You want to add a new tab in Slack that you want to use, and I think AI will be able to do that quite well.

- Actually, that's a really great example. How far away are we from that?

- Like this year.

- See, I don't know. I don't know.

- I don't know how bad production codebases are, but I think that within the next few years, many people are going to be pushed to be more of a designer and product manager. You'll have multiple agents that can try things for you, and they might take a day or two to implement a feature or attempt to fix a bug. And you'll have these dashboards—which are actually good dashboards—where your agents will talk to you, and you'll then give feedback. But things like making a passable website logo... I think these cohesive design elements and style will be very hard for models, as will deciding what to add next.

- Okay. So I hang out with a lot of programmers, and some of them are a little bit on the skeptical side, in general. I think there's a lot of complexity involved in adding features to complex systems. For example, if you look at the browser, Chrome. If I wanted to add a feature—if I wanted to have tabs on the left side as opposed to up top... Interface-wise, I think we're not... This is not a &quot;next year&quot; thing.

- For one of the Claude releases this year, one of their tests was to give it a piece of software and leave Claude to run to recreate it entirely. It could already almost rebuild Slack from scratch, just given the software's parameters and left in a sandbox environment to do so.

- So the &quot;from scratch&quot; part, I almost like better.

- It might be that smaller and newer companies are advantaged, saying, &quot;We don't have to have the bloat and complexity, and therefore this feature exists.&quot;

- And I think this gets to your point that some people you talk to are skeptical. I think that's not because the LLM can't do X, Y, or Z. It's because people don't want it to do it this way. Some of that could be a skill issue on the human side. We have to be honest with ourselves. Some of that could be an underspecification issue. So programming is like... you're just assuming... This is like a communication issue in relationships and friendships. You're assuming the LLM is supposed to read your mind. I think this is where spec-driven design is really important: you just use natural language to specify what you want.

- If you talk to people at the labs, they use these in their training and production code. Claude Code is built with Claude Code, and they all use these tools extensively. Dario talks about how much of Claude's code is generated this way. These people are slightly ahead in terms of the capabilities they have and what they probably spend on inference. They could spend 10 to 100 times as much as we're spending, while we're on a lowly $100 or $200 a month plan. They truly let it rip. And I think that, with the pace of progress we have, it seems like—a year ago we didn't have Claude Code, and we didn't really have reasoning models. The difference between sitting here today and what we can do with these models is huge. It seems like there's a lot of low-hanging fruit to improve them. The failure modes are pretty dumb. It's like—&quot;Claude, you tried to use the CLI command I don't have installed 14 times, and then I sent you the command to run.&quot; That kind of thing, from a modeling perspective, is pretty fixable. So, I don't know.

- I agree with you. I've been becoming more and more bullish in general. Speaking to what you're articulating, I think it is a human skill issue. Anthropic is leading the way, along with other companies, in understanding how to best use models for programming; therefore, they're effectively using them. I think many programmers are on the outskirts. They're like... they don't... I mean, there isn't a really good guide on how to use them. People are trying to figure it out, but—

- It might be very expensive. The entry point might be $2,000 a month, which is only for tech companies and wealthy individuals. That could be it.

- But it might be worth it. If the final result is a working software system, it might be worth it. By the way, it's funny how we converged from the discussion of timelines to AGI to something more pragmatic and useful. Is there anything concrete, interesting, and profound to be said about timelines to AGI and ASI? Or are these discussions a bit too detached from the day-to-day?

- There are interesting bets. So there are a lot of people trying to do reinforcement learning with verifiable rewards, but in real scientific domains. There are startups that have hundreds of millions of dollars in funding and wet labs where they're having language models propose hypotheses that are tested in the real world. And I would say that they're early, but with the pace of progress, it's like—maybe they're early by six months and succeed because they were there first, or maybe they're early by eight years. You don't really know. I think that type of moonshot, to branch this momentum into other sciences, is like, &quot;Okay, that would be very transformative if AlphaFold moments happen in all sorts of other scientific domains, with a startup solving this.&quot; I think there are startups—I think maybe Harmonic is one—where they're going all in on language models plus Lean for math. I think you had another podcast guest where you talked about this recently, and it's like we don't know exactly what's going to come out of spending $100 million on that model. And most of them will fail, but a couple of them might be big breakthroughs that are very different than ChatGPT or Claude Code-type software experiences. Like a tool that's only good for a PhD mathematician but makes them 100 times more effective...

- I agree. I think this will happen in a lot of domains, especially in domains with many resources, like finance, legal, and pharmaceutical companies. But then again, is it really AGI? Because we are now specializing it again. And then again, is it really that much different from how we had specialized algorithms back in the day? I think it's just the same thing, way more sophisticated. But I don't know, is there a threshold when we call it AGI? I think the really cool thing here is that we have foundation models that we can specialize. I think that's the breakthrough. Right now, I think we're not there yet because first, it's too expensive. But also, ChatGPT doesn't just give away their model for customization. I think once that's true... And I can imagine this as a business model where OpenAI says at some point, &quot;Hey, Bank of America, for $100 million, we will build your custom model,&quot; or something like that. And I think that will be a huge economic value add. The other thing, though, is also... companies. I mean, right now, what is the differentiating factor? If everyone uses the same LLM, if everyone uses ChatGPT, they will all do the same thing. Again, I mean, then everyone is moving in lockstep. But usually, companies want a competitive advantage, and I think there's no way around using some of their private data, experimenting, and specializing. It's going to be interesting.

- Seeing the pace of progress, it does just feel like things are coming. I don't think the AGI and ASI thresholds are particularly useful.

- I think the real question, and this takes us back to the remote worker idea, is when are we going to see a big, obvious leap in economic impact? Because currently, there hasn't been an obvious leap in the economic impact of LLM models, for example. And that's, you know, aside from AGI or ASI and all that kind of stuff, there's a real question of, &quot;When are we going to see a GDP...&quot;

- ...jump?&quot;

- Yeah, what is GDP made up of? A lot of it is financial services, so I don't know what this looks like.

- Right, GDP's a—

- It's just hard for me to think about the GDP bump. But I would say that software development becomes valuable in a different way when you no longer have to look at the code anymore. When Claude allows you to run a small business. Essentially, Claude can set up your website, your bank account, your email, and whatever else. And you just have to express what you're trying to put into the world. That's not just an enterprise market, but it is hard. I don't know how you get people to try doing that. I guess if ChatGPT can do it—people will try ChatGPT.

- I think it boils down to the scientific question of how hard tool use is to solve. Because a lot of the stuff you're implying—the remote work stuff—is tool use. Computer use, like how you have an LLM that goes out there, this agentic system, and does something in the world, and only screws up 1% of the time.

- Computer use—

- Or less.

- ...is a good example of what labs care about, and we haven't seen a lot of progress on. We saw multiple demos in 2024, like &quot;Claude can use your computer,&quot; or OpenAI had &quot;Operator,&quot; and they all suck. They're investing money in this, and I think that will be a good example. Whereas actually, something where it just seems pretty... Taking over the whole screen seems a lot harder than having an API they can call in the back end. And some of that involves setting up a different environment for them to work in. They're not working on your MacBook; they are individually interfacing with Google, Amazon, and Slack, and they handle things in a very different way than humans do. So some of this might be structural blockers.

Here's the cleaned-up portion of the interview transcript:

- Also, in terms of specification, I think the problem is that for arbitrary tasks, you still have to specify what you want your LLM to do. What is the environment? How do you specify? You can say what the end goal is, but if it can't solve the end goal—with LLMs, if you ask for text, it can always clarify or do substeps. How do you put that information into a system that, for example, books a trip for you? You can say, &quot;You screwed up my credit card info,&quot; but even to get it to that point, how do you, as a user, guide the model before it can even attempt that? I think the interface is really hard.
- Yeah, it has to learn a lot about you specifically. And this goes to continual learning—about the general mistakes that are made overall, as well as those specific to you.
- All the AI interfaces are getting set up to ask humans for input. We talked a lot about Claude. It asks for feedback. If it doesn't have enough specification on your plan or your desires, it starts to ask, &quot;Would you rather?&quot; We talked about Memory, which saves across chats. Its first implementation is somewhat odd, where it'll mention my dog's name or something in a chat. I'm like, &quot;You don't need to be subtle about this. I don't care.&quot; But things are emerging, like ChatGPT has the Pulse feature. It's a curated couple of paragraphs with links to look at or talk about. People talk about how language models are going to ask you questions, which I think is very likely to work. The language model knows you had a doctor appointment; it's like, &quot;Hey, how are you feeling?&quot; This ventures into territory where humans are very susceptible, and it will lead to significant social change. But additionally, they're experimenting with having models engage. Some people like this Pulse feature; it processes your chats and automatically searches for information and puts it in the ChatGPT app. There are a lot of things coming.
- I used that feature before, and I feel bad because it does that every day, and I rarely check it. I think about how much compute is burned on something I don't even look at, you know? It's like, &quot;Oh...&quot;
- There's also a lot of idle compute in the world, so don't feel too bad.
- Okay. Do you think new ideas might be needed? Is it possible that the path to AGI—whatever that is, however we define that, to solve computer use more generally, to solve biology and chemistry and physics, sort of Dario's definition of AGI or powerful AI—do you think it's possible that totally new ideas are needed? Ideas that are non-LLM, non-RL. What might they look like? We're delving into philosophy a bit now.
- For something like a singularity to happen, I would say yes. And the new ideas could be architectures or training algorithms—fundamental deep learning concepts. But that's, by nature, quite hard to predict. I think we won't get very far without those advances. We might get this software solution, but it might stop at software and not encompass general computer use without further innovation. So I think a lot of progress will be coming, but if you zoom out, there will still be ideas emerging in the next 30 years that will be seen as major scientific innovations, enabling the next chapter of this. And I don't know if it comes in one year or in 15 years.
- Yeah. I wonder if the bitter lesson holds true for the next 100 years, what that would look like.
- If scaling laws are fundamental in deep learning, I think the bitter lesson will always apply, which is that compute will become more abundant. However, even with abundant compute, those with a steeper scaling law slope or a better offset (thinking of it as a 2D plot of performance and compute)... Even if there's more compute available, the ones that get 100X out of it will win.
- It might be something like literally computer clusters orbiting Earth with solar panels.
- The problem with that is heat dissipation. You get all the radiation from the sun, and you don't have any air for heat dissipation. However, there is ample space for clusters and a lot of solar energy. While heat dissipation would be a challenge, there is significant energy available, and engineering ingenuity could likely solve the heat problem. So it could be feasible.
- Is it possible (and we should acknowledge it definitely is possible) that we're basically going to be plateauing this year? Not in terms of system capabilities themselves, but in terms of what those capabilities actually mean for human civilization. So on the coding front, really nice websites will be built. Very nice auto-complete. Very nice way to understand code bases and maybe help debug, but ultimately just a very nice helper on the coding front. It can help research mathematicians do some math. It can help you with shopping. It's a nice helper. It's Clippy on steroids. What else? It may be a good educational tool and all that kind of stuff, but general computer use turns out to be extremely difficult to solve. So, I'm trying to frame the cynical case across all these domains: where there isn't a truly huge economic impact, and realizing how costly it is to train these systems at every level—both pre-training and inference, including the cost of inference, reasoning, and all of that. Is that scenario possible, and how likely do you think it is?
- When you look at the models, there are so many obvious things to improve. It takes a long time to train these models and to develop this 'art,' and it will take us multiple years with our current ideas to actually saturate in terms of whatever benchmark or performance we are searching for. It might serve very narrow niches; for instance, the average user among ChatGPT's 800 million might not get a lot of benefit from this, but it is going to serve different populations by getting better at different things.
- But I think what everybody's chasing now is a general system that's useful to everybody. So if that's not achieved... That can plateau, right?
- I think that dream is actually somewhat dying. As you talked about with the specialized models where, for example, multimodal—video generation is a totally different thing.
- &quot;That dream is somewhat dying&quot; is a big statement, because I don't know if it's dying. If you ask the actual Frontier Lab people, they're still chasing it, right?
- I do think they are still rushing to get the next model out, which will be much better than the previous one. And I can't see them slowing down. I just think the gains will be made or felt more not only through scaling the model, but I feel like there's now a lot of technical debt. It's been like, &quot;Well, let's just put the better model in there&quot;—better model after better model. Now, people are also saying, &quot;Okay, let's simultaneously improve everything around it too,&quot; such as the engineering of context and inference scaling. And the big labs will still keep doing that. And smaller labs will also catch up, as they are hiring more. There will be more people working with LLMs. It's kind of a cycle. LLMs also make them more productive, leading to amplification. I think what we can expect is amplification, but not a change of any paradigm. I don't think that implies a paradigm shift, but everything will simply be amplified repeatedly, and I can see that continuing for a long time, you know?
- Yeah. I guess my statement that the dream is dying depends on exactly what you think it's going to be doing. Claude is a general model that can do a lot of things, but it's not necessarily... It depends a lot on integrations. For example, I bet Claude could do a fairly good job of doing your email, and the hardest part is figuring out how to give it information and how to get it to be able to send your emails and similar tasks. But that just circles back to what is the &quot;one model to rule everything&quot; ethos, which is a single entity in the cloud that handles your entire digital life and is vastly smarter than everyone. It's like it's operating in a... So it's an interesting leap of faith to go from &quot;Claude becomes that&quot;—which in some ways has avenues for development—but I do think the rhetoric of the industry is a little bit different.
- I think the immediate thing we will feel next as a normal person using LLMs will probably be related to something trivial, such as making figures. Right now, LLMs are terrible at making figures. Is it because we are being served cheaper models with much less inference compute than what's available behind the scenes? There are some ways we can already get better figures, but if you ask today, &quot;Draw a flowchart of XYZ,&quot; it's terrible most of the time, and it is a very simple task for a human. Sometimes, it's almost easier to draw something than to write something.
- Yeah, the multimodal understanding does feel like something that is oddly not better solved.
- I think there's one obvious thing that we're not realizing, a gigantic yet hard-to-measure thing: making all of human knowledge accessible to the entire world. One thing that's hard to articulate is the huge difference between Google Search and an LLM. I feel like I can ask an LLM almost anything and get an answer, and it's hallucinating less and less. This means understanding my own life, figuring out a career trajectory, solving problems around me, or learning about anything throughout human history. I feel like nobody's really talking about that because it's immediately taken for granted as awesome. That's why everyone's using it. You get answers for things, and consider the impact of that over time. This is not just in the United States. This is all across the world. Kids throughout the world being able to learn these ideas—the impact that has across time is probably where the real, you know, GDP leap is. It won't be a small jump. That's how we get to Mars, how we build things, how we foster a million new OpenAIs and the kind of innovation that will happen. That's just this quiet force that permeates everything: human knowledge.
- I agree with you, and in a sense, it makes knowledge more accessible, but it also depends on the topic. For something like math, you can ask it questions and it answers, but if you want to learn a topic from scratch, I think the sweet spot is this: there are really good math textbooks laid out linearly, and that is a proven strategy for learning a topic. It makes sense, if you start from zero, to ramp up using information-dense text to absorb it, but then use the LLM to create infinite exercises. If you encounter problems in a certain area, have questions, or are uncertain about certain things, you ask it to generate example problems, solve them, and then ask further questions. Then you might need more background knowledge, and you ask it to generate that. But it won't give you anything that is not in the textbook. It's just packaging it differently, if that makes sense. But then there are things where I feel like it adds value in a more timely sense, where there is no good alternative besides a human doing it on the fly. For example, if you're planning a trip to Disneyland and trying to figure out which tickets to buy for which park and when, there's no textbook on that. No information-dense resource exists. There's only the sparse internet, and there is a lot of value in an LLM. You have specific travel constraints—&quot;I want to go here and there&quot;—and you can ask it to figure out what you need, what it costs, and similar details. It offers a very customized, on-the-fly package. This is one of a thousand examples of personalization. Personalization is essentially pulling information from the sparse internet, which is a non-information-dense source where no better version exists. It just doesn't exist, so you're almost making it from scratch.
- And if it does exist, it's full of—speaking of Disney World—full of... what would you call it? Ad slop. It's impossible to filter through. For example, to find the top 10 things to do in any city in the world, an LLM is just way better to ask than relying on anything else on the internet.
- For now, that's because they are massively subsidized. They're eventually going to be paid for by ads.
- Oh my goodness.
- It's coming.
- No. No. I'm hoping there's a very clear indication of what is and isn't an ad in that context.
- That's something I mentioned a few years ago. I don't know; if you are looking for a new running shoe, is it a coincidence that Nike comes up first? Maybe, maybe not. There are clear laws around this requiring transparency, but that's what everyone fears: the subtle messages embedded within. It also brings us to the topic of ads. I think this was something they tried to launch in 2025 because it's still not making money in other ways right now. Having ad spots in there... the issue is, they couldn't do it because there are alternatives without ads, and people would flock to those other products. It's just crazy how they are constantly one-upping each other, spending so much money simply to acquire users.
- I think so. Like with Instagram ads—I don't use Instagram, but I understand the appeal of paying a platform to find users who will genuinely like your product. That is the best-case scenario for things like Instagram ads. But there are also plenty of cases where advertising is very detrimental to incentives, and I think a world where the power of AI can integrate with that positive view—where, for example, &quot;I am a person, I have a small business, and I want to make the best, damn steak knives in the world, and I want to sell them to somebody who needs them.&quot; If AI can make that sort of advertising work even better, that's very good for the world, especially with digital infrastructure, because that's how the modern web has been built. But that's not to say that addicting feeds simply to show people more content is a good thing. I think that's even what OpenAI would say; they want to find a way to achieve the monetization upside of ads while still giving their users agency. I personally would think Google is probably going to be better at figuring this out because they already have an ad supply. If they figure out how to turn this demand in their Gemini app into useful ads, then they can activate it. And somebody will figure it out—I don't know if it's this year, but there will be experiments with it.
- I do think what holds companies back right now is simply that the competition isn't doing it. It's more about reputation. I think people are currently just afraid of ruining or losing their reputation and users, because it would make headlines if someone launched these ads.
- Unless they were great, but the first ads won't be great because it's a hard problem. We don't know how to solve it yet.
- I think also the first version of that will likely be something like on X, similar to the timeline where you sometimes see a promoted post in between regular content. It will say &quot;promoted&quot; or something small, and then there will be an image. I think right now the problem is: who makes the first move?
- If we go 10 years out, the proposition for ads is that by having so many users, you will make so much money from ads that you can use this to fund better R&amp;D and create better models, which is why YouTube is dominating the market. Netflix is scared of YouTube. They have ads, and they make—I pay $28 a month for Premium—at least $28 a month off of me and many other people. And they're just creating such a dominant position in video. That's the proposition: that ads can give you a sustained advantage in what you're spending per user. But there's so much money in it right now that starting that flywheel is scary because it's a long-term bet.
- Do you think there'll be some crazy big business moves this year? Like Google or Apple acquiring Anthropic or something like this?
- Dario will never sell, but we are starting to see some consolidation, with Groq for $20 billion and Scale AI for almost $30 billion, and countless other deals structured in a way that is actually detrimental to the Silicon Valley ecosystem. These sorts of licensing deals, where not everybody gets brought along, contrast with a full acquisition that benefits the rank-and-file employee by getting their stock vested. That's a big issue for the culture to address because the startup ecosystem is the lifeblood. If you join a startup, even if it's not that successful, your startup may get acquired for a cheap premium, and you'll get paid out for your equity. And these licensing deals are essentially taking the top talent. I think the rumored deal for Groq to Nvidia is better for the employees, but it is still this antitrust-avoiding maneuver. But I think this trend of consolidation will continue. Many smart people I respect, including myself, have been expecting consolidation to have happened sooner, but it seems like some of these things are starting to turn. But at the same time, you have companies raising ridiculous amounts of money for reasons I don't know. I'm like, &quot;I don't know why you're taking that money.&quot; So it's a mixed situation this year, but some consolidation pressure is starting to build.
- What kind of surprising consolidation will we see? You say Anthropic is a &quot;never.&quot; I mean, Groq is a big one. Groq (with a Q), by the way.
- Yeah. There's just a lot of startups and a very high premium on AI startups. So there could be a lot of these kinds of $10 billion range acquisitions, which is really big for a startup that might have been founded only a year ago. I think Manus.ai—a company based in Singapore that Meta founded eight months ago—then had a $2 billion exit. I think there will be other big, multi-billion dollar acquisitions, like Perplexity.
- Like Perplexity, right?
- Yeah, people rumored them being acquired by Apple. I think there's a lot of pressure and liquidity in the AI space. There's pressure on big companies to achieve outcomes, and I would guess a big acquisition gives people leeway to then tell the next chapter of that story.
- I mean, Cursor—we've been talking about code, and if somebody acquires Cursor...
- They are in such a good position by having so much user data. And we talked about continual learning. They had one of the most interesting two sentences in a blog post, stating that their new Composer model was a fine-tune of one of these large mixture-of-expert models from China. You can know that by asking Gossip, or because the model sometimes responds in Chinese—which none of the American models do. They had a blog post where they stated, &quot;We're updating the model weights every 90 minutes based on real-world feedback from people using it.&quot; This is like the closest thing to real-world reinforcement learning (RL) happening on a model, and it was just mentioned in one of their blog posts.
- That's incredible.
- And by the way, I use Composer a lot because one of the benefits it has is that it's fast.
- I need to try it, because everyone says this.
- And there'll be some IPOs potentially. Do you think Anthropic, OpenAI, xAI will have IPOs?
- They can all raise so much money so easily that they don't feel a need to. As long as fundraising is easy, they're not going to IPO because public markets apply pressure. I think we're seeing in China that the ecosystem is a little different, with both MiniMax and Zhipu AI filing IPO paperwork, which will be interesting to see how that market reacts. I would actually guess that it's going to be similarly hype-driven to the US, as long as this continues and isn't based on the realities that they're both losing a ton of money. I wish more of the American gigantic AI startups were public because it would be very interesting to see how they're spending their money and have more insight. And also just to give people access to investing in these, because I think that they are some of the most formidable—they are the companies of the era. And the tradition is now for so many of the big startups in the US to not go public. It's like we're still waiting for Stripe's IPO, but Databricks definitely didn't go public; they raised like a Series G or something. And I just feel like it's a kind of weird equilibrium for the market, where I would like to see these companies go public and evolve in the way that a company can.
- Do you think 10 years from now some of the frontier model companies are still around? Anthropic, OpenAI?
- I definitely don't see it becoming a winner-takes-all scenario unless there truly is some algorithmic secret that one of them finds that propels this flywheel. Because their development paths are so similar. Google and OpenAI have very similar products, and Anthropic's more focused, but when you talk to people it sounds like they're solving a lot of the same problems. And offerings will spread out. It's a very big cake that's being made, and people are going to take money out of it.
- I don't want to trivialize it, but OpenAI and Anthropic are primarily LLM service providers. And some of the other companies, like Google and xAI (linked to X), do other things too. And so it's very possible if AI becomes more commodified, the companies that are just providing LLMs will die.
- Their advantage is having a lot of users, and I think they will simply pivot. I think, if they figure out... For example, Anthropic, I think, pivoted. I don't think they originally planned to work on code, but they found, &quot;Okay, this is a nice niche, and now we are comfortable in this niche and we can push on it.&quot; And I can see the same thing happening once—hypothetically speaking, I'm not sure if it will be true—but let's say Google takes all the market share of the general chatbot. Maybe OpenAI will then focus on some other sub-topic. They have too many users to go away in the foreseeable future, I think.
- I think Google is always ready to say, &quot;Hold my beer,&quot; with AI models.
- I think the question is if the companies can support the valuations. I see AI companies being looked at in some ways like AWS, Azure, and GCP: all competing in the same space and all very successful businesses. There's a chance that the API market is so unprofitable that they will move up and down the stack to products and hardware. They have so much cash they can build power plants and data centers, which is a durable advantage now. But there's also a reasonable outcome that these APIs are so valuable and flexible for developers that they become something akin to AWS. But AWS and Azure are also going to have these APIs, so there's some... That's a tough market, having five or six companies competing in the API market. So maybe they get squeezed out.
- You mentioned &quot;RIP Llama.&quot; Is there a path to winning for Meta?
- I think nobody knows. They're moving a lot, so they're signing licensing deals with Black Forest Labs (an image generation company) or Midjourney. So, in some ways, on the product and consumer-facing AI front, it's too early to tell. I think they have some excellent and very motivated people close to Zuckerberg. So I think that there's still a story to unfold there. Llama is a bit different, as it was the most focused expression of the organization. And I don't see Llama being supported to that same extent. I think it was a very successful brand for them. So they still might participate in the open ecosystem or continue the Llama brand into a different service because people know what Llama is.
- You think there's a Llama 5?
- Not an open-weight one.
- It's interesting. I think also, just to recap a bit, Llama was, I would say, the pioneering open-weight model. And then Llama 1, 2, 3 got a lot of love. But then, hypothesizing or speculating, I think what happened was that the leaders at Meta, specifically the upper executives, got very excited about LLaMA because they saw how popular it was in the community. And then I think the problem was trying to—let's say—monetize, or at least use, open source to make a bigger splash. It almost felt forced, like developing these very big LLaMA 4 models to be at the top of the benchmarks. But I don't think the goal of LLaMA models is to be at the top of benchmarks, beating, for instance, ChatGPT or other models. I think the goal was to have a model that people can use, trust, modify, and understand. So that includes having smaller models; they don't have to be the best models. And what happened was that these models—the benchmarks suggested they were better than they actually were, because I think they had specific models trained on preferences, so they performed well on benchmarks. That's kind of like this overfitting phenomenon, to force it to be the best. But then, at the same time, they didn't release the small models that people could use. And I think no one could run these big models at that time. And then there was a weird situation. I think it's just because people got too excited about headlines pushing the frontier. I think that's all it was.
- I think it imploded under internal political fighting and misaligned incentives. So, for example, the researchers want to build the best models, but there's a layer of organization and management trying to demonstrate their achievements. And then there are many anecdotes and rumors about how some horrible technical decision was made and how that played a role. And it just seems like it kind of got too bad, to the point where it all just crashed.
- Yeah, but we should also give huge props to Mark Zuckerberg. I think it actually comes from Mark, from the top of the leadership, saying open source is important. The fact that that exists means there could be a LLaMA 5, where they learn the lessons from the benchmarking and say, &quot;We're going to be GPT-class and provide a really awesome library of open source.&quot;
- What people say is that there's a debate between Mark and Alexander Wang, who is very bright but much more against open source. And to the extent that he has a lot of influence over the AI organization, it seems much less likely, because it seems like Mark brought him in for fresh leadership and to direct AI. And if being open or closed is no longer the defining nature of the model, I don't expect that to be a defining argument between Mark and Alex. They're both very bright, but I have a hard time understanding all of it because Mark wrote a piece in July 2024, which was probably the best blog post at the time, making the case for open source AI. And then July 2025 came around, and it was like, &quot;We're reevaluating our relationship with open source.&quot;
- But I think also, not to say it was the problem, but I think we may have been a bit too harsh, and that caused some of that. Because we, as open source developers or the open source community, gave it a lot of backlash, even though the model was perhaps not what everyone hoped for. And I think that was a bit unfortunate, because I can see that as a company, they were hoping for positive headlines. And instead of just getting no headlines or positive ones, they in turn got negative headlines. It kind of reflected badly on the company, and I think that's also something where it's maybe a spite reaction, almost like, &quot;Okay, we tried to do something nice, we tried to give you something cool like an open source model, and now you are being negative about us.&quot; So in that sense, it looks like, &quot;Well, maybe then we'll change our mind.&quot; I don't know.
- Yeah, that's where the dynamics of discourse on X can lead us as a community astray. Because sometimes it feels random. People pick the thing they like and don't like. I mean, you can see the same thing with Grok 4.1 and Grok Code Fast 1.0. I don't think, vibe-wise, people publicly love it. But a lot of people use it. So if you look at Reddit and X, the programming community doesn't really give it praise, but they use it. And probably the same thing with Llama. I don't understand the dynamics of either positive or negative hype. I just don't understand it.
- I mean, one of the stories of 2025 is the US filling the gap left by Llama, referring to the rise of these Chinese open-weight models, to the point where that was the single issue I've spent a lot of energy on in the last five months, trying to do policy work to get the US to invest in this.
- So tell me the story of Adam.
- The Adam Project started with me calling it the American DeepSeek Project, which doesn't really work for DC audiences, but it's the story of what is the most impactful thing I can do with my career. These Chinese open-weight models are cultivating a lot of power, and there is a lot of demand for building on these open models, especially in US enterprises that are very cagey about them.
- According to Perplexity, the Adam Project (American Truly Open Models) is a US-based initiative to build and host high-quality, genuinely open-weight AI models and supporting infrastructure, explicitly aimed at competing with and catching up to China's rapidly advancing open-source AI ecosystem.
- I think the two-sentence summary would be: one, the proposition that open models are going to be an engine for AI research because that is what people start with; therefore, it's important to own them. And the second is that, therefore, the US should be building the best models so that the best research happens in the US, and those US companies take the value from being the home of where AI research is happening. And without more investment in open models—we have plots on the website showing &quot;Qwen, Qwen, Qwen, Qwen&quot;—it's all these excellent models from Chinese companies that are cultivating influence in the US and internationally. And I think the US is spending way more on AI, and the ability to create open models that are half a generation or a generation beyond the cutting edge of closed labs costs $100 million. While that's a lot of money, it's not a lot of money for these companies. So, we need a centralizing force of people who want to do this. And I think we got engagement from people pretty much across the full stack, including in policy.
- So there has been support from the administration?
- I don't think anyone in government, technically, has signed it publicly, but I know that people who have worked in AI policy in both the Biden and Trump administrations are very supportive of promoting open-source models in the US. For example, AI2 received a grant from the NSF for $100 million over four years, which is the biggest CS grant the NSF has ever awarded, and it's for AI2 to attempt this. I think it's a starting point. But the best outcome happens when there are multiple organizations building models, because they can cross-pollinate ideas and build this ecosystem. I don't think it works if it's just Llama releasing models to the world, because Llama could go away. The same thing applies to AI2; I can't be the only one building models. And that's a lot of time spent talking to people, whether they're in policy... I know NVIDIA is very excited about this. Jensen Huang has been talking about the urgency for this, and they've done a lot more in 2025, where the Nemotron models are more of a focus. They've started releasing some data along with NVIDIA's open models, and very few companies do this, especially companies of NVIDIA's size. So there are signs of progress. We hear about Reflection AI, where they say their $2 billion fundraise is dedicated to building US open models, and I feel like their announcement tweet reads like a blog post. I think that cultural tide is starting to turn. In July, we had four or five DeepSeek-caliber Chinese open-weight models and zero from the US. That was the moment when I thought, &quot;Oh, I guess I have to spend energy on this because nobody else is going to do it.&quot; So it takes a lot of people contributing together. I'm not saying the Adam Project is the only thing helping to move the ecosystem, but it's people like me doing this sort of thing to get the word out.
- Do you like the 2025 America's AI Action Plan? It includes open source aspects. The White House AI Action Plan includes a dedicated section titled &quot;Encourage Open-Source and Open-Weight AI,&quot; defining such models and arguing they have unique value for innovation and startups.
- Yeah. I mean, the AI Action Plan is a plan, but largely, I think it's perhaps the most coherent policy document that has come out of the administration, and I hope it largely succeeds. I know people who have worked on the AI Action Plan, and the challenge is taking policy and making it real. I have no idea how to do this as an AI researcher, but largely, many things within it were very real. There's a huge build-out of AI in the country, and there are many issues that people are hearing about, from water use to whatever. We should be able to build things in this country, but we also need to avoid ruining places in our country in the process of building it, and it's worthwhile to spend energy on this. I think that's a role the federal government plays. They set the agenda. And setting the agenda that open-weight should be a primary consideration is a large part of what they can do, which then influences how people think about it.
- Also, for education and talent for these companies, I think it's very important, because otherwise, if there are only closed models, how do you get the next generation of people contributing? Because at some point, you will only be able to learn after you join a company. But at that point, how do you hire and identify talented people? I think open source is important for many things, but also for educating the population and training the next generation of researchers. It's the way, or perhaps the only way.
- The way I could have gotten this to go more viral was to tell a story of Chinese AI integrating with an authoritarian state, becoming ASI, and taking over the world. Therefore, we need our own American models. But it's intentional why I talk about innovation and science in the US, because I think it's both a more realistic outcome and a world that I would like to manifest.
- I would say, though, any open-weight model is a valuable model.
- Yeah. And my argument is that we should be in a leading position. But I think it's worth stating it so simply because there are still voices in the AI ecosystem that say we should consider banning the release of open models due to safety risks. And I think it's worth adding that, effectively, that's impossible without making the US have its own &quot;Great Firewall,&quot; which is also known not to work that well. This is because the cost for training these models—whether it's one to a hundred million dollars—is attainable to a huge number of people in the world who want to have influence, so these models will be trained all over the world. And we want these models (especially when, I mean, there are safety concerns) and these tools and information to flow freely across the world and into the US so that people can use and learn from them. Stopping that would be such a restructuring of our internet that it seems impossible.
- Do you think maybe in that case, the big open-weight models from China are actually a good thing for US companies? Because perhaps the US companies you mentioned earlier are usually one generation behind in terms of what they release open source versus what they are actually using. For example, GPT-4o might not be the cutting-edge model, nor might Gemma 3 be, but they release them because they know these are safe. But then, when these companies see, for example, DeepSeek-V3, which is really awesome, getting used with no backlash or security risk, that could then encourage them to release better models. Perhaps that, in a sense, is a very positive thing.
- A hundred percent. These Chinese companies have set things into motion that I think would potentially not have happened if they were not all releasing models. So I'm almost sure that those discussions have been had by leadership.
- Is there a possible future where the dominant AI models in the world are all open source?
- It depends on the trajectory of progress that you predict. If you think saturation in progress is coming within a few years—essentially, within the time where financial support is still very good—then open models will be so optimized and so much cheaper to run that they will win out. Essentially, this goes back to open source principles, where so many more people will be putting money into optimizing the serving of these open-weight common architectures that they will become standards. Then, you could have chips dedicated to them, and it will be much cheaper than the custom offerings from these closed companies.
- We should mention that the Situational Awareness report predicts, from a narrative perspective, that there will be a lot of centralization. As AI systems get smarter and smarter, national security concerns will emerge, leading to centralized, super-secretive labs and a full-blown military race between China and the United States. And so, with all these fun conversations we're having about LLMs, the generals and soldiers will come into the room and say, &quot;Alright. We're now in the Manhattan Project stage of this whole thing.&quot;
- For 2025, '26, '27, I don't think something like that is even remotely possible. I mean, you can make the same argument for computers, right? You can say, &quot;Okay, computers are capable, and we don't want the general public to get them.&quot; Or chips, even AI chips, but you see how Huawei makes chips now. It took a few years, but I don't think there's a way you can contain knowledge like that. In this day and age, it's impossible, much like the internet. I don't think this is a possibility.
- On the Manhattan Project thing, one of my humorous observations looking at them is that a Manhattan Project-like initiative for open models would actually be pretty reasonable, because it wouldn't cost that much. But I think that will happen. It seems like culturally, the companies are changing. But I agree with Sebastian on all of the stuff he just said. I just don't see it happening, nor do I see it being helpful.
- Yeah. I mean, the motivating force behind the Manhattan Project was civilizational risk. It's harder to motivate that for open-source models.
- There's no civilizational risk.
- On the hardware side, we mentioned NVIDIA a number of times. Do you think Jensen and NVIDIA will continue to win?
- I think they have the downside of having to iterate and manufacture a lot. And I think what they're doing—they do innovate—but there's always the chance that someone does something fundamentally different and gets very lucky. But the problem is adoption. NVIDIA's moat is probably not just the GPU itself; it's more the CUDA ecosystem, which has evolved over two decades. I mean, even back when I was a grad student, we were in a lab doing biophysical simulations, molecular dynamics, and we had a Tesla GPU back then just for computations. That was 15 years ago now. And they just built this up for a long time, and I think that's the moat. It's not the chip itself. Although they now have the money to iterate, build, and scale, the real challenge lies in compatibility. If you're a company at that scale, why would you go with something risky where only a few chips can be made per year? You go with the established one. But I do think with LLMs now, it will be easier to design something similar to CUDA. It took 15 years because it was hard, but now with LLMs, we can perhaps replicate CUDA.
- And I wonder if there will be a separation of training and inference compute as we stabilize and more compute is needed for inference.
- That's supposed to be the point of the Groq acquisition. And that's why part of what the Vera Rubin project is: they have a new chip with no high-bandwidth memory (or very little, which is one of the most expensive pieces). It's designed for pre-fill, which is the part of inference where you essentially do a lot of matrix multiplications. You only need the memory when you're doing this autoregressive generation and you have the KV cache swaps. So they have this new GPU that's designed for that specific use case, and the cost of ownership per flop (or whatever metric) is actually way lower. But I think NVIDIA's fate still lies in the diffusion of AI. Their biggest clients are still these hyperscale companies. Google, for example, can obviously make TPUs. Amazon is making Trainium. Microsoft will try to do its own thing. And as long as the pace of AI progress is high, NVIDIA's platform is the most flexible, and people will want that. But if there's stagnation, then there's more time to create bespoke chips.
- It's interesting that NVIDIA is quite active in trying to develop all kinds of different products.
- They try to create areas of commercial value that will use a lot of GPUs.
- But they keep innovating, and they are doing a lot of incredible research.
- Everyone says the company is super-oriented around Jensen and how operationally plugged in he is. And it sounds so unlike many other big companies that I've heard about. And as long as that's the culture, I think they will expect that to keep progress happening. It's like he's still in the Steve Jobs era of Apple. As long as that is how it operates, I'm pretty optimistic for their situation because it is their top-order problem, and I don't know if making these chips for the whole ecosystem is the top goal of all these other companies. They'll do a good job, but it might not be *as* good of a job.
- Since you mentioned Jensen, I've been reading a lot about history and about singular figures in history. What do you think about the &quot;single man/woman&quot; view of history? How important are individuals for steering the direction of history in the tech sector? So, what's NVIDIA without Jensen? You mentioned Steve Jobs. What's Apple without Steve Jobs? What's xAI without Elon or DeepMind without Demis?
- People make things earlier and faster. Scientifically, many great scientists credit being in the right place at the right time and still making the innovation, where eventually someone else would still have the idea. So I think that in that way, Jensen is helping manifest this GPU revolution much faster and much more focused than it would be without a person like him there. And this is making the entire AI build-out faster. But I do still think that eventually, something like ChatGPT would have happened and a build-out like this would have happened, but it probably would not have been as fast. I think that's the kind of influence applied.
- Individual people are placing bets on something. Some get lucky, some don't. But if you don't have these people at the helm, it would be more diffused. It's almost like investing in an ETF versus individual stocks. Individual stocks might go up or down more heavily than an ETF, which is more balanced. It will eventually go up over time. We'll get there. But I think the key thing is focus: passion and focus.
- Isn't there a real case to be made that without Jensen, there's not a reinvigoration of the deep learning revolution?
- It could have been 20 years later, that's the point.
- Or another AI winter—like a deep learning winter—could have come if GPUs weren't around.
- That could change history completely, because you could think of all the other technologies that could have emerged in the meantime, and the focus of human civilization would have been different... Silicon Valley would have been captured by different hype.
- But I do think there's certainly an aspect where the GPU trajectory was all planned. But on the other hand, it's also a lot of lucky coincidences or good intuition. For example, the investment into, let's say, biophysical simulations. I mean, I think it started with video games, and then it just happened to be good at linear algebra because video games require a lot of linear algebra. And then you have biophysical simulations. But still, I don't think the master plan was AI. I think it just happened that Alex Krizhevsky came along. Someone took these GPUs and said, &quot;Hey, let's try to train a neural network on that.&quot; It happened to work really well, and I think it only happened because you could purchase those GPUs.
- Gaming would have created a demand for faster processors if NVIDIA had gone out of business in the early days. That's what I would think. I think the GPUs would have been different for AlexNet—but I think GPUs would still exist at the time of AlexNet and at the time of the Transformer. It was just hard to know if it would be one company as successful, or multiple smaller companies with worse chips. But I don't think that's a 100-year delay. It might be a decade delay.
- Well, it could be a multi-decade delay. I mean, I just can't see Intel or AMD doing what NVIDIA did.
- I don't think it would be a company that currently exists. I think it would be a different company that would rise.
- Like Silicon Graphics or something.
- Yeah, some company that has died would have done it.
- But looking at it, it seems like these singular figures, these leaders, have a huge impact on the world's trajectory. Obviously, incredible teams behind them. But having that kind of very singular, almost dogmatic focus is necessary to make progress.
- Yeah, I mean, even with GPT, it wouldn't exist if there wasn't a person like Ilya who pushed for this scaling, right?
- Yeah, Dario was also deeply involved in that. If you read some of the histories from OpenAI, it almost seems wild thinking about how early these people were saying, &quot;We need to hook up 10,000 GPUs and take all of OpenAI's compute to train one model.&quot; Many people there didn't want to do that.
- Which is an insane thing to believe—to believe in scaling before scaling had any indication it was going to materialize. Again, singular figures. Speaking of which, 100 years from now—presumably post-singularity, however that singularity is defined. When historians look back at our time now, what technological breakthroughs would they really emphasize as having led to the singularity? So far, we have from Turing to today: 80 years.
- I think it would still be computing, as in the umbrella term &quot;computing.&quot; I don't necessarily think that even 100 or 200 years from now, it would be &quot;AI.&quot; It could still very well be &quot;computers,&quot; you know? We are now taking better advantage of computers, but it's the fundamental concept of computing.
- It's basically a Moore's Law kind of discussion. Even the details of CUDA and GPUs won't be remembered, and there won't be all this software turmoil. It will just be, obviously, compute.
- I generally agree, but can the connectivity of the internet and compute be merged? Or is it both?
- I think the internet will probably be related to communication. It could be a phone, the internet, or a satellite—that kind of stuff. Whereas compute is more about the scaling aspect of it.
- It's possible that the internet is completely forgotten. That the internet is wrapped into communication networks. This is just another manifestation of that, and the real breakthrough comes from increased compute—Moore's Law, broadly defined.
- Well, I think the connection of people is very fundamental to this. So, for example, you can talk to anyone. If you want to find the best person in the world for something, they are somewhere in the world. And AIs will also rely on being able to have that flow of information. I think I've been fixating on when I said the dream of one central model was dead, and what is evolving is people having many agents for different tasks. People already started doing this with different clouds for different tasks. And it's described as many AGIs in the data center, where each one manages and they talk to each other. And that is so reliant on networking and the free flow of information, on top of compute. But networking, especially with GPUs, is such an integral part of scaling compute; the GPUs in the data centers need to talk to each other.
- Will anything about neural networks be remembered? Do you think there's something specific and singular about neural networks being seen as a breakthrough, like a stroke of genius, where you're basically replicating the structure of the human brain/mind in a very crude way?
- I think without the human mind, we probably wouldn't have neural networks because it was just an inspiration for that. But on the other hand, I think it's just so different. I mean, it's digital versus biological, so I do think it will probably be grouped more as an algorithm.
- That's massively parallelizable on this particular kind of compute?
- It could have well been genetic algorithms, just parallelized. It just happens that this is more efficient and works better.
- And it very well could be that neural networks, the way we architect them, is just a small component of the system that leads to singularity.
- If you think of it in 100 years, I think society can be changed more with more compute and intelligence because of autonomy. But looking at this, what are the things we remember from the Industrial Revolution? We remember the engine, which is probably the equivalent of the computer in this context. But there are many other physical transformations people are aware of, like the cotton gin and all these machines that are still known—air conditioning, refrigerators. Some of these things from AI will still be known.

The word &quot;transformer&quot; could still be known. I would guess that deep learning is definitely still known, but the transformer might be evolved away from in 100 years with AI researchers everywhere. But I think deep learning is likely to be a term that is remembered.
- And I wonder what the air conditioning and refrigeration of the future is that AI brings. If we travel forward 100 years from now, what do you think is different? How do you think the world looks different? Do you think there are humans? Do you think there are robots everywhere walking around?
- I do think specialized robots for sure for certain tasks.
- Humanoid form?
- Maybe half humanoid. We'll see. I think for certain things, yes, there will be humanoid robots because it's just amenable to the environment. But for certain tasks, it might make sense. What's harder to imagine is how we interact with the devices and what humans do with devices. I'm pretty sure it will probably not be the cellphone or the laptop. Will it be implants?
- I mean, it has to be brain-computer interfaces, right? I mean, 100 years from now, it has to... Like, given the progress we're seeing now, there has to be... unless there's legitimately complete alteration of how we interact with reality.
- On the other hand, if you think of cars, cars are older than 100 years, right? And it's still the same interface. We haven't replaced cars with something else. We just made the cars better, but it's still a steering wheel, it's still wheels, you know?
- I think we'll still carry around a physical brick of compute... because people want some ability to have a private... Like, you might not engage with it as much as a phone, but having something for private information that is yours as an interface between the rest of the internet, I think, is something that will still exist. It might not look like an iPhone, and it might be used a lot less, but I still expect to have people carry things around.
- Why do you think the smartphone is the embodiment of private? There's a camera on it. There's a—
- Private for you, like encrypted messages, encrypted photos; you know what your life is. I guess this is a question on how optimistic you are about brain-machine interfaces. Is all that just going to be stored in the cloud, or your whole calendar? It's hard to think about processing all the information that we can process visually through brain-machine interfaces, presenting something like a calendar or something to you. Like, it's hard to just think about knowing without looking, like your email inbox. Like you signal to a computer and then you just know your email inbox. What does that... is that something the human brain can handle being piped into it non-visually? I don't know exactly how those transformations happen. Because humans aren't changing in 100 years. I think agency and community are things that people actually want.
- A local community, yeah.
- So, like, people you are close to, being able to do things with them and being able to ascribe meaning to your life. I think that, if not in 100 years, I don't think human biology is changing away from those on a time scale that we can discuss. And I think that UBI does not solve agency. I do expect mass wealth, and I hope that it has spread so that the average life does look very different in 100 years. But that's still a lot to happen in 100 years. If you think about countries that are early in their development process to getting access to computing and internet, to build all the infrastructure, and to have policy that shares one nation's wealth with another... I think it's an optimistic view to see all that happening in 100 years while they are still independent entities and not just absorbed into some international order by force.
- But there could be just better, more elaborate, more effective social support systems that help alleviate some levels of basic suffering in the world. You know, the transformation of society where a lot of jobs are lost in the short term—I think we have to really remember that each individual job that's lost is a human being who's suffering. That's like a tragedy. When jobs are lost, the scale of the loss is a real tragedy. You can make all kinds of arguments about economics or it's all going to be okay. It's good for the GDP; there's going to be new jobs created. Fundamentally, at the individual level for that human being, that's real suffering. That's a real personal sort of tragedy. And we have to not forget that as the technologies are being developed. And also, my hope for all the AI slop we're seeing is that there will be a greater and greater premium for the fundamental aspects of the human experience that are in-person, like seeing each other, talking together in-person.
- The next few years are definitely going to be an increased value on physical goods and events, and even more pressure on slop. The slop is only starting. The next few years will be more and more diverse—
- Do you think we'll all be drown—
- versions of slop.
- drowning in slop? Is that what—
- So, I'm hoping that society drowns in slop enough to snap out of it and be like, &quot;We can't deal with it.&quot; It just doesn't matter. We all can't deal with it. And then the physical has such a higher premium on it.
- Even classic examples, I honestly think this is true, and I think we will get tired of it. We are already kind of tired of it. Same with art. I don't think art will go away. You have paintings, physical paintings. There's more value, not just monetary value, but just more appreciation for the actual painting than a photocopy of that painting. It could be a perfect digital reprint, but there is something when you go to a museum and you see that real thing, and you just think about, &quot;Okay, a human.&quot; It's like a craft. You have, like, an appreciation for that. And I think the same is true for writing, for talking, for any type of experience, it will be... I do, unfortunately, think it will be a dichotomy, like a fork where some things will be automated. Like, you know, there aren't as many paintings as there used to be 200 years ago. There are more photographs, more photocopies. But at the same time, it won't go away. There will be value in that. I think the difference will just be what's the proportion of that. But personally, I have a hard time reading things where I see it's obviously AI generated. I'm sorry. It might be really good information, but I have a certain, &quot;Nah, not for me.&quot;
- I think eventually they'll fool you, and it'll be on platforms that give ways of verifying or building trust. So you will trust that Lex is not AI generated, having been here. So then you have trust in this channel. But it's harder for new people who don't have that trust.
- Well, that will get interesting because, fundamentally, I think it's a solvable problem by having trust in certain outlets that they won't do it, but it's all going to be trust-based. There will be systems to authorize, &quot;Okay, this is real. This is not real.&quot; There will be some telltale signs where you can tell this is AI generated and this is not. But some will be so good that it's hard to tell, and then you have to trust. And that will get interesting and a bit problematic.
- The extreme case of this is to watermark all human content. So all photos that we take on our own have some watermark until they are edited or something like this. And software can manage communications with the device manufacturer to maintain human editing, which is the opposite of the discussion to watermark AI images. And then you can make a Google image that has a watermark and use a different tool to remove the watermark.
- Yep. It's going to be an arms race, basically.
- And we've been mostly focusing on the positive aspects of AI. All the capabilities that we've been talking about can be used to destabilize human civilization with even just relatively dumb AI applied at scale, and then, further, superintelligent AI systems. Of course, there's the sort of &quot;doomer&quot; take that's important to consider a little bit as we develop these technologies. What gives you hope about the future of human civilization? Everything we've been talking about. Are we going to be okay?
- I think we will. I'm definitely a worrier both about AI and non-AI things. But humans do tend to find a way. I think that's what humans are built for: to have community and find a way to figure out problems. And that's what has gotten us to this point. And to think that the AI opportunity and related technologies are really big. And I think that there are big social and political problems to help everybody understand that. And I think that's what we're staring at a lot of right now, is like the world is a scary place, and AI is a very uncertain thing. And it takes a lot of work that is not necessarily building things. It's like telling people and understanding people, which is something the people building AI are historically not motivated or wanting to do. But it is something that's probably doable. It just will take longer than people want. And we have to go through that long period of hard, distraught AI discussions if we want to have the lasting benefits.
- Yeah. Through that process, I'm especially excited that we get a chance to better understand ourselves, at the individual level as humans and at the civilization level, and answer some of the big mysteries, like, &quot;What is this whole consciousness thing going on here?&quot; It seems to be truly special. There's a real miracle in our mind. And AI puts a mirror to ourselves, and we get to answer some of the big questions about, &quot;What is this whole thing going on here?&quot;
- Well, one thing about that is also what I think makes us very different from AI and why I don't worry about AI taking over is, as you said, consciousness. We humans, we decide what we want to do. AI, in its current implementation, I can't see it changing. You have to tell it what to do. And so, you still have the agency. It doesn't take the agency from you because it becomes a tool. You can think of it as a tool. You tell it what to do. It will be more automatic than other previous tools. It's certainly more powerful than a hammer; it can figure things out, but you're still in charge, right? So, the AI is not in charge; you're in charge. You tell the AI what to do, and it's doing it for you.
- So, in the post-singularity, post-apocalyptic war between humans and machines, you're saying humans are worth fighting for?
- 100%. I mean, this is... The movie *Terminator*, they made in the '80s, essentially. And I think, well, the only thing I can see going wrong is, of course, if things are explicitly programmed to do what is harmful, basically.
- I think, actually, in a *Terminator*-type setup, humans win. I think we're too clever. It's hard to explain how we figure it out, but we do. And we'll probably be using local LLMs, open-source LLMs, to help fight the machines. I apologize for the ridiculousness. As I said, Nathan already knows I've been a big fan of his for a long time. I've been a big fan of yours, Sebastian, for a long time, so it's an honor to finally meet you. Thank you for everything you put out into the world. Thank you for the excellent books you're writing. Thank you for teaching us. And thank you for talking today. This was fun.
- Thank you for inviting us here and having this human connection, which is actually—
- Extremely valuable—human connection. Thanks for listening to this conversation with Sebastian Raschka and Nathan Lambert. To support this podcast, please check out our sponsors in the description, where you can also find links to contact me, ask questions, give feedback, and so on. And now let me leave you with some words from Albert Einstein: &quot;It is not that I'm so smart, but I stay with the questions much longer.&quot; Thank you for listening, and I hope to see you next time.
                    </div>
                </div>
                
            </article>
            
        </main>

        <footer>
            Generated by Follower Tool
        </footer>
    </div>
    <script>
        function toggleSummary(id) {
            const preview = document.getElementById('preview-' + id);
            const full = document.getElementById('full-' + id);
            const btn = document.getElementById('sum-btn-' + id);

            if (full.classList.contains('expanded')) {
                full.classList.remove('expanded');
                preview.style.display = 'block';
                btn.classList.remove('expanded');
                btn.innerHTML = 'Read more <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M6 9l6 6 6-6"/></svg>';
            } else {
                full.classList.add('expanded');
                preview.style.display = 'none';
                btn.classList.add('expanded');
                btn.innerHTML = 'Show less <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M6 9l6 6 6-6"/></svg>';
            }
        }

        function toggleTranscript(id) {
            const content = document.getElementById('transcript-' + id);
            const btn = document.getElementById('trans-btn-' + id);

            if (content.classList.contains('expanded')) {
                content.classList.remove('expanded');
                btn.textContent = 'View transcript';
            } else {
                content.classList.add('expanded');
                btn.textContent = 'Hide transcript';
            }
        }
        </script>
</body>
</html>